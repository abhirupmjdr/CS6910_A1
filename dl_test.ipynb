{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhirupmjdr/CS6910_A1/blob/main/dl_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJFmZOitTGsi",
        "outputId": "fd70488f-2724-46fd-908d-a95b94435605"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.7917198301022377 -2.2821180729166732\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Gradinat Descent (Mitesh Khapra)\n",
        "import numpy as np\n",
        "X = [0.5,2.5]\n",
        "Y = [0.2,0.9]\n",
        "\n",
        "def fun(x,w,b):\n",
        "    return 1/(1+np.exp(-(w*x+b)))\n",
        "\n",
        "def grad_w(x,y,w,b):\n",
        "    fx=fun(x,w,b)\n",
        "    return (fx-y)*fx*(1-fx)*x\n",
        "\n",
        "def grad_b(x,y,w,b):\n",
        "    fx=fun(x,w,b)\n",
        "    return (fx-y)*fx*(1-fx)\n",
        "\n",
        "def error(w,b):\n",
        "    err=0.0\n",
        "    n=0\n",
        "    for x,y in zip(X,Y):\n",
        "        err+=(fun(x,w,b)-y)**2\n",
        "        n+=1\n",
        "    return err/n\n",
        "\n",
        "def gradiant_descent():\n",
        "    w,b,eta,max_itr=-2,-2,1.0,1000\n",
        "    for i in range (max_itr):\n",
        "        dw,db=0,0\n",
        "        for x,y in zip(X,Y):\n",
        "            dw+=grad_w(x,y,w,b)\n",
        "            db+=grad_b(x,y,w,b)\n",
        "        w=w-eta*dw\n",
        "        b=b-eta*db\n",
        "    if i==999:\n",
        "        print(w,b)\n",
        "        print(error(w,b))\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    gradiant_descent()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2meJgX2p700u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gHrEpPC5XKHv",
        "outputId": "df5824fc-8aab-4c7f-bee7-7a9e421d44d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.11994532 0.13682959 0.06105959 ... 0.13723868 0.59235629 0.13689276]\n",
            " [0.25163098 0.25992294 0.28016269 ... 0.25403603 0.12895459 0.57248959]\n",
            " [0.03669064 0.03684221 0.03513156 ... 0.03789504 0.01426023 0.0039766 ]\n",
            " ...\n",
            " [0.02844356 0.0500063  0.02622639 ... 0.02750218 0.04976222 0.0100033 ]\n",
            " [0.19163651 0.14171315 0.19899944 ... 0.19362844 0.04255918 0.1139606 ]\n",
            " [0.04519285 0.07596379 0.06984453 ... 0.04543387 0.0215163  0.02550157]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Question5.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/11gmNfKBG39ABwUVhkJr-KnagzUBnwk4z\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import seaborn as sns\n",
        "'''\n",
        "10 distinct images of fashion_mnist class is written and then wandb is initialized and the images are plotted\n",
        "'''\n",
        "ImageClasses = [\"Pullover\",\"Shirt\",\"Coat\",\"Trouser\",\"Dress\",\"Sandal\",\"Bag\",\"Sneaker\",\"Ankle boot\",\"T-shirt/top\"]\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "'''normalizing the data'''\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "x_valT = np.transpose(x_val.reshape(x_val.shape[0] , x_val.shape[1] * x_val.shape[2]))\n",
        "x_trainT = np.transpose(x_train.reshape(x_train.shape[0] , x_train.shape[1] * x_train.shape[2]))\n",
        "x_testT = np.transpose(x_test.reshape(x_test.shape[0] , x_test.shape[1] * x_test.shape[2]))\n",
        "y_trainT = y_train.reshape(1 , y_train.shape[0])\n",
        "y_valT = y_val.reshape(1 , y_val.shape[0])\n",
        "y_testT = y_test.reshape(1 , y_test.shape[0])\n",
        "\n",
        "class ActivationFunction:\n",
        "  '''all activation functions are defined here'''\n",
        "  def sigmoid(x):\n",
        "    return  1 / (1 + np.exp(-x))\n",
        "  def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "  def Relu(x):\n",
        "    return np.maximum(x,0)\n",
        "  def tanh(x):\n",
        "    return np.tanh(x)\n",
        "  def softmax_derivative(x):\n",
        "    return ActivationFunction.softmax(x) * (1-ActivationFunction.softmax(x))\n",
        "  def sigmoid_derivative(Z):\n",
        "    s = 1 /(1 + np.exp(-Z))\n",
        "    dA = s * (1 - s)\n",
        "    return dA\n",
        "  def Relu_derivative(x):\n",
        "    return 1*(x > 0)\n",
        "  def tanh_derivative(x):\n",
        "    return (1 - (np.tanh(x)**2))\n",
        "\n",
        "class NeuralNetwork:\n",
        "  '''\n",
        "  mode_of_initialization =str , The way the weights are initialized\n",
        "  n_layers = int ,total number of layers\n",
        "  activation_function = str , activation function of the neural network,default is sigmoid activation function\n",
        "  n_input = int ,number of inputs\n",
        "  n_outputs = int , number of outputs\n",
        "  num_neurns_in_hidden_layer = int ,how many neurons are in each hidden layer\n",
        "  n_neurons = list having number of neurons in each layer\n",
        "  TrainInput = input layer's input\n",
        "  TrainOutput = output layer's output\n",
        "  ValInput = Validation Input\n",
        "  ValOutput = validation output\n",
        "  parameters = dict ,stores the parameters of the model\n",
        "  cache = dict , stores the H and A for each layer which help in gradient computation\n",
        "  grads = dict ,store the gradients\n",
        "    '''\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  num_neurons_in_hidden_layers = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  parameters = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "  def __init__(self,mode_of_initialization = 'random',number_of_hidden_layers = 1,num_neurons_in_hidden_layers = 4,activation = 'sigmoid',TrainInput = x_trainT,TrainOutput = y_trainT,ValInput = x_valT,ValOutput = y_valT):\n",
        "\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    self.num_neurons_in_hidden_layers = num_neurons_in_hidden_layers\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.parameters[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.parameters[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.parameters[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def output(self,A):\n",
        "    '''\n",
        "    uses softmax function in the last layer for calculating the prediction\n",
        "    '''\n",
        "    return ActivationFunction.softmax(A)\n",
        "\n",
        "\n",
        "  def forward(self,X,activation,parameters):\n",
        "    '''\n",
        "    calculates H and A for each layer and stores them into cache dictionary. It uses the parameter dictionary .\n",
        "    '''\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1 , self.n_layers):\n",
        "      H = self.cache[\"H\" + str(l - 1)]\n",
        "      # print(H.shape)\n",
        "      W = self.parameters[\"W\" + str(l)]\n",
        "      b = self.parameters[\"b\" + str(l)]\n",
        "      A = np.dot(W , H) + b\n",
        "      # print(W.shape)\n",
        "      # print(b.shape)\n",
        "      self.cache[\"A\" + str(l)] = A\n",
        "      if activation == 'sigmoid':\n",
        "         H = ActivationFunction.sigmoid(A)\n",
        "      elif activation == 'relu':\n",
        "         H = ActivationFunction.Relu(A)\n",
        "      elif activation == 'tanh':\n",
        "         H = ActivationFunction.tanh(A)\n",
        "      self.cache[\"H\" + str(l)] = H\n",
        "    yPredicted = self.output(self.cache[\"A\" + str(self.n_layers - 1)])\n",
        "    return yPredicted\n",
        "\n",
        "  def predict(self,input):\n",
        "    '''\n",
        "    predicts the class of the image , although result is not int , it is probability\n",
        "    '''\n",
        "    H = input\n",
        "    activation = self.activation_function\n",
        "    for l in range(1 , self.n_layers - 1):\n",
        "      # print(H.shape)\n",
        "      W = self.parameters[\"W\" + str(l)]\n",
        "      b = self.parameters[\"b\" + str(l)]\n",
        "      A = np.dot(W , H) + b\n",
        "      # print(W.shape)\n",
        "      # print(b.shape)\n",
        "      if activation == 'sigmoid':\n",
        "         H = ActivationFunction.sigmoid(A)\n",
        "      elif activation == 'relu':\n",
        "         H = ActivationFunction.Relu(A)\n",
        "      elif activation == 'tanh':\n",
        "         H = ActivationFunction.tanh(A)\n",
        "\n",
        "\n",
        "    W = self.parameters[\"W\" + str(self.n_layers - 1)]\n",
        "    b = self.parameters[\"b\" + str(self.n_layers - 1)]\n",
        "    A = np.dot(W , H) + b\n",
        "    y_predicted = self.output(A)\n",
        "    return y_predicted\n",
        "\n",
        "NN = NeuralNetwork()\n",
        "\n",
        "print(NN.predict(x_trainT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gzb7eJ0k3X4W"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhYfErKwx7OU",
        "outputId": "34cf77d4-1ef7-4092-a4ba-35c5a03b7451"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.10013348 0.00676222 0.02234989 ... 0.0397687  0.02069737 0.00749427]\n",
            " [0.10013738 0.15123605 0.39413149 ... 0.03664666 0.19659424 0.30401039]\n",
            " [0.09979417 0.0482745  0.00955078 ... 0.05159459 0.00698927 0.05777833]\n",
            " ...\n",
            " [0.09991326 0.14251926 0.0278301  ... 0.28297623 0.12459664 0.03887115]\n",
            " [0.10009272 0.04417972 0.182001   ... 0.04951272 0.13784489 0.06064374]\n",
            " [0.10002165 0.07634184 0.08359702 ... 0.05792878 0.07231635 0.08949334]]\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "(train_input, train_output), (test_input, test_output) = fashion_mnist.load_data()\n",
        "train_input = train_input / 255.0\n",
        "test_input = test_input / 255.0\n",
        "\n",
        "# Assuming 'data' is your dataset and 'labels' are corresponding labels/targets\n",
        "train_input, test_input, train_output, test_output = train_test_split(\n",
        "    train_input, train_output, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_inputT = np.transpose(train_input.reshape(train_input.shape[0], -1))\n",
        "\n",
        "class Activation:\n",
        "\n",
        "    def sigmoid(x):\n",
        "        x = np.clip(x, -500, 500)\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(x):\n",
        "        exp_values = np.exp(x - np.max(x))\n",
        "        return exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
        "\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "\n",
        "class Neural_Network:\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers=1,\n",
        "        hidden_layer_nodes=4,\n",
        "        activation_function=\"sigmoid\",\n",
        "        x_train=train_inputT,\n",
        "        y_train=train_output,\n",
        "        x_test=test_input,\n",
        "        y_test=test_output,\n",
        "    ):\n",
        "        self.layers = layers\n",
        "        self.hidden_layer_nodes = hidden_layer_nodes\n",
        "        self.activation_function = activation_function\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        self.decode = {\"h0\": x_train}\n",
        "        self.decode[\"A0\"] = x_train\n",
        "        self.n_input = x_train.shape[0]\n",
        "        self.n_output = 10  # Assuming you have 10 classes in Fashion MNIST\n",
        "        self.nodes_in_level = [self.n_input]  # Initialize with input layer nodes\n",
        "        self.thetas = {}\n",
        "        for l in range(1, self.layers + 2):\n",
        "            self.nodes_in_level.append(hidden_layer_nodes) if l < self.layers + 1 else self.nodes_in_level.append(\n",
        "                self.n_output\n",
        "            )\n",
        "            self.thetas[\"W\" + str(l)] = np.random.randn(\n",
        "                self.nodes_in_level[l], self.nodes_in_level[l - 1]\n",
        "            )\n",
        "            self.thetas[\"b\" + str(l)] = np.zeros((self.nodes_in_level[l], 1))\n",
        "\n",
        "    def forward_propagation(self):\n",
        "        for k in range(1, self.layers + 2):\n",
        "            W = self.thetas[\"W\" + str(k)]\n",
        "            b = self.thetas[\"b\" + str(k)]\n",
        "            h = self.decode[\"h\" + str(k - 1)]\n",
        "            A = b + np.dot(W, h)\n",
        "            if self.activation_function == \"sigmoid\":\n",
        "                h = Activation.sigmoid(A)\n",
        "            elif self.activation_function == \"tanh\":\n",
        "                h = Activation.tanh(A)\n",
        "            self.decode[\"h\" + str(k)] = h\n",
        "            self.decode[\"A\" + str(k)] = A\n",
        "\n",
        "        y_hat = Activation.softmax(self.decode[\"A\" + str(self.layers + 1)])\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "neural_network = Neural_Network()\n",
        "result = neural_network.forward_propagation()\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "so5OWDZD3Yjl"
      },
      "outputs": [],
      "source": [
        "#question 3\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "(train_input, train_output), (test_input, test_output) = fashion_mnist.load_data()\n",
        "\n",
        "train_input = train_input / 255.0\n",
        "test_input = test_input / 255.0\n",
        "\n",
        "# Assuming 'data' is your dataset and 'labels' are corresponding labels/targets\n",
        "train_input, test_input, train_output, test_output = train_test_split(\n",
        "    train_input, train_output, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_inputT = np.transpose(train_input.reshape(train_input.shape[0], -1))\n",
        "\n",
        "class Activation:\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        # Use np.clip to prevent overflow issues\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        exp_values = np.exp(x - np.max(x))\n",
        "        return exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "\n",
        "\n",
        "class Neural_Network:\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers=10,\n",
        "        hidden_layer_nodes=8,\n",
        "        activation_function=\"sigmoid\",\n",
        "        x_train=train_inputT,\n",
        "        y_train=train_output,\n",
        "        x_test=test_input,\n",
        "        y_test=test_output,\n",
        "    ):\n",
        "        self.layers = layers\n",
        "        self.hidden_layer_nodes = hidden_layer_nodes\n",
        "        self.activation_function = activation_function\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        self.decode = {\"h0\": x_train}\n",
        "        self.decode[\"A0\"] = x_train\n",
        "        self.n_input = x_train.shape[0]\n",
        "        self.n_output = 10  # Assuming you have 10 classes in Fashion MNIST\n",
        "        self.nodes_in_level = [self.n_input]  # Initialize with input layer nodes\n",
        "        self.thetas = {}\n",
        "        self.derivatives={}\n",
        "        self.previous_updates={}\n",
        "        for l in range(1,self.layers+2):\n",
        "            self.previous_updates[\"W\"+str(l)]=np.zeros(self.nodes_in_level[l], self.nodes_in_level[l - 1])\n",
        "            self.previous_updates[\"b\"+str(l)]=np.zeros(self.nodes_in_level[l],1)\n",
        "        self.y_hat=[]\n",
        "        for l in range(1, self.layers + 2):\n",
        "            self.nodes_in_level.append(hidden_layer_nodes) if l < self.layers + 1 else self.nodes_in_level.append(\n",
        "                self.n_output\n",
        "            )\n",
        "            self.thetas[\"W\" + str(l)] = np.random.randn(\n",
        "                self.nodes_in_level[l], self.nodes_in_level[l - 1]\n",
        "            )\n",
        "            self.thetas[\"b\" + str(l)] = np.zeros((self.nodes_in_level[l], 1))\n",
        "            # printf(self.thetas[\"W1\"])\n",
        "\n",
        "    def forward_propagation(self):\n",
        "        for k in range(1, self.layers +  2):\n",
        "            W = self.thetas[\"W\" + str(k)]\n",
        "            b = self.thetas[\"b\" + str(k)]\n",
        "            h = self.decode[\"h\" + str(k - 1)]\n",
        "            A = b + np.dot(W, h)\n",
        "            # print(\"dim of A = \",A.shape)\n",
        "            if self.activation_function == \"sigmoid\":\n",
        "                h = Activation.sigmoid(A)\n",
        "            elif self.activation_function == \"tanh\":\n",
        "                h = Activation.tanh(A)\n",
        "            # print(\"printing h in\",k)\n",
        "            # print(h)\n",
        "            self.decode[\"h\" + str(k)] = h\n",
        "            self.decode[\"A\" + str(k)] = A\n",
        "\n",
        "        self.y_hat = Activation.softmax(self.decode[\"A\" + str(self.layers + 1)])\n",
        "        return self.y_hat\n",
        "\n",
        "\n",
        "    def backward_propagation(self):\n",
        "        e_l = np.transpose(np.eye(10)[self.y_train[:]])\n",
        "        # print(\"dim of e_l\",e_l.shape)\n",
        "        # print(\"dim of y_hat\",self.y_hat.shape)\n",
        "        dA = (self.y_hat - e_l)\n",
        "        # print(\"dim of dA\", dA.shape)\n",
        "        self.derivatives[\"dA\" + str(self.layers + 1)] = dA\n",
        "        for k in range(self.layers +1, 0, -1):\n",
        "            dA = self.derivatives[\"dA\" + str(k)]\n",
        "            h_prev = self.decode[\"h\" + str(k - 1)]\n",
        "            A_prev = self.decode[\"A\" + str(k - 1)]\n",
        "            W = self.thetas[\"W\" + str(k)]\n",
        "            dW = np.zeros(W.shape)\n",
        "            db = np.zeros((W.shape[0],1))\n",
        "            dh_prev = np.zeros(h_prev.shape)\n",
        "            dA_prev = np.zeros(A_prev.shape)\n",
        "            dW = np.dot(dA, h_prev.T)\n",
        "            self.derivatives[\"dW\" + str(k)] = dW\n",
        "            db = np.sum(dA, axis=1, keepdims=True)\n",
        "            self.derivatives[\"db\" + str(k)] = db\n",
        "            if k >= 2:\n",
        "                dh_prev = np.matmul(W.T,dA)\n",
        "                if self.activation_function == \"sigmoid\":\n",
        "                    dA_prev = np.multiply(dh_prev, (Activation.sigmoid(A_prev) * (1 - Activation.sigmoid(A_prev))))\n",
        "            self.derivatives[\"dA\" + str(k - 1)] = dA_prev\n",
        "\n",
        "    def do_mgd(self,eta,beta):\n",
        "        for l in range(1,self.layers+2):\n",
        "            self.previous_updates[\"W\"+str(l)]=beta*self.previous_updates[\"W\"+str(l)]+self.derivatives[\"dW\"+str(l)]\n",
        "            self.previous_updates[\"b\"+str(l)]=beta*self.previous_updates[\"b\"+str(l)]+self.derivatives[\"db\"+str(l)]\n",
        "            self.thetas[\"W\"+str(l)]=self.thetas[\"W\"+str(l)]-eta*self.previous_updates[\"W\"+str(l)]\n",
        "            self.thetas[\"b\"+str(l)]=self.thetas[\"b\"+str(l)]-eta*self.previous_updates[\"b\"+str(l)]\n",
        "\n",
        "\n",
        "\n",
        "    def compute(self):\n",
        "        epoch = 100\n",
        "        eta = 0.01\n",
        "        for iter in range(epoch):\n",
        "            self.forward_propagation()\n",
        "            self.backward_propagation()\n",
        "            # Update weights and biases\n",
        "            for l in range(1,self.layers):\n",
        "                W = self.thetas[\"W\" + str(l)]\n",
        "                dW = self.derivatives[\"dW\" + str(l)]\n",
        "                W = W - eta * dW\n",
        "                # print(eta*dW)\n",
        "                self.thetas[\"W\" + str(l)] = W\n",
        "                b = self.thetas[\"b\" + str(l)]\n",
        "                db = self.derivatives[\"db\" + str(l)]\n",
        "                b = b - eta * db\n",
        "                self.thetas[\"b\" + str(l)] = b\n",
        "            if iter >=0 :\n",
        "                OneHotOfTrueOutput = np.transpose(np.eye(10)[self.y_train[:]])\n",
        "                sum = -np.sum(OneHotOfTrueOutput * np.log(self.y_hat), axis=0)\n",
        "                print(sum)\n",
        "\n",
        "\n",
        "\n",
        "neural_network = Neural_Network()\n",
        "neural_network.compute()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MCZ5quDV-9Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a66ea33b-c750-4c57-ffaa-0bfac2ec59af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.41.0-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.41.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabhirupmjdr2\u001b[0m (\u001b[33mabhirupmjdr_dl\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240309_093317-06i0jnd6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abhirupmjdr_dl/deep-learning/runs/06i0jnd6' target=\"_blank\">atomic-rain-270</a></strong> to <a href='https://wandb.ai/abhirupmjdr_dl/deep-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abhirupmjdr_dl/deep-learning' target=\"_blank\">https://wandb.ai/abhirupmjdr_dl/deep-learning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abhirupmjdr_dl/deep-learning/runs/06i0jnd6' target=\"_blank\">https://wandb.ai/abhirupmjdr_dl/deep-learning/runs/06i0jnd6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "29515/29515 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26421880/26421880 [==============================] - 2s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "5148/5148 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4422102/4422102 [==============================] - 1s 0us/step\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "#question 1\n",
        "!pip install wandb\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "wandb.login()\n",
        "wandb.init(\n",
        "      project=\"deep-learning\",\n",
        ")\n",
        "\n",
        "(train_input, train_output), (test_input,test_output) = fashion_mnist.load_data()\n",
        "\n",
        "class_names = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\"\n",
        "}\n",
        "\n",
        "# Plot one sample image for each class\n",
        "class_counts = np.bincount(train_output)\n",
        "total=len(train_output)\n",
        "for i in range(len(class_names)):\n",
        "    # Find the first image with the corresponding class label\n",
        "    idx=np.where(train_output==i)[0][0]\n",
        "    # plotting the image\n",
        "    plt.subplot(2,5,i+1)\n",
        "    plt.imshow(train_input[idx],cmap='gray')\n",
        "    plt.title(class_names[i])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "wandb.log({\"Sample Images\": plt})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5lGVSyLWPGv",
        "outputId": "f0fb6e0f-8bf1-43b4-894b-e02a77a93d23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.17189836e-04, 7.92382105e-01, 2.68383330e-02, ...,\n",
              "        5.61381915e-01, 6.14603922e-01, 9.85917766e-04],\n",
              "       [8.80643725e-04, 9.34197738e-05, 6.21725357e-04, ...,\n",
              "        1.41268356e-03, 1.77846488e-05, 3.10867992e-04],\n",
              "       [1.40592255e-05, 2.72622768e-04, 2.73089385e-03, ...,\n",
              "        1.04240689e-04, 6.99702004e-06, 1.22765073e-04],\n",
              "       ...,\n",
              "       [6.48656450e-03, 9.71615584e-03, 9.10872740e-03, ...,\n",
              "        1.20357346e-02, 9.66029082e-03, 2.45277330e-04],\n",
              "       [7.31659850e-01, 6.71746791e-02, 2.83928657e-01, ...,\n",
              "        1.80278600e-01, 1.37713602e-02, 9.03969312e-03],\n",
              "       [2.58008213e-01, 1.24365640e-01, 6.51342471e-01, ...,\n",
              "        2.34246496e-01, 3.59600689e-01, 9.89132354e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#question 2\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Reshape the data for the neural network\n",
        "x_val_T = np.transpose(x_val.reshape(x_val.shape[0], -1))\n",
        "x_train_T = np.transpose(x_train.reshape(x_train.shape[0], -1))\n",
        "x_test_T = np.transpose(x_test.reshape(x_test.shape[0], -1))\n",
        "y_train_T = y_train.reshape(1, -1)\n",
        "y_val_T = y_val.reshape(1, -1)\n",
        "y_test_T = y_test.reshape(1, -1)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, init_mode='random', num_hidden_layers=3, neurons_per_hidden_layer=64,\n",
        "                 activation_function='sigmoid', train_input=x_train_T, train_output=y_train_T, val_input=x_val_T,\n",
        "                 val_output=y_val_T):\n",
        "        self.init_mode = init_mode\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.neurons_per_hidden_layer = neurons_per_hidden_layer\n",
        "        self.activation_function = activation_function\n",
        "        self.train_input = train_input\n",
        "        self.train_output = train_output\n",
        "        self.val_input = val_input\n",
        "        self.val_output = val_output\n",
        "        self.n_layers = num_hidden_layers + 2\n",
        "        self.n_input = train_input.shape[0]\n",
        "        self.n_output = np.max(train_output) + 1\n",
        "        self.n_neurons = [self.n_input] + [neurons_per_hidden_layer] * num_hidden_layers + [self.n_output]\n",
        "        self.cache = {\"H0\": train_input, \"A0\": train_input}\n",
        "        self.theta = {}\n",
        "        self.grads = {}\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        for l in range(1, self.n_layers):\n",
        "            if self.init_mode == \"random\":\n",
        "                self.theta[f\"W{l}\"] = np.random.randn(self.n_neurons[l], self.n_neurons[l - 1])\n",
        "            self.theta[f\"b{l}\"] = np.zeros((self.n_neurons[l], 1))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(x, 0)\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def feedforward(self, input_data):\n",
        "        H = input_data\n",
        "        activation = self.activation_function\n",
        "        for l in range(1, self.n_layers - 1):\n",
        "            W = self.theta[f\"W{l}\"]\n",
        "            b = self.theta[f\"b{l}\"]\n",
        "            A = np.dot(W, H) + b\n",
        "            H = getattr(self, activation)(A)\n",
        "        W = self.theta[f\"W{self.n_layers - 1}\"]\n",
        "        b = self.theta[f\"b{self.n_layers - 1}\"]\n",
        "        A = np.dot(W, H) + b\n",
        "        y_predicted = np.exp(A) / np.sum(np.exp(A), axis=0)\n",
        "        return y_predicted\n",
        "\n",
        "# Create an instance of the NeuralNetwork class\n",
        "my_model = NeuralNetwork()\n",
        "\n",
        "# Perform a feedforward pass on the training data\n",
        "my_model.feedforward(x_train_T)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#question 2\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Reshape the data for the neural network\n",
        "x_val_T = np.transpose(x_val.reshape(x_val.shape[0], -1))\n",
        "x_train_T = np.transpose(x_train.reshape(x_train.shape[0], -1))\n",
        "x_test_T = np.transpose(x_test.reshape(x_test.shape[0], -1))\n",
        "y_train_T = y_train.reshape(1, -1)\n",
        "y_val_T = y_val.reshape(1, -1)\n",
        "y_test_T = y_test.reshape(1, -1)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, init_mode='random', num_hidden_layers=3, neurons_per_hidden_layer=64,\n",
        "                 activation_function='sigmoid', train_input=x_train_T, train_output=y_train_T, val_input=x_val_T,\n",
        "                 val_output=y_val_T):\n",
        "        self.init_mode = init_mode\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.neurons_per_hidden_layer = neurons_per_hidden_layer\n",
        "        self.activation_function = activation_function\n",
        "        self.train_input = train_input\n",
        "        self.train_output = train_output\n",
        "        self.val_input = val_input\n",
        "        self.val_output = val_output\n",
        "        self.n_layers = num_hidden_layers + 2\n",
        "        self.n_input = train_input.shape[0]\n",
        "        self.n_output = np.max(train_output) + 1\n",
        "        self.n_neurons = [self.n_input] + [neurons_per_hidden_layer] * num_hidden_layers + [self.n_output]\n",
        "        self.cache = {\"H0\": train_input, \"A0\": train_input}\n",
        "        self.theta = {}\n",
        "        self.grads = {}\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        for l in range(1, self.n_layers):\n",
        "            if self.init_mode == \"random\":\n",
        "                self.theta[f\"W{l}\"] = np.random.randn(self.n_neurons[l], self.n_neurons[l - 1])\n",
        "            self.theta[f\"b{l}\"] = np.zeros((self.n_neurons[l], 1))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(x, 0)\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def feedforward(self, input_data):\n",
        "        H = input_data\n",
        "        activation = self.activation_function\n",
        "        for layer in range(1, self.n_layers - 1):\n",
        "            weights = self.theta[f\"W{layer}\"]\n",
        "            bias = self.theta[f\"b{layer}\"]\n",
        "            activation_input = np.dot(weights, H) + bias\n",
        "            H = getattr(self, activation)(activation_input)\n",
        "        weights = self.theta[f\"W{self.n_layers - 1}\"]\n",
        "        bias = self.theta[f\"b{self.n_layers - 1}\"]\n",
        "        activation_input = np.dot(weights, H) + bias\n",
        "        softmax_output = np.exp(activation_input) / np.sum(np.exp(activation_input), axis=0)\n",
        "        return softmax_output\n",
        "\n",
        "# Create an instance of the NeuralNetwork class\n",
        "my_model = NeuralNetwork()\n",
        "\n",
        "# Perform a feedforward pass on the training data\n",
        "my_model.feedforward(x_train_T)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsNfcNxYQNSC",
        "outputId": "7bdcc1f2-fdff-4749-f9c9-6328df6f2542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.35965753e-04, 2.49221314e-03, 2.22181714e-03, ...,\n",
              "        2.59736652e-01, 9.20369685e-04, 3.06940101e-01],\n",
              "       [9.45407402e-01, 9.67868449e-01, 9.91186892e-01, ...,\n",
              "        4.97157722e-01, 9.67851250e-01, 6.42201677e-01],\n",
              "       [1.51041597e-08, 1.04321415e-06, 2.04764479e-07, ...,\n",
              "        1.13048466e-06, 9.79920523e-09, 2.49051787e-05],\n",
              "       ...,\n",
              "       [4.82273698e-02, 2.72474901e-02, 3.18747684e-03, ...,\n",
              "        1.92843696e-01, 2.60612753e-03, 2.20670951e-02],\n",
              "       [1.07825970e-06, 6.03195039e-09, 1.89584315e-08, ...,\n",
              "        6.95255501e-06, 1.98360661e-07, 3.10788388e-07],\n",
              "       [4.57009762e-05, 1.32943321e-06, 9.80227918e-06, ...,\n",
              "        3.04087786e-04, 3.88490630e-06, 9.79691384e-05]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K33zpd9y716C"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install wandb\n",
        "\n",
        "# import wandb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# wandb.login()\n",
        "\n",
        "# wandb.init(\n",
        "#     # set the wandb project where this run will be logged\n",
        "#     project=\"deep-learning\",\n",
        "\n",
        "#     # track hyperparameters and run metadata\n",
        "#     config={\n",
        "#     \"eta\": 0.02\n",
        "#     }\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.datasets import m\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "'''normalizing the data'''\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "x_train_T = x_train.reshape(-1, x_train.shape[1]*x_train.shape[2]).T\n",
        "x_val_T = x_val.reshape(-1, x_val.shape[1]*x_val.shape[2]).T\n",
        "x_test_T = x_test.reshape(-1, x_test.shape[1]*x_test.shape[2]).T\n",
        "y_train_T, y_val_T, y_test_T = y_train.reshape(1, -1), y_val.reshape(1, -1), y_test.reshape(1, -1)\n",
        "\n",
        "\n",
        "\n",
        "class Compute:\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return  1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu(x):\n",
        "        return np.maximum(x,0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(x):\n",
        "        return Compute.softmax(x) * (1-Compute.softmax(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(Z):\n",
        "        s = Compute.sigmoid(Z)\n",
        "        dA = s * (1 - s)\n",
        "        return dA\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu_derivative(x):\n",
        "        return 1*(x > 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(x):\n",
        "        return (1 - (np.tanh(x)**2))\n",
        "\n",
        "\n",
        "class Update:\n",
        "    @staticmethod\n",
        "    def simple_gradient_descent(eta):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            W -= eta * dW\n",
        "            b -= eta * db\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss):\n",
        "        theta = {}\n",
        "        input_data = my_network.TrainInput[:, i:i + batch_size]\n",
        "        output_data = my_network.TrainOutput[0, i:i + batch_size]\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            theta[\"W\" + str(l)] = my_network.theta[\"W\" + str(l)] - beta * previous_updates[\"W\" + str(l)]\n",
        "            theta[\"b\" + str(l)] = my_network.theta[\"b\" + str(l)] - beta * previous_updates[\"b\" + str(l)]\n",
        "        y_predicted = my_network.forward(input_data, my_network.activation_function, my_network.theta)\n",
        "        e_y = np.transpose(np.eye(my_network.n_output)[output_data])\n",
        "        my_network.backpropagation(y_predicted, e_y, batch_size, loss, my_network.activation_function, my_network.theta)\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + my_network.grads[\"dW\" + str(l)]\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + my_network.grads[\"db\" + str(l)]\n",
        "            my_network.theta[\"W\" + str(l)] -= eta * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= eta * my_network.grads[\"db\" + str(l)]\n",
        "        return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def momentum_gradient_descent(eta, beta, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            uW, ub = previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)]\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            uW = beta * uW + dW\n",
        "            ub = beta * ub + db\n",
        "            W -= eta * uW\n",
        "            b -= eta * ub\n",
        "            previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)] = uW, ub\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def rms_prop(eta, beta, epsilon, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"dW\" + str(l)])\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"db\" + str(l)])\n",
        "            factorW = eta / (np.sqrt(previous_updates[\"W\" + str(l)] + epsilon))\n",
        "            factorb = eta / (np.sqrt(previous_updates[\"b\" + str(l)] + epsilon))\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * my_network.grads[\"db\" + str(l)]\n",
        "\n",
        "    @staticmethod\n",
        "    def nadam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            term1 = 1 - (beta1 ** (t))\n",
        "            term2 = (1 - beta1) * my_network.grads[\"dW\" + str(l)] / term1\n",
        "            term3 = (1 - beta1) * my_network.grads[\"db\" + str(l)] / term1\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * (beta1 * MW_corrected + term2)\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * (beta1 * Mb_corrected + term3)\n",
        "        return M, V, t + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def adam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** t))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** t))\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** t))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** t))\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * MW_corrected\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * Mb_corrected\n",
        "        return M, V, t + 1\n",
        "\n",
        "\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  theta = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "\n",
        "\n",
        "  def __init__(self,mode_of_initialization=\"random\",number_of_hidden_layers=6,num_neurons_in_hidden_layers=32,activation=\"sigmoid\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T):\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.theta[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.theta[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.theta[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def forward(self, X, activation, theta):\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1, self.n_layers):\n",
        "        H = self.cache[\"H\" + str(l - 1)]\n",
        "        W = self.theta[\"W\" + str(l)]\n",
        "        b = self.theta[\"b\" + str(l)]\n",
        "        A = np.dot(W, H) + b\n",
        "        self.cache[\"A\" + str(l)] = A\n",
        "        H = self.apply_activation(A, activation)\n",
        "        self.cache[\"H\" + str(l)] = H\n",
        "    Al = self.cache[\"A\" + str(self.n_layers - 1)]\n",
        "    y_hat= Compute.softmax(Al)\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "  def apply_activation(self, A, activation):\n",
        "        if activation == 'sigmoid':\n",
        "            return Compute.sigmoid(A)\n",
        "        elif activation == 'relu':\n",
        "            return Compute.Relu(A)\n",
        "        elif activation == 'tanh':\n",
        "            return Compute.tanh(A)\n",
        "\n",
        "  def loss(self, input, true_output, predicted_output, loss, batch_size):\n",
        "    if loss == 'cross_entropy':\n",
        "        one_hot_true_output = np.eye(self.n_output)[true_output[0]].T\n",
        "        loss_value = -np.sum(one_hot_true_output * np.log(predicted_output + 1e-9)) / batch_size\n",
        "    return loss_value\n",
        "\n",
        "  def accuracy(self, input, true_output, predicted_output):\n",
        "    predicted_labels = np.argmax(predicted_output, axis=0)\n",
        "    correct_predictions = np.sum(true_output == predicted_labels)\n",
        "    total_samples = true_output.shape[1]\n",
        "    accuracy_percentage = (correct_predictions / total_samples) * 100\n",
        "    return accuracy_percentage\n",
        "\n",
        "  def backpropagation(self, y_predicted, e_y, batch_size, loss, activation, theta):\n",
        "        if loss == 'cross_entropy':\n",
        "            dA = y_predicted - e_y\n",
        "        m = dA.shape[1]\n",
        "        self.grads[\"dA\" + str(self.n_layers - 1)] = dA\n",
        "\n",
        "        for k in range(self.n_layers - 1, 0, -1):\n",
        "            dA = self.grads[\"dA\" + str(k)]\n",
        "            H_prev = self.cache[\"H\" + str(k - 1)]\n",
        "            A_prev = self.cache[\"A\" + str(k - 1)]\n",
        "            W = self.theta[\"W\" + str(k)]\n",
        "\n",
        "            dW, db, dH_prev, dA_prev = self.calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size)\n",
        "\n",
        "            self.grads[\"dA\" + str(k - 1)] = dA_prev\n",
        "            self.grads[\"dW\" + str(k)] = dW\n",
        "            self.grads[\"db\" + str(k)] = db\n",
        "\n",
        "        return\n",
        "\n",
        "  def calculate_gradients(self, k, dA, H_prev, A_prev, W, activation, batch_size):\n",
        "        dW = self.calculate_dW(dA, H_prev, batch_size)\n",
        "        db = self.calculate_db(dA, batch_size)\n",
        "        dH_prev, dA_prev = self.calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev) if k > 1 else (np.zeros(H_prev.shape), np.zeros(A_prev.shape))\n",
        "\n",
        "        return dW, db, dH_prev, dA_prev\n",
        "\n",
        "  def calculate_dW(self, dA, H_prev, batch_size):\n",
        "        return np.dot(dA, H_prev.T) / batch_size\n",
        "\n",
        "  def calculate_db(self, dA, batch_size):\n",
        "        return np.sum(dA, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "  def calculate_dH_prev_dA_prev(self, k, W, dA, activation, A_prev):\n",
        "        dH_prev = np.matmul(W.T, dA)\n",
        "        dA_prev = self.calculate_dA_prev(dH_prev, activation, A_prev)\n",
        "        return dH_prev, dA_prev\n",
        "\n",
        "  def calculate_dA_prev(self, dH_prev, activation, A_prev):\n",
        "        if activation == 'sigmoid':\n",
        "            return dH_prev * Compute.sigmoid_derivative(A_prev)\n",
        "        elif activation == 'tanh':\n",
        "            return dH_prev * Compute.tanh_derivative(A_prev)\n",
        "        elif activation == 'relu':\n",
        "            return dH_prev * Compute.Relu_derivative(A_prev)\n",
        "\n",
        "\n",
        "  def compute(self, eta = 0.001,beta = 0.9,beta1 = 0.9,beta2 = 0.999 ,epsilon = 1e-6, optimizer = 'sgd',batch_size = 100,loss = 'cross_entropy',epochs = 20):\n",
        "    train_c_epoch, tarin_acc_per_epoch, val_c_per_epoch, val_acc_per_epoch, previous_updates, M, V = [], [], [], [], {}, {}, {}\n",
        "    for l in range(1 , self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      previous_updates[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    for l in range(1 , self.n_layers):\n",
        "      M[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      M[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "      V[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      V[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    t = 1\n",
        "    for count in range(epochs):\n",
        "      for i in range(0 , self.TrainInput.shape[1],batch_size):\n",
        "        if i + batch_size > self.TrainInput.shape[1]:\n",
        "          continue\n",
        "        theta = self.theta\n",
        "        yPredicted = self.forward(self.TrainInput[:,i:i + batch_size],self.activation_function,theta)\n",
        "        e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])\n",
        "        self.backpropagation(yPredicted,e_y,batch_size,loss,self.activation_function,theta)\n",
        "        if optimizer == 'sgd':   #referred slide page 54\n",
        "            Update.simple_gradient_descent(eta)\n",
        "        elif optimizer == 'nag':\n",
        "            previous_updates=Update.nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss)\n",
        "\n",
        "        elif optimizer == 'momentum': #referred from slide 43\n",
        "          Update.momentum_gradient_descent(eta,beta,previous_updates)\n",
        "\n",
        "        elif optimizer == 'RMSprop':\n",
        "          Update.rms_prop(eta,beta,epsilon,previous_updates)\n",
        "        elif optimizer == 'adam':\n",
        "          epsilon = 1e-8\n",
        "          M , V , t= Update.adam(eta,beta1,beta2,epsilon,M , V , t)\n",
        "        elif optimizer == 'nadam':\n",
        "          epsilon = 1e-8\n",
        "          M , V , t= Update.nadam(eta,beta1,beta2,epsilon,M , V , t)\n",
        "\n",
        "      y_hat = self.forward(self.TrainInput,self.activation_function,self.theta)\n",
        "      valy_hat = self.forward(self.ValInput,self.activation_function,self.theta)\n",
        "      train_cost = self.loss(self.TrainInput,self.TrainOutput,y_hat,loss,self.TrainInput.shape[1])\n",
        "      train_c_epoch.append(train_cost)\n",
        "      val_cost = self.loss(self.ValInput,self.ValOutput,valy_hat,loss,self.ValInput.shape[1])\n",
        "      val_c_per_epoch.append(val_cost)\n",
        "      train_acc = self.accuracy(self.TrainInput, self.TrainOutput,y_hat)\n",
        "      tarin_acc_per_epoch.append(train_acc)\n",
        "\n",
        "      val_acc = self.accuracy(self.ValInput, self.ValOutput,valy_hat)\n",
        "      val_acc_per_epoch.append(val_acc)\n",
        "\n",
        "    #   if count==epochs-1:\n",
        "        # conf_data=confusion_matrix(self.ValOutput[0], valy_hat[0])\n",
        "        # seaborn.heatmap(conf_data, annot=True, cmap='Blues', fmt='g')\n",
        "        # plt.xlabel('Predicted labels')\n",
        "        # plt.ylabel('True labels')\n",
        "        # plt.title('Confusion Matrix')\n",
        "        # plt.show()\n",
        "\n",
        "    #   print(\"---------\"*20)\n",
        "    #   print(f\"Epoch Number = {format(count+1)}\")\n",
        "    #   print(f\"Training Accuracy = {format(tarin_acc_per_epoch[-1])}\")\n",
        "    #   print(f\"Validation Accuracy = {format(val_acc_per_epoch[-1])}\")\n",
        "      # wandb.log({\"training_accuracy\": train_acc,\"validation_accuracy\": val_acc,\"training_loss\":train_cost,\"validation_loss\": val_cost,\"epoch\": count})\n",
        "    return train_c_epoch,tarin_acc_per_epoch,val_c_per_epoch,val_acc_per_epoch\n",
        "\n",
        "\n",
        "my_network = MyNeuralNetwork()\n",
        "my_network.compute(eta = 0.01, beta = 0.9 , beta1 = 0.9 , beta2 = 0.999 , epsilon = 0.001, optimizer = 'sgd' , batch_size = 16 , loss = 'cross_entropy' , epochs = 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAS--NwnqTa5"
      },
      "outputs": [],
      "source": [
        "\n",
        "# !pip install wandb\n",
        "\n",
        "# import wandb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score,ConfusionMatrixDisplay\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# wandb.login()\n",
        "\n",
        "# wandb.init(\n",
        "#     # set the wandb project where this run will be logged\n",
        "#     project=\"deep-learning\",\n",
        "\n",
        "#     # track hyperparameters and run metadata\n",
        "#     config={\n",
        "#     \"eta\": 0.02\n",
        "#     }\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "'''normalizing the data'''\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "x_train_T = x_train.reshape(-1, x_train.shape[1]*x_train.shape[2]).T\n",
        "x_val_T = x_val.reshape(-1, x_val.shape[1]*x_val.shape[2]).T\n",
        "x_test_T = x_test.reshape(-1, x_test.shape[1]*x_test.shape[2]).T\n",
        "y_train_T, y_val_T, y_test_T = y_train.reshape(1, -1), y_val.reshape(1, -1), y_test.reshape(1, -1)\n",
        "\n",
        "class Util:\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_activation(A, activation):\n",
        "            if activation == 'sigmoid':\n",
        "                return Compute.sigmoid(A)\n",
        "            elif activation == 'relu':\n",
        "                return Compute.Relu(A)\n",
        "            elif activation == 'tanh':\n",
        "                return Compute.tanh(A)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(input, true_output, predicted_output, loss, batch_size):\n",
        "        if loss == 'cross_entropy':\n",
        "            one_hot_true_output = np.eye(my_network.n_output)[true_output[0]].T\n",
        "            return -np.sum(one_hot_true_output * np.log(predicted_output + 1e-9)) / batch_size\n",
        "\n",
        "\n",
        "        if loss=='squared_loss':\n",
        "            one_hot_true_output = np.eye(my_network.n_output)[true_output[0]].T\n",
        "            loss_factor=np.square(predicted_output-one_hot_true_output)\n",
        "            return np.sum(loss_factor)/batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(input, true_output, predicted_output):\n",
        "        predicted_labels = np.argmax(predicted_output, axis=0)\n",
        "        correct_predictions = np.sum(true_output == predicted_labels)\n",
        "        total_samples = true_output.shape[1]\n",
        "        accuracy_percentage = (correct_predictions / total_samples) * 100\n",
        "        return accuracy_percentage\n",
        "\n",
        "class Compute:\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return  1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu(x):\n",
        "        return np.maximum(x,0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return (2 * Compute.sigmoid(2 * x)) - 1\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(x):\n",
        "        return Compute.softmax(x) * (1-Compute.softmax(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(Z):\n",
        "        s = Compute.sigmoid(Z)\n",
        "        dA = s * (1 - s)\n",
        "        return dA\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu_derivative(x):\n",
        "        return 1*(x > 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(x):\n",
        "        return (1 - (Compute.tanh(x)**2))\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size):\n",
        "            dW = Compute.calculate_dW(dA, H_prev, batch_size)\n",
        "            db = Compute.calculate_db(dA, batch_size)\n",
        "            dH_prev, dA_prev = Compute.calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev) if k > 1 else (np.zeros(H_prev.shape), np.zeros(A_prev.shape))\n",
        "\n",
        "            return dW, db, dH_prev, dA_prev\n",
        "    @staticmethod\n",
        "    def calculate_dW(dA, H_prev, batch_size):\n",
        "            return np.dot(dA, H_prev.T) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_db(dA, batch_size):\n",
        "            return np.sum(dA, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev):\n",
        "            dH_prev = np.matmul(W.T, dA)\n",
        "            dA_prev = Compute.calculate_dA_prev(dH_prev, activation, A_prev)\n",
        "            return dH_prev, dA_prev\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dA_prev(dH_prev, activation, A_prev):\n",
        "            if activation == 'sigmoid':\n",
        "                return dH_prev * Compute.sigmoid_derivative(A_prev)\n",
        "            elif activation == 'tanh':\n",
        "                return dH_prev * Compute.tanh_derivative(A_prev)\n",
        "            elif activation == 'relu':\n",
        "                return dH_prev * Compute.Relu_derivative(A_prev)\n",
        "\n",
        "\n",
        "class Update:\n",
        "    @staticmethod\n",
        "    def simple_gradient_descent(eta):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            W -= eta * dW\n",
        "            b -= eta * db\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss):\n",
        "        theta = {}\n",
        "        input_data = my_network.TrainInput[:, i:i + batch_size]\n",
        "        output_data = my_network.TrainOutput[0, i:i + batch_size]\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            theta[\"W\" + str(l)] = my_network.theta[\"W\" + str(l)] - beta * previous_updates[\"W\" + str(l)]\n",
        "            theta[\"b\" + str(l)] = my_network.theta[\"b\" + str(l)] - beta * previous_updates[\"b\" + str(l)]\n",
        "        y_predicted = my_network.forward(input_data, my_network.activation_function, my_network.theta)\n",
        "        e_y = np.transpose(np.eye(my_network.n_output)[output_data])\n",
        "        my_network.backpropagation(y_predicted, e_y, batch_size, loss, my_network.activation_function, my_network.theta)\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + my_network.grads[\"dW\" + str(l)]\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + my_network.grads[\"db\" + str(l)]\n",
        "            my_network.theta[\"W\" + str(l)] -= eta * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= eta * my_network.grads[\"db\" + str(l)]\n",
        "        return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def momentum_gradient_descent(eta, beta, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            uW, ub = previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)]\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            uW = beta * uW + dW\n",
        "            ub = beta * ub + db\n",
        "            W -= eta * uW\n",
        "            b -= eta * ub\n",
        "            previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)] = uW, ub\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "            return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def rms_prop(eta, beta, epsilon, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"dW\" + str(l)])\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"db\" + str(l)])\n",
        "            factorW = eta / (np.sqrt(previous_updates[\"W\" + str(l)] + epsilon))\n",
        "            factorb = eta / (np.sqrt(previous_updates[\"b\" + str(l)] + epsilon))\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * my_network.grads[\"db\" + str(l)]\n",
        "            return previous_updates\n",
        "            '''\n",
        "            Working previously fetched an issue that the previous_updates should be returned\n",
        "            if not then it is showing validation accuracy as 9.05%\n",
        "            but after returning this slightly better\n",
        "            '''\n",
        "\n",
        "    @staticmethod\n",
        "    def nadam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            term1 = 1 - (beta1 ** (t))\n",
        "            term2 = (1 - beta1) * my_network.grads[\"dW\" + str(l)] / term1\n",
        "            term3 = (1 - beta1) * my_network.grads[\"db\" + str(l)] / term1\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * (beta1 * MW_corrected + term2)\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * (beta1 * Mb_corrected + term3)\n",
        "        return M, V, t + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def adam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** t))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** t))\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** t))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** t))\n",
        "            my_network.theta[\"W\" + str(l)] -= (eta / (np.sqrt(VW_corrected) + epsilon)) * MW_corrected\n",
        "            my_network.theta[\"b\" + str(l)] -= (eta / (np.sqrt(Vb_corrected) + epsilon)) * Mb_corrected\n",
        "        return M, V, t + 1\n",
        "\n",
        "\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  theta = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "\n",
        "\n",
        "  def __init__(self,mode_of_initialization=\"random\",number_of_hidden_layers=1,num_neurons_in_hidden_layers=4,activation=\"sigmoid\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T):\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.theta[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.theta[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.theta[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def forward(self, X, activation, theta):\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1, self.n_layers):\n",
        "        H = self.cache[\"H\" + str(l - 1)]\n",
        "        W = self.theta[\"W\" + str(l)]\n",
        "        b = self.theta[\"b\" + str(l)]\n",
        "        A = np.dot(W, H) + b\n",
        "        self.cache[\"A\" + str(l)] = A\n",
        "        H = Util.apply_activation(A, activation)\n",
        "        self.cache[\"H\" + str(l)] = H\n",
        "    Al = self.cache[\"A\" + str(self.n_layers - 1)]\n",
        "    y_hat= Compute.softmax(Al)\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "\n",
        "  def backpropagation(self, y_predicted, e_y, batch_size, loss, activation, theta):\n",
        "        if loss == 'cross_entropy':\n",
        "            dA = y_predicted - e_y\n",
        "        elif loss=='squared_loss':\n",
        "            dA=(y_predicted - e_y)*Compute.softmax_derivative(self.cache[\"A\" + str(self.n_layers - 1)])\n",
        "        m = dA.shape[1]\n",
        "        self.grads[\"dA\" + str(self.n_layers - 1)] = dA\n",
        "\n",
        "        for k in range(self.n_layers - 1, 0, -1):\n",
        "            dA = self.grads[\"dA\" + str(k)]\n",
        "            H_prev = self.cache[\"H\" + str(k - 1)]\n",
        "            A_prev = self.cache[\"A\" + str(k - 1)]\n",
        "            W = self.theta[\"W\" + str(k)]\n",
        "\n",
        "            dW, db, dH_prev, dA_prev = Compute.calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size)\n",
        "\n",
        "            self.grads[\"dA\" + str(k - 1)] = dA_prev\n",
        "            self.grads[\"dW\" + str(k)] = dW\n",
        "            self.grads[\"db\" + str(k)] = db\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def compute(self, eta = 0.1,beta = 0.5,beta1 = 0.5,beta2 = 0.5 ,epsilon = 0.000001, optimizer = 'sgd',batch_size = 4,loss = 'cross_entropy',epochs = 1):\n",
        "    train_c_epoch, tarin_acc_per_epoch, val_c_per_epoch, val_acc_per_epoch, previous_updates, M, V = [], [], [], [], {}, {}, {}\n",
        "    for l in range(1 , self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      previous_updates[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    for l in range(1 , self.n_layers):\n",
        "      M[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      M[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "      V[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      V[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    t = 1\n",
        "    for count in range(epochs):\n",
        "      for i in range(0 , self.TrainInput.shape[1],batch_size):\n",
        "        if i + batch_size > self.TrainInput.shape[1]:\n",
        "          continue\n",
        "        theta = self.theta\n",
        "        yPredicted = self.forward(self.TrainInput[:,i:i + batch_size],self.activation_function,theta)\n",
        "        e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])\n",
        "        self.backpropagation(yPredicted,e_y,batch_size,loss,self.activation_function,theta)\n",
        "        if optimizer == 'sgd':   #referred slide page 54\n",
        "            Update.simple_gradient_descent(eta) #working\n",
        "        elif optimizer == 'nag':\n",
        "            previous_updates=Update.nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss) #working\n",
        "\n",
        "        elif optimizer == 'momentum': #referred from slide 43\n",
        "          previous_updates=Update.momentum_gradient_descent(eta,beta,previous_updates) #working\n",
        "\n",
        "        elif optimizer == 'RMSprop':\n",
        "          previosu_updates=Update.rms_prop(eta,beta,epsilon,previous_updates) #not working\n",
        "        elif optimizer == 'adam':\n",
        "          epsilon = 1e-8\n",
        "          M , V ,t= Update.adam(eta,beta1,beta2,epsilon,M , V , t)\n",
        "        elif optimizer == 'nadam':\n",
        "          epsilon = 1e-8\n",
        "          M , V , t= Update.nadam(eta,beta1,beta2,epsilon,M , V , t)\n",
        "\n",
        "      y_hat = self.forward(self.TrainInput,self.activation_function,self.theta)\n",
        "      valy_hat = self.forward(self.ValInput,self.activation_function,self.theta)\n",
        "      train_cost = Util.loss(self.TrainInput,self.TrainOutput,y_hat,loss,self.TrainInput.shape[1])\n",
        "      train_c_epoch.append(train_cost)\n",
        "      val_cost = Util.loss(self.ValInput,self.ValOutput,valy_hat,loss,self.ValInput.shape[1])\n",
        "      val_c_per_epoch.append(val_cost)\n",
        "      train_acc = Util.accuracy(self.TrainInput, self.TrainOutput,y_hat)\n",
        "      tarin_acc_per_epoch.append(train_acc)\n",
        "\n",
        "      val_acc = Util.accuracy(self.ValInput, self.ValOutput,valy_hat)\n",
        "      val_acc_per_epoch.append(val_acc)\n",
        "    #   print(np.eye(self.n_output)[self.ValOutput[0]].T.shape,valy_hat.shape)\n",
        "      conf_matrix = confusion_matrix(np.argmax(np.eye(self.n_output)[self.ValOutput[0]].T, axis=0),\n",
        "                                np.argmax(valy_hat, axis=0))\n",
        "\n",
        "      '''\n",
        "        # Print confusion matrix\n",
        "      cm_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"T-shirt/top\",\n",
        "                                                                        \"Trouser\",\n",
        "                                                                        \"Pullover\",\n",
        "                                                                        \"Dress\",\n",
        "                                                                        \"Coat\",\n",
        "                                                                        \"Sandal\",\n",
        "                                                                        \"Shirt\",\n",
        "                                                                        \"Sneaker\",\n",
        "                                                                        \"Bag\",\n",
        "                                                                        \"Ankle boot\"])\n",
        "\n",
        "        # Customize appearance\n",
        "      cm_display.plot(cmap='Oranges')\n",
        "      plt.xlabel('Predicted Label', fontsize=14)\n",
        "      plt.ylabel('True Label', fontsize=14)\n",
        "      plt.title('Confusion Matrix', fontsize=16)\n",
        "      plt.xticks(rotation=45, ha='right')\n",
        "      plt.tight_layout()\n",
        "      wandb.log({\"Confusion Matrix at epoch\"+str(count+1) : plt})\n",
        "      '''\n",
        "\n",
        "\n",
        "      print(\"---------\"*20)\n",
        "      print(f\"Epoch Number = {format(count+1)}\")\n",
        "      print(f\"Training Accuracy = {format(tarin_acc_per_epoch[-1])}\")\n",
        "      print(f\"Validation Accuracy = {format(val_acc_per_epoch[-1])}\")\n",
        "    #   wandb.log({\"training_accuracy\": train_acc,\"validation_accuracy\": val_acc,\"training_loss\":train_cost,\"validation_loss\": val_cost,\"epoch\": count})\n",
        "    return train_c_epoch,tarin_acc_per_epoch,val_c_per_epoch,val_acc_per_epoch\n",
        "\n",
        "\n",
        "my_network = MyNeuralNetwork(mode_of_initialization=\"xavier\",number_of_hidden_layers=1,num_neurons_in_hidden_layers=4,activation=\"sigmoid\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T)\n",
        "my_network.compute(eta = 0.1,beta = 0.5,beta1 = 0.5,beta2 = 0.5 ,epsilon = 0.05, optimizer = 'adam',batch_size = 4,loss = 'cross_entropy',epochs = 5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 909,
          "referenced_widgets": [
            "1e1ae32c5865475e8d44fc27f4563045",
            "981974721d46417f93377ffa9ee70ac7",
            "a90d5e163fdd4430aeccda5f3e6f3552",
            "75e928fbf21343caa15dad9b0b23785e",
            "4ddbd2c66dcf48c28c3e18d6fcda8905",
            "93aae286f90b4fe2be389b8fa264b996",
            "0711d25ffd9b419cb02bba788482e403",
            "5761f3014df441b19a81484f13a2f1c2"
          ]
        },
        "id": "K5BuEJ8Iic-3",
        "outputId": "ea798452-5b90-4b02-d0ec-84ff529a2563"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.4)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.42)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.41.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Ignored wandb.init() arg project when running a sweep.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:xcmk0ctw) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:xcmk0ctw). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011113514466665543, max=1.0…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e1ae32c5865475e8d44fc27f4563045"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Problem at: <ipython-input-21-36f220bbbb95> 14 <cell line: 14>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-36f220bbbb95>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m wandb.init(\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m# set the wandb project where this run will be logged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"deep-learning\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"interrupted\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1199\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1200\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m         \u001b[0merror_seen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[1;32m   1174\u001b[0m         \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1176\u001b[0;31m             \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1177\u001b[0m             \u001b[0mexcept_exit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_except_exit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0mrun_init_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeliver_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m         result = run_init_handle.wait(\n\u001b[0m\u001b[1;32m    757\u001b[0m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0mon_progress\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_progress_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[1;32m    281\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mMailboxError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"transport failed\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mfound\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabandoned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_and_clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfound\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;31m# Always update progress to 100% when done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\u001b[0m in \u001b[0;36m_get_and_clear\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_and_clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0mfound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/wandb/sdk/lib/mailbox.py\u001b[0m in \u001b[0;36m_wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_and_clear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    605\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    608\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install wandb\n",
        "\n",
        "import wandb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score,ConfusionMatrixDisplay\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "wandb.login()\n",
        "\n",
        "wandb.init(\n",
        "    # set the wandb project where this run will be logged\n",
        "    project=\"deep-learning\",\n",
        "\n",
        "    # track hyperparameters and run metadata\n",
        "    config={\n",
        "    \"eta\": 0.001,\n",
        "    \"beta\" : 0.5,\n",
        "    \"beta1\" : 0.1,\n",
        "    \"beta2\": 0.1 ,\n",
        "    \"epsilon\" : 0.05\n",
        "\n",
        "    }\n",
        ")\n",
        "sweep_config = {\n",
        "    'method': 'random'\n",
        "}\n",
        "metric = {\n",
        "    'name': 'val_acc_per_epoch',\n",
        "    'goal': 'maximize'\n",
        "    }\n",
        "\n",
        "sweep_config['metric'] = metric\n",
        "\n",
        "parameters_dict = {\n",
        "    'optimizer': {\n",
        "        'values': ['adam', 'nadam', 'sgd']\n",
        "        },\n",
        "    'number_of_hidden_layers':{\n",
        "        'values': [3,4,5]\n",
        "    },\n",
        "    'num_neurons_in_hidden_layers': {\n",
        "        'values': [16, 32, 64,128]\n",
        "        },\n",
        "    'activation': {\n",
        "          'values': ['sigmoid', 'tanh', 'relu']\n",
        "        },\n",
        "    'epoch':{\n",
        "        'values': [5,10,15]\n",
        "    }\n",
        "}\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict\n",
        "\n",
        "parameters_dict.update({\n",
        "    'eta': {\n",
        "        # a flat distribution between 0 and 0.1\n",
        "        'distribution': 'uniform',\n",
        "        'min': 0,\n",
        "        'max': 0.1\n",
        "      },\n",
        "    'batch_size': {\n",
        "        # integers between 32 and 256\n",
        "        # with evenly-distributed logarithms\n",
        "        'distribution': 'q_log_uniform_values',\n",
        "        'q': 8,\n",
        "        'min': 32,\n",
        "        'max': 256,\n",
        "      }\n",
        "    })\n",
        "\n",
        "import pprint\n",
        "\n",
        "pprint.pprint(sweep_config)\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"deep-learning\")\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "'''normalizing the data'''\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "x_train_T = x_train.reshape(-1, x_train.shape[1]*x_train.shape[2]).T\n",
        "x_val_T = x_val.reshape(-1, x_val.shape[1]*x_val.shape[2]).T\n",
        "x_test_T = x_test.reshape(-1, x_test.shape[1]*x_test.shape[2]).T\n",
        "y_train_T, y_val_T, y_test_T = y_train.reshape(1, -1), y_val.reshape(1, -1), y_test.reshape(1, -1)\n",
        "\n",
        "class Util:\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_activation(A, activation):\n",
        "            if activation == 'sigmoid':\n",
        "                return Compute.sigmoid(A)\n",
        "            elif activation == 'relu':\n",
        "                return Compute.Relu(A)\n",
        "            elif activation == 'tanh':\n",
        "                return Compute.tanh(A)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(input, true_output, predicted_output, loss, batch_size):\n",
        "        if loss == 'cross_entropy':\n",
        "            one_hot_true_output = np.eye(my_network.n_output)[true_output[0]].T\n",
        "            return -np.sum(one_hot_true_output * np.log(predicted_output + 1e-9)) / batch_size\n",
        "\n",
        "\n",
        "        if loss=='squared_loss':\n",
        "            one_hot_true_output = np.eye(my_network.n_output)[true_output[0]].T\n",
        "            loss_factor=np.square(predicted_output-one_hot_true_output)\n",
        "            return np.sum(loss_factor)/batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(input, true_output, predicted_output):\n",
        "        predicted_labels = np.argmax(predicted_output, axis=0)\n",
        "        correct_predictions = np.sum(true_output == predicted_labels)\n",
        "        total_samples = true_output.shape[1]\n",
        "        accuracy_percentage = (correct_predictions / total_samples) * 100\n",
        "        return accuracy_percentage\n",
        "\n",
        "\n",
        "class Compute:\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return  1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu(x):\n",
        "        return np.maximum(x,0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return (2 * Compute.sigmoid(2 * x)) - 1\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(x):\n",
        "        return Compute.softmax(x) * (1-Compute.softmax(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(Z):\n",
        "        s = Compute.sigmoid(Z)\n",
        "        dA = s * (1 - s)\n",
        "        return dA\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu_derivative(x):\n",
        "        return 1*(x > 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(x):\n",
        "        return (1 - (Compute.tanh(x)**2))\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size):\n",
        "            dW = Compute.calculate_dW(dA, H_prev, batch_size)\n",
        "            db = Compute.calculate_db(dA, batch_size)\n",
        "            dH_prev, dA_prev = Compute.calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev) if k > 1 else (np.zeros(H_prev.shape), np.zeros(A_prev.shape))\n",
        "\n",
        "            return dW, db, dH_prev, dA_prev\n",
        "    @staticmethod\n",
        "    def calculate_dW(dA, H_prev, batch_size):\n",
        "            return np.dot(dA, H_prev.T) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_db(dA, batch_size):\n",
        "            return np.sum(dA, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev):\n",
        "            dH_prev = np.matmul(W.T, dA)\n",
        "            dA_prev = Compute.calculate_dA_prev(dH_prev, activation, A_prev)\n",
        "            return dH_prev, dA_prev\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dA_prev(dH_prev, activation, A_prev):\n",
        "            if activation == 'sigmoid':\n",
        "                return dH_prev * Compute.sigmoid_derivative(A_prev)\n",
        "            elif activation == 'tanh':\n",
        "                return dH_prev * Compute.tanh_derivative(A_prev)\n",
        "            elif activation == 'relu':\n",
        "                return dH_prev * Compute.Relu_derivative(A_prev)\n",
        "\n",
        "\n",
        "class Update:\n",
        "    @staticmethod\n",
        "    def stochastic_gradient_descent(eta):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            W -= eta * dW\n",
        "            b -= eta * db\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss):\n",
        "        theta = {}\n",
        "        input_data = my_network.TrainInput[:, i:i + batch_size]\n",
        "        output_data = my_network.TrainOutput[0, i:i + batch_size]\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            theta[\"W\" + str(l)] = my_network.theta[\"W\" + str(l)] - beta * previous_updates[\"W\" + str(l)]\n",
        "            theta[\"b\" + str(l)] = my_network.theta[\"b\" + str(l)] - beta * previous_updates[\"b\" + str(l)]\n",
        "        y_predicted = my_network.forward(input_data, my_network.activation_function, my_network.theta)\n",
        "        e_y = np.transpose(np.eye(my_network.n_output)[output_data])\n",
        "        my_network.backpropagation(y_predicted, e_y, batch_size, loss, my_network.activation_function, my_network.theta)\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + my_network.grads[\"dW\" + str(l)]\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + my_network.grads[\"db\" + str(l)]\n",
        "            my_network.theta[\"W\" + str(l)] -= eta * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= eta * my_network.grads[\"db\" + str(l)]\n",
        "        return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def momentum_gradient_descent(eta, beta, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            uW, ub = previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)]\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            uW = beta * uW + dW\n",
        "            ub = beta * ub + db\n",
        "            W -= eta * uW\n",
        "            b -= eta * ub\n",
        "            previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)] = uW, ub\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "            return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def rms_prop(eta, beta, epsilon, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"dW\" + str(l)])\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"db\" + str(l)])\n",
        "            factorW = eta / (np.sqrt(previous_updates[\"W\" + str(l)] + epsilon))\n",
        "            factorb = eta / (np.sqrt(previous_updates[\"b\" + str(l)] + epsilon))\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * my_network.grads[\"db\" + str(l)]\n",
        "            return previous_updates\n",
        "            '''\n",
        "            Working previously fetched an issue that the previous_updates should be returned\n",
        "            if not then it is showing validation accuracy as 9.05%\n",
        "            but after returning this slightly better\n",
        "            '''\n",
        "\n",
        "    @staticmethod\n",
        "    def nadam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            term1 = 1 - (beta1 ** (t))\n",
        "            term2 = (1 - beta1) * my_network.grads[\"dW\" + str(l)] / term1\n",
        "            term3 = (1 - beta1) * my_network.grads[\"db\" + str(l)] / term1\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * (beta1 * MW_corrected + term2)\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * (beta1 * Mb_corrected + term3)\n",
        "        return M, V\n",
        "\n",
        "    @staticmethod\n",
        "    def adam(eta, beta1, beta2, epsilon, M, V, t): #taken from slide-2 page 42 [cs6910]\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            MW_hat = M[\"W\" + str(l)] / (1 - np.power(beta1,t))\n",
        "            Mb_hat = M[\"b\" + str(l)] / (1 - np.power(beta1,t))\n",
        "            VW_hat = V[\"W\" + str(l)] / (1 - np.power(beta2,t))\n",
        "            Vb_hat = V[\"b\" + str(l)] / (1 - np.power(beta2,t))\n",
        "            my_network.theta[\"W\" + str(l)] -= (eta / (np.sqrt(VW_hat) + epsilon)) * MW_hat\n",
        "            my_network.theta[\"b\" + str(l)] -= (eta / (np.sqrt(Vb_hat) + epsilon)) * Mb_hat\n",
        "        return M, V\n",
        "\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  theta = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "\n",
        "\n",
        "  def __init__(self,mode_of_initialization=\"random\",number_of_hidden_layers=1,num_neurons_in_hidden_layers=4,activation=\"sigmoid\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T):\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.theta[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.theta[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.theta[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def forward(self, X, activation, theta):\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1, self.n_layers):\n",
        "        H = self.cache[\"H\" + str(l - 1)]\n",
        "        W = self.theta[\"W\" + str(l)]\n",
        "        b = self.theta[\"b\" + str(l)]\n",
        "        A = np.dot(W, H) + b\n",
        "        self.cache[\"A\" + str(l)] = A\n",
        "        H = Util.apply_activation(A, activation)\n",
        "        self.cache[\"H\" + str(l)] = H\n",
        "        # print(\"----\"*5,l,\"------\"*5)\n",
        "        # print(\"A\")\n",
        "        # print(H)\n",
        "        # print('H')\n",
        "        # print(H)\n",
        "    Al = self.cache[\"A\" + str(self.n_layers - 1)]\n",
        "    y_hat= Compute.softmax(Al)\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "\n",
        "  def backpropagation(self, y_predicted, e_y, batch_size, loss, activation, theta):\n",
        "        if loss == 'cross_entropy':\n",
        "            dA = y_predicted - e_y\n",
        "        elif loss=='squared_loss':\n",
        "            dA=(y_predicted - e_y)*Compute.softmax_derivative(self.cache[\"A\" + str(self.n_layers - 1)])\n",
        "        m = dA.shape[1]\n",
        "        self.grads[\"dA\" + str(self.n_layers - 1)] = dA\n",
        "\n",
        "        for k in range(self.n_layers - 1, 0, -1):\n",
        "            dA = self.grads[\"dA\" + str(k)]\n",
        "            H_prev = self.cache[\"H\" + str(k - 1)]\n",
        "            A_prev = self.cache[\"A\" + str(k - 1)]\n",
        "            W = self.theta[\"W\" + str(k)]\n",
        "\n",
        "            dW, db, dH_prev, dA_prev = Compute.calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size)\n",
        "\n",
        "            self.grads[\"dA\" + str(k - 1)] = dA_prev\n",
        "            self.grads[\"dW\" + str(k)] = dW\n",
        "            self.grads[\"db\" + str(k)] = db\n",
        "            # print(\"----\"*5,k,\"-----\"*5)\n",
        "            # print(\"dw\")\n",
        "            # print(dW)\n",
        "            # print(\"db\")\n",
        "            # print(db)\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def compute(self, eta = 0.1,beta = 0.5,beta1 = 0.5,beta2 = 0.5 ,epsilon = 0.000001, optimizer = 'sgd',batch_size = 4,loss = 'cross_entropy',epochs = 1):\n",
        "    train_c_epoch, tarin_acc_per_epoch, val_c_per_epoch, val_acc_per_epoch, previous_updates, M, V = [], [], [], [], {}, {}, {}\n",
        "    for l in range(1 , self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      previous_updates[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    for l in range(1 , self.n_layers):\n",
        "      M[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      M[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "      V[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      V[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    t = 1\n",
        "    for count in range(epochs):\n",
        "      for i in range(0 , self.TrainInput.shape[1],batch_size):\n",
        "        if i + batch_size > self.TrainInput.shape[1]:\n",
        "          continue\n",
        "        theta = self.theta\n",
        "        yPredicted = self.forward(self.TrainInput[:,i:i + batch_size],self.activation_function,theta)\n",
        "        e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])\n",
        "        self.backpropagation(yPredicted,e_y,batch_size,loss,self.activation_function,theta)\n",
        "        if optimizer == 'sgd':   #referred slide page 54\n",
        "            Update.stochastic_gradient_descent(eta) #working\n",
        "        elif optimizer == 'nag':\n",
        "            previous_updates=Update.nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss) #working\n",
        "\n",
        "        elif optimizer == 'momentum': #referred from slide 43\n",
        "          previous_updates=Update.momentum_gradient_descent(eta,beta,previous_updates) #working\n",
        "\n",
        "        elif optimizer == 'RMSprop':\n",
        "          previous_updates=Update.rms_prop(eta,beta,epsilon,previous_updates) #working\n",
        "        elif optimizer == 'adam':\n",
        "          epsilon = 1e-10\n",
        "          M , V = Update.adam(eta,beta1,beta2,epsilon,M , V , count+1)\n",
        "        elif optimizer == 'nadam':\n",
        "          epsilon = 1e-8\n",
        "          M , V = Update.nadam(eta,beta1,beta2,epsilon,M , V , count+1)\n",
        "\n",
        "      y_hat = self.forward(self.TrainInput,self.activation_function,self.theta)\n",
        "      valy_hat = self.forward(self.ValInput,self.activation_function,self.theta)\n",
        "      train_cost = Util.loss(self.TrainInput,self.TrainOutput,y_hat,loss,self.TrainInput.shape[1])\n",
        "      train_c_epoch.append(train_cost)\n",
        "      val_cost = Util.loss(self.ValInput,self.ValOutput,valy_hat,loss,self.ValInput.shape[1])\n",
        "      val_c_per_epoch.append(val_cost)\n",
        "      train_acc = Util.accuracy(self.TrainInput, self.TrainOutput,y_hat)\n",
        "      tarin_acc_per_epoch.append(train_acc)\n",
        "\n",
        "      val_acc = Util.accuracy(self.ValInput, self.ValOutput,valy_hat)\n",
        "      val_acc_per_epoch.append(val_acc)\n",
        "    #   print(np.eye(self.n_output)[self.ValOutput[0]].T.shape,valy_hat.shape)\n",
        "      conf_matrix = confusion_matrix(np.argmax(np.eye(self.n_output)[self.ValOutput[0]].T, axis=0),\n",
        "                                np.argmax(valy_hat, axis=0))\n",
        "\n",
        "      '''\n",
        "        # Print confusion matrix\n",
        "      cm_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"T-shirt/top\",\n",
        "                                                                        \"Trouser\",\n",
        "                                                                        \"Pullover\",\n",
        "                                                                        \"Dress\",\n",
        "                                                                        \"Coat\",\n",
        "                                                                        \"Sandal\",\n",
        "                                                                        \"Shirt\",\n",
        "                                                                        \"Sneaker\",\n",
        "                                                                        \"Bag\",\n",
        "                                                                        \"Ankle boot\"])\n",
        "\n",
        "        # Customize appearance\n",
        "      cm_display.plot(cmap='Oranges')\n",
        "      plt.xlabel('Predicted Label', fontsize=14)\n",
        "      plt.ylabel('True Label', fontsize=14)\n",
        "      plt.title('Confusion Matrix', fontsize=16)\n",
        "      plt.xticks(rotation=45, ha='right')\n",
        "      plt.tight_layout()\n",
        "      wandb.log({\"Confusion Matrix at epoch\"+str(count+1) : plt})\n",
        "      '''\n",
        "\n",
        "\n",
        "      print(\"---------\"*20)\n",
        "      print(f\"Epoch Number = {format(count+1)}\")\n",
        "      print(f\"Training Accuracy = {format(tarin_acc_per_epoch[-1])}\")\n",
        "      print(f\"Validation Accuracy = {format(val_acc_per_epoch[-1])}\")\n",
        "    #   if(count==1):\n",
        "    #     print(self.cache[\"A1\"])\n",
        "    #   wandb.log({\"training_accuracy\": train_acc,\"validation_accuracy\": val_acc,\"training_loss\":train_cost,\"validation_loss\": val_cost,\"epoch\": count})\n",
        "    return train_c_epoch,tarin_acc_per_epoch,val_c_per_epoch,val_acc_per_epoch\n",
        "\n",
        "\n",
        "my_network = MyNeuralNetwork(mode_of_initialization=\"random\",number_of_hidden_layers=4,num_neurons_in_hidden_layers=32,activation=\"relu\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T)\n",
        "my_network.compute(eta = 0.001,beta = 0.9,beta1 = 0.1,beta2 = 0.1 ,epsilon = 0.05, optimizer = 'momentum',batch_size = 32,loss = 'cross_entropy',epochs = 5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgCib6k31rsj"
      },
      "outputs": [],
      "source": [
        "wandb.agent(sweep_id, x_val_T, count=5)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Question5.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/11gmNfKBG39ABwUVhkJr-KnagzUBnwk4z\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Question4.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1eNACW0NpHLdZ4r7uYnuR3Vv3GUvP3d68\n",
        "\"\"\"\n",
        "\n",
        "!pip install wandb\n",
        "\n",
        "import wandb\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import seaborn as sns\n",
        "'''\n",
        "10 distinct images of fashion_mnist class is written and then wandb is initialized and the images are plotted\n",
        "'''\n",
        "ImageClasses = [\"Pullover\",\"Shirt\",\"Coat\",\"Trouser\",\"Dress\",\"Sandal\",\"Bag\",\"Sneaker\",\"Ankle boot\",\"T-shirt/top\"]\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "wandb.login()\n",
        "\n",
        "\n",
        "'''normalizing the data'''\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "x_valT = np.transpose(x_val.reshape(x_val.shape[0] , x_val.shape[1] * x_val.shape[2]))\n",
        "x_trainT = np.transpose(x_train.reshape(x_train.shape[0] , x_train.shape[1] * x_train.shape[2]))\n",
        "x_testT = np.transpose(x_test.reshape(x_test.shape[0] , x_test.shape[1] * x_test.shape[2]))\n",
        "y_trainT = y_train.reshape(1 , y_train.shape[0])\n",
        "y_valT = y_val.reshape(1 , y_val.shape[0])\n",
        "y_testT = y_test.reshape(1 , y_test.shape[0])\n",
        "\n",
        "class ActivationFunction:\n",
        "  '''all activation functions are defined here'''\n",
        "  def sigmoid(x):\n",
        "    return  1 / (1 + np.exp(-x))\n",
        "  def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "  def Relu(x):\n",
        "    return np.maximum(x,0)\n",
        "  def tanh(x):\n",
        "    return np.tanh(x)\n",
        "  def softmax_derivative(x):\n",
        "    return ActivationFunction.softmax(x) * (1-ActivationFunction.softmax(x))\n",
        "  def sigmoid_derivative(Z):\n",
        "    s = 1 /(1 + np.exp(-Z))\n",
        "    dA = s * (1 - s)\n",
        "    return dA\n",
        "  def Relu_derivative(x):\n",
        "    return 1*(x > 0)\n",
        "  def tanh_derivative(x):\n",
        "    return (1 - (np.tanh(x)**2))\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "  '''\n",
        "  mode_of_initialization =str , The way the weights are initialized\n",
        "  n_layers = int ,total number of layers\n",
        "  activation_function = str , activation function of the neural network,default is sigmoid activation function\n",
        "  n_input = int ,number of inputs\n",
        "  n_outputs = int , number of outputs\n",
        "  num_neurns_in_hidden_layer = int ,how many neurons are in each hidden layer\n",
        "  n_neurons = list having number of neurons in each layer\n",
        "  TrainInput = input layer's input\n",
        "  TrainOutput = output layer's output\n",
        "  ValInput = Validation Input\n",
        "  ValOutput = validation output\n",
        "  parameters = dict ,stores the parameters of the model\n",
        "  cache = dict , stores the H and A for each layer which help in gradient computation\n",
        "  grads = dict ,store the gradients\n",
        "  '''\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  num_neurons_in_hidden_layers = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  parameters = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "  def __init__(self,mode_of_initialization,number_of_hidden_layers,num_neurons_in_hidden_layers,activation,TrainInput,TrainOutput,ValInput,ValOutput):\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    self.num_neurons_in_hidden_layers = num_neurons_in_hidden_layers\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.parameters[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.parameters[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.parameters[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def output(self,A):\n",
        "    '''\n",
        "    uses softmax function in the last layer for calculating the prediction\n",
        "    '''\n",
        "    return ActivationFunction.softmax(A)\n",
        "\n",
        "\n",
        "  def forward(self,X,activation,parameters):\n",
        "    '''\n",
        "    X = numpy array ,input dataset\n",
        "    activation = str , the type of activation function used\n",
        "    parameters = dict , parameters of the model\n",
        "    calculates H and A for each layer and stores them into cache dictionary. It uses the parameter dictionary .\n",
        "    '''\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1 , self.n_layers):\n",
        "      H = self.cache[\"H\" + str(l - 1)]\n",
        "      # print(H.shape)\n",
        "      W = self.parameters[\"W\" + str(l)]\n",
        "      b = self.parameters[\"b\" + str(l)]\n",
        "      A = np.dot(W , H) + b\n",
        "      # print(W.shape)\n",
        "      # print(b.shape)\n",
        "      self.cache[\"A\" + str(l)] = A\n",
        "      if activation == 'sigmoid':\n",
        "         H = ActivationFunction.sigmoid(A)\n",
        "      elif activation == 'relu':\n",
        "         H = ActivationFunction.Relu(A)\n",
        "      elif activation == 'tanh':\n",
        "         H = ActivationFunction.tanh(A)\n",
        "      self.cache[\"H\" + str(l)] = H\n",
        "    yPredicted = self.output(self.cache[\"A\" + str(self.n_layers - 1)])\n",
        "    return yPredicted\n",
        "\n",
        "  def predict(self,input):\n",
        "    '''\n",
        "    input = numpy array ,input dataset\n",
        "    parameters = dict , parameters of the model\n",
        "    predicts the class of the image , although result is not int , it is probability\n",
        "    '''\n",
        "    H = input\n",
        "    activation = self.activation_function\n",
        "    for l in range(1 , self.n_layers - 1):\n",
        "      # print(H.shape)\n",
        "      W = self.parameters[\"W\" + str(l)]\n",
        "      b = self.parameters[\"b\" + str(l)]\n",
        "      A = np.dot(W , H) + b\n",
        "      # print(W.shape)\n",
        "      # print(b.shape)\n",
        "      if activation == 'sigmoid':\n",
        "         H = ActivationFunction.sigmoid(A)\n",
        "      elif activation == 'relu':\n",
        "         H = ActivationFunction.Relu(A)\n",
        "      elif activation == 'tanh':\n",
        "         H = ActivationFunction.tanh(A)\n",
        "\n",
        "\n",
        "    W = self.parameters[\"W\" + str(self.n_layers - 1)]\n",
        "    b = self.parameters[\"b\" + str(self.n_layers - 1)]\n",
        "    A = np.dot(W , H) + b\n",
        "    y_predicted = self.output(A)\n",
        "    return y_predicted\n",
        "\n",
        "  def lossFunction(self,input,TrueOutput,PredictedOutput,loss,batch_size,wd):\n",
        "    '''\n",
        "    input = numpy array ,input dataset\n",
        "    TrueOutput = list of str ,TrueOutput label\n",
        "    PredictedOutput = numpy array ,applying the model to the input we get it\n",
        "    loss = str , which type of loss will occur\n",
        "    batch_size = int , on how many data sample it is doing the calculation\n",
        "    wd = float ,weight deacy\n",
        "    Finds the loss value\n",
        "    '''\n",
        "    lossValue = 0\n",
        "    # print(PredictedOutput.shape)   (10,54000)\n",
        "    if loss == 'cross_entropy':\n",
        "      OneHotOfTrueOutput = np.transpose(np.eye(self.n_output)[TrueOutput[0]])\n",
        "      sum = -np.sum(OneHotOfTrueOutput * np.log(PredictedOutput + 1e-9))\n",
        "      lossValue = sum / batch_size\n",
        "      # print(loss)\n",
        "      val = 0\n",
        "      for l in range(1 , self.n_layers):\n",
        "        for i in range(self.parameters['W' + str(l)].shape[0]):\n",
        "          for j in range(self.parameters['W' + str(l)].shape[1]):\n",
        "            val = val + self.parameters['W' + str(l)][i][j] ** 2\n",
        "    lossValue = lossValue + wd * val\n",
        "    return lossValue\n",
        "  def accuracy(self,input,TrueOutput,PredictedOutput):\n",
        "    '''\n",
        "    input = numpy array ,input given\n",
        "    TrueOutput = numpy array, true output label\n",
        "    PredictedOutput = numpy array, model predicts this output\n",
        "    finds the accuracy of the model\n",
        "    '''\n",
        "    PredictedOutput = np.argmax(PredictedOutput,axis = 0)\n",
        "    count = 0\n",
        "    for i in range(TrueOutput.shape[1]):\n",
        "      if TrueOutput[0,i] == PredictedOutput[i]:\n",
        "        count = count + 1\n",
        "    accu = (count / TrueOutput.shape[1]) * 100\n",
        "    return accu\n",
        "\n",
        "\n",
        "\n",
        "  def backprop(self, yPredicted , e_y , batchSize,loss,activation,parameters,wd):\n",
        "    '''\n",
        "    yPredicted = numpy array ,predicted value by the model\n",
        "    e_y = numpy array ,One hot vector representation of the True Label\n",
        "    batch_size = int ,on how many data sample it is doing the calculation\n",
        "    loss = str , which type of loss will occur\n",
        "    activation = str , the type of activation function used\n",
        "    parameters = dict , parameters of the model\n",
        "    wd = float ,weight decay\n",
        "    Implements back propagation algorithm and stores the gradient values in the grads dict of the class\n",
        "    '''\n",
        "    if loss == 'cross_entropy':\n",
        "      dA = yPredicted - e_y\n",
        "    m = dA.shape[1]\n",
        "    self.grads[\"dA\" + str(self.n_layers - 1)] = dA\n",
        "    for k in (range(self.n_layers - 1,0,-1)):\n",
        "      # print(k)\n",
        "      dA = self.grads[\"dA\" + str(k)]\n",
        "      H_prev = self.cache[\"H\" + str(k - 1)]\n",
        "      A_prev = self.cache[\"A\" + str(k - 1)]\n",
        "      W = self.parameters[\"W\" + str(k)]\n",
        "      dW = np.zeros(W.shape)\n",
        "      db = np.zeros((W.shape[0],1))\n",
        "      dH_prev = np.zeros(H_prev.shape)\n",
        "      dA_prev = np.zeros(A_prev.shape)\n",
        "\n",
        "      dW = (np.dot(dA, H_prev.T)) / batchSize\n",
        "      db = np.sum(dA , axis = 1,keepdims = True)/batchSize\n",
        "      if k > 1:\n",
        "        dH_prev = np.matmul(W.T , dA)\n",
        "        if activation == 'sigmoid':\n",
        "          dA_prev = dH_prev * ActivationFunction.sigmoid_derivative(A_prev)\n",
        "        elif activation == 'tanh':\n",
        "          dA_prev = dH_prev * ActivationFunction.tanh_derivative(A_prev)\n",
        "        elif activation == 'relu':\n",
        "          dA_prev = dH_prev * ActivationFunction.Relu_derivative(A_prev)\n",
        "      self.grads[\"dA\" + str(k - 1)] = dA_prev\n",
        "      self.grads[\"dW\" + str(k)] = dW + ((2 * wd * self.parameters['W' + str(k)]) / batchSize)\n",
        "      self.grads[\"db\" + str(k)] = db\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def update_parameters_sgd(self,learning_rate):\n",
        "    '''\n",
        "    learning_rate = float ,learning_rate used by the model to update the parameters\n",
        "    Implements stochastic gradient descent\n",
        "    '''\n",
        "    for l in range(1 ,self.n_layers):\n",
        "      self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * self.grads[\"dW\" + str(l)]\n",
        "      self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * self.grads[\"db\" + str(l)]\n",
        "    return\n",
        "\n",
        "\n",
        "  def update_parameters_momentum(self,learning_rate,beta,previous_updates):\n",
        "    '''\n",
        "    learning_rate = float ,learning_rate used by the model to update the parameters\n",
        "    beta = float , beta used by the model\n",
        "    previous_updates = dict , previous updates of the model\n",
        "    Implements momentum gradient Descent\n",
        "    '''\n",
        "    for l in range(1 ,self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + self.grads[\"dW\" + str(l)]\n",
        "      previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + self.grads[\"db\" + str(l)]\n",
        "      self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * previous_updates[\"W\" + str(l)]\n",
        "      self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * previous_updates[\"b\" + str(l)]\n",
        "    return previous_updates\n",
        "\n",
        "\n",
        "\n",
        "  def update_parameters_RMSprop(self,learning_rate,beta,epsilon,previous_updates):\n",
        "    '''\n",
        "    learning_rate = float ,learning_rate used by the model to update the parameters\n",
        "    beta = float , beta used by the model\n",
        "    epsilon = float\n",
        "    previous_updates = dict , previous updates of the model\n",
        "    Implements RMSprop based gradient descent\n",
        "    '''\n",
        "    for l in range(1 ,self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + (1 - beta) * np.square(self.grads[\"dW\" + str(l)])\n",
        "      previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + (1 - beta) * np.square(self.grads[\"db\" + str(l)])\n",
        "      factorW = learning_rate / (np.sqrt(previous_updates[\"W\" + str(l)] + epsilon))\n",
        "      factorb = learning_rate / (np.sqrt(previous_updates[\"b\" + str(l)] + epsilon))\n",
        "      self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - factorW * self.grads[\"dW\" + str(l)]\n",
        "      self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - factorb * self.grads[\"db\" + str(l)]\n",
        "    return previous_updates\n",
        "\n",
        "  def update_parameters_adam(self,learning_rate,beta1,beta2,epsilon,M , V , t):\n",
        "    '''\n",
        "    learning_rate = float ,learning_rate used by the model to update the parameters\n",
        "    beta1 = float , beta1 used by the model\n",
        "    beta2 = float , beta2 used by the model\n",
        "    epsilon = float\n",
        "    M,V = dict , previous updates of the model\n",
        "    t = int ,step count\n",
        "    Implements adam based gradient descent\n",
        "    '''\n",
        "    for l in range(1 ,self.n_layers):\n",
        "      M[\"W\" + str(l)] = beta1 *M[\"W\" + str(l)] + (1 - beta1) * self.grads[\"dW\" + str(l)]\n",
        "      M[\"b\" + str(l)] = beta1 *M[\"b\" + str(l)] + (1 - beta1) * self.grads[\"db\" + str(l)]\n",
        "      V[\"W\" + str(l)] = beta2 *V[\"W\" + str(l)] + (1 - beta2) * np.square(self.grads[\"dW\" + str(l)])\n",
        "      V[\"b\" + str(l)] = beta2 *V[\"b\" + str(l)] + (1 - beta2) * np.square(self.grads[\"db\" + str(l)])\n",
        "      MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** t))\n",
        "      Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** t))\n",
        "      VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** t))\n",
        "      Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** t))\n",
        "      factorW = learning_rate / (np.sqrt(VW_corrected) + epsilon)\n",
        "      factorb = learning_rate / (np.sqrt(Vb_corrected) + epsilon)\n",
        "      self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - factorW * MW_corrected\n",
        "      self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - factorb * Mb_corrected\n",
        "    return M,V,t + 1\n",
        "\n",
        "\n",
        "  def update_parameters_NAG(self,input,output,learning_rate,beta,previous_updates,batch_size,loss,wd):\n",
        "    '''\n",
        "    input = numpy array ,input dataset\n",
        "    output = numpy array,true output dataset\n",
        "    learning_rate = float ,learning_rate used by the model to update the parameters\n",
        "    beta = float , beta used by the model\n",
        "    previous_updates = dict , previous updates of the model\n",
        "    batch_size = int, on how many data samples it is running the  algorithm\n",
        "    loss = str , which type of loss will occur\n",
        "    Implements nesterov accelerated gradient descent\n",
        "    '''\n",
        "    parameters = {}\n",
        "    for l in range(1 , self.n_layers):\n",
        "      parameters[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      parameters[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    for l in range(1 ,self.n_layers):\n",
        "      parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - beta * previous_updates[\"W\" + str(l)]\n",
        "      parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - beta * previous_updates[\"b\" + str(l)]\n",
        "    yPredicted = self.forward(input,self.activation_function,parameters)\n",
        "    e_y = np.transpose(np.eye(self.n_output)[output])\n",
        "    self.backprop(yPredicted,e_y,batch_size,loss,self.activation_function,parameters,wd)\n",
        "    for l in range(1 ,self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + self.grads[\"dW\" + str(l)]\n",
        "      previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + self.grads[\"db\" + str(l)]\n",
        "      self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - learning_rate * self.grads[\"dW\" + str(l)]\n",
        "      self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - learning_rate * self.grads[\"db\" + str(l)]\n",
        "\n",
        "    return previous_updates\n",
        "\n",
        "\n",
        "\n",
        "  def update_parameters_nadam(self,learning_rate,beta1,beta2,epsilon,M , V , t,wd,batchSize):\n",
        "    '''\n",
        "    learning_rate = float ,learning_rate used by the model to update the parameters\n",
        "    beta1 = float , beta1 used by the model\n",
        "    beta2 = float , beta2 used by the model\n",
        "    epsilon = float\n",
        "    M,V = dict , previous updates of the model\n",
        "    t = int ,step count\n",
        "    Implements adam based gradient descent\n",
        "    '''\n",
        "    for l in range(1 ,self.n_layers):\n",
        "      M[\"W\" + str(l)] = beta1 *M[\"W\" + str(l)] + (1 - beta1) * self.grads[\"dW\" + str(l)]\n",
        "      M[\"b\" + str(l)] = beta1 *M[\"b\" + str(l)] + (1 - beta1) * self.grads[\"db\" + str(l)]\n",
        "      MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "      Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "\n",
        "\n",
        "      V[\"W\" + str(l)] = beta2 *V[\"W\" + str(l)] + (1 - beta2) * np.square(self.grads[\"dW\" + str(l)])\n",
        "      V[\"b\" + str(l)] = beta2 *V[\"b\" + str(l)] + (1 - beta2) * np.square(self.grads[\"db\" + str(l)])\n",
        "      VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "      Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "\n",
        "\n",
        "      factorW = learning_rate / (np.sqrt(VW_corrected) + epsilon)\n",
        "      factorb = learning_rate / (np.sqrt(Vb_corrected) + epsilon)\n",
        "      term1 = 1 - (beta1 ** (t))\n",
        "      term2 = (1 - beta1) * self.grads[\"dW\" + str(l)] / term1\n",
        "      term3 = (1 - beta1) * self.grads[\"db\" + str(l)] / term1\n",
        "\n",
        "      self.parameters[\"W\" + str(l)] = self.parameters[\"W\" + str(l)] - factorW * (beta1 * MW_corrected + term2)\n",
        "      self.parameters[\"b\" + str(l)] = self.parameters[\"b\" + str(l)] - factorb * (beta1 * Mb_corrected + term3)\n",
        "    return M,V,t + 1\n",
        "\n",
        "  def fit(self,learning_rate = 0.001,beta = 0.9,beta1 = 0.9,beta2 = 0.999 ,epsilon = 1e-6, optimizer = 'sgd',batch_size = 100,loss = 'cross_entropy',epochs = 20,wd = 0):\n",
        "    '''\n",
        "    learning_rate = float ,learning_rate used by the model to update the parameters\n",
        "    beta = float , beta used by the model\n",
        "    beta1 = float , beta1 used by the model\n",
        "    beta2 = float , beta2 used by the model\n",
        "    epsilon = float ,epsilon used by the model\n",
        "    optimizer = str, optimizer used by the model\n",
        "    batch_size = int, on how many data samples it is running the  algorithm\n",
        "    loss = str , which type of loss will occur\n",
        "    epochs = int , number of epochs over the dataset\n",
        "    It trains the model\n",
        "    '''\n",
        "    run_name = \"lr_{}_o_{}_b_{}_l_{}_w_i_{}_nhl_{}_sz_{}_a_{}_w_d_{}\".format(learning_rate , optimizer , batch_size ,loss,self.mode_of_initialization,self.n_layers - 2,self.num_neurons_in_hidden_layers, self.activation_function,wd)\n",
        "    print(\"run name = {}\".format(run_name))\n",
        "    wandb.run.name = run_name\n",
        "    TrainCostPerEpoch = []\n",
        "    TrainCostPerBatch = []\n",
        "    TrainAccuracyPerEpoch = []\n",
        "    ValCostPerEpoch = []\n",
        "    ValAccuracyPerEpoch = []\n",
        "\n",
        "    previous_updates = {}\n",
        "    M = {}\n",
        "    V = {}\n",
        "    for l in range(1 , self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      previous_updates[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    for l in range(1 , self.n_layers):\n",
        "      M[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      M[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "      V[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      V[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    t = 1\n",
        "\n",
        "\n",
        "    for count in range(epochs):\n",
        "      for i in range(0 , self.TrainInput.shape[1],batch_size):\n",
        "        if i + batch_size > self.TrainInput.shape[1]:\n",
        "          continue\n",
        "\n",
        "\n",
        "\n",
        "        if optimizer == 'nesterov_accelerated_gradient':\n",
        "          previous_updates = self.update_parameters_NAG(self.TrainInput[:,i:i + batch_size],self.TrainOutput[0,i : i + batch_size],learning_rate,beta,previous_updates,batch_size,loss,wd)\n",
        "          # print(previous_updates)\n",
        "        else:\n",
        "          parameters = self.parameters\n",
        "          yPredicted = self.forward(self.TrainInput[:,i:i + batch_size],self.activation_function,parameters)\n",
        "          e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])\n",
        "          self.backprop(yPredicted,e_y,batch_size,loss,self.activation_function,parameters,wd)\n",
        "          if optimizer == 'sgd':\n",
        "            self.update_parameters_sgd(learning_rate)\n",
        "          elif optimizer == 'momentum':\n",
        "            previous_updates = self.update_parameters_momentum(learning_rate,beta,previous_updates)\n",
        "          elif optimizer == 'RMSprop':\n",
        "            previous_updates = self.update_parameters_RMSprop(learning_rate,beta,epsilon,previous_updates)\n",
        "          elif optimizer == 'adam':\n",
        "            epsilon = 1e-8\n",
        "            M , V , t= self.update_parameters_adam(learning_rate,beta1,beta2,epsilon,M , V , t)\n",
        "          elif optimizer == 'nadam':\n",
        "            epsilon = 1e-8\n",
        "            M , V , t= self.update_parameters_nadam(learning_rate,beta1,beta2,epsilon,M , V , t,wd,batch_size)\n",
        "\n",
        "      yPredicted = self.forward(self.TrainInput,self.activation_function,self.parameters)\n",
        "      train_cost = self.lossFunction(self.TrainInput,self.TrainOutput,yPredicted,loss,self.TrainInput.shape[1],wd)\n",
        "      TrainCostPerEpoch.append(train_cost)\n",
        "\n",
        "      valYPredicted = self.forward(self.ValInput,self.activation_function,self.parameters)\n",
        "      val_cost = self.lossFunction(self.ValInput,self.ValOutput,valYPredicted,loss,self.ValInput.shape[1],wd)\n",
        "      ValCostPerEpoch.append(val_cost)\n",
        "\n",
        "      train_acc = self.accuracy(self.TrainInput, self.TrainOutput,yPredicted)\n",
        "      TrainAccuracyPerEpoch.append(train_acc)\n",
        "\n",
        "      val_acc = self.accuracy(self.ValInput, self.ValOutput,valYPredicted)\n",
        "      ValAccuracyPerEpoch.append(val_acc)\n",
        "\n",
        "\n",
        "      print(\"********************************\")\n",
        "      print(\"Epoch Number = {}\".format(count))\n",
        "      print(\"Training Accuracy = {}\".format(TrainAccuracyPerEpoch[-1]))\n",
        "      print(\"Validation Accuracy = {}\".format(ValAccuracyPerEpoch[-1]))\n",
        "      if math.isnan(train_cost) == True:\n",
        "        train_cost = 0\n",
        "      if math.isnan(val_cost) == True:\n",
        "        val_cost = 0\n",
        "      wandb.log({\"training_acc\": train_acc, \"validation_accuracy\": val_acc, \"training_loss\": train_cost, \"validation cost\": val_cost,\"Epoch\" : count})\n",
        "    plt.plot(TrainCostPerEpoch,label = 'Train cost')\n",
        "    plt.plot(ValCostPerEpoch , label = 'Validation cost')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('cost')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.plot(TrainAccuracyPerEpoch , label = 'Train Accuracy')\n",
        "    plt.plot(ValAccuracyPerEpoch , label = 'Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    # wandb.run.save()\n",
        "    # wandb.run.finish()\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def fitWithoutWandb(self,learning_rate = 0.001,beta = 0.9,beta1 = 0.9,beta2 = 0.999 ,epsilon = 1e-6, optimizer = 'sgd',batch_size = 100,loss = 'cross_entropy',epochs = 20,wd = 0):\n",
        "    '''\n",
        "    learning_rate = float ,learning_rate used by the model to update the parameters\n",
        "    beta = float , beta used by the model\n",
        "    beta1 = float , beta1 used by the model\n",
        "    beta2 = float , beta2 used by the model\n",
        "    epsilon = float ,epsilon used by the model\n",
        "    optimizer = str, optimizer used by the model\n",
        "    batch_size = int, on how many data samples it is running the  algorithm\n",
        "    loss = str , which type of loss will occur\n",
        "    epochs = int , number of epochs over the dataset\n",
        "    It trains the model , but does not plot with wandb like the fit function\n",
        "    '''\n",
        "    TrainCostPerEpoch = []\n",
        "    TrainCostPerBatch = []\n",
        "    TrainAccuracyPerEpoch = []\n",
        "    ValCostPerEpoch = []\n",
        "    ValAccuracyPerEpoch = []\n",
        "\n",
        "    previous_updates = {}\n",
        "    M = {}\n",
        "    V = {}\n",
        "    for l in range(1 , self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      previous_updates[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    for l in range(1 , self.n_layers):\n",
        "      M[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      M[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "      V[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      V[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    t = 1\n",
        "\n",
        "\n",
        "    for count in range(epochs):\n",
        "      for i in range(0 , self.TrainInput.shape[1],batch_size):\n",
        "        if i + batch_size > self.TrainInput.shape[1]:\n",
        "          continue\n",
        "\n",
        "\n",
        "\n",
        "        if optimizer == 'nesterov_accelerated_gradient':\n",
        "          previous_updates = self.update_parameters_NAG(self.TrainInput[:,i:i + batch_size],self.TrainOutput[0,i : i + batch_size],learning_rate,beta,previous_updates,batch_size,loss,wd)\n",
        "          # print(previous_updates)\n",
        "        else:\n",
        "          parameters = self.parameters\n",
        "          yPredicted = self.forward(self.TrainInput[:,i:i + batch_size],self.activation_function,parameters)\n",
        "          e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])\n",
        "          self.backprop(yPredicted,e_y,batch_size,loss,self.activation_function,parameters,wd)\n",
        "          if optimizer == 'sgd':\n",
        "            self.update_parameters_sgd(learning_rate)\n",
        "          elif optimizer == 'momentum':\n",
        "            previous_updates = self.update_parameters_momentum(learning_rate,beta,previous_updates)\n",
        "          elif optimizer == 'RMSprop':\n",
        "            previous_updates = self.update_parameters_RMSprop(learning_rate,beta,epsilon,previous_updates)\n",
        "          elif optimizer == 'adam':\n",
        "            epsilon = 1e-8\n",
        "            M , V , t= self.update_parameters_adam(learning_rate,beta1,beta2,epsilon,M , V , t)\n",
        "          elif optimizer == 'nadam':\n",
        "            epsilon = 1e-8\n",
        "            M , V , t= self.update_parameters_nadam(learning_rate,beta1,beta2,epsilon,M , V , t,wd,batch_size)\n",
        "\n",
        "      yPredicted = self.forward(self.TrainInput,self.activation_function,self.parameters)\n",
        "      train_cost = self.lossFunction(self.TrainInput,self.TrainOutput,yPredicted,loss,self.TrainInput.shape[1],wd)\n",
        "      TrainCostPerEpoch.append(train_cost)\n",
        "\n",
        "      valYPredicted = self.forward(self.ValInput,self.activation_function,self.parameters)\n",
        "      val_cost = self.lossFunction(self.ValInput,self.ValOutput,valYPredicted,loss,self.ValInput.shape[1],wd)\n",
        "      ValCostPerEpoch.append(val_cost)\n",
        "\n",
        "      train_acc = self.accuracy(self.TrainInput, self.TrainOutput,yPredicted)\n",
        "      TrainAccuracyPerEpoch.append(train_acc)\n",
        "\n",
        "      val_acc = self.accuracy(self.ValInput, self.ValOutput,valYPredicted)\n",
        "      ValAccuracyPerEpoch.append(val_acc)\n",
        "\n",
        "\n",
        "      print(\"********************************\")\n",
        "      print(\"Epoch Number = {}\".format(count))\n",
        "      print(\"Training Accuracy = {}\".format(TrainAccuracyPerEpoch[-1]))\n",
        "      print(\"Validation Accuracy = {}\".format(ValAccuracyPerEpoch[-1]))\n",
        "      if math.isnan(train_cost) == True:\n",
        "        train_cost = 0\n",
        "      if math.isnan(val_cost) == True:\n",
        "        val_cost = 0\n",
        "\n",
        "    return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# adam 0.5\n",
        "#nadam 0.01\n",
        "\n",
        "\n",
        "\n",
        "# NN = NeuralNetwork('random',3,128,'relu',x_trainT,y_trainT,x_valT,y_valT)\n",
        "\n",
        "# TrainCostPerEpoch,TrainAccuracyPerEpoch,ValCostPerEpoch,ValAccuracyPerEpoch = NN.fit(learning_rate = 0.001,epochs = 3,optimizer = 'sgd',loss = 'categorical_crossentropy')\n",
        "\n",
        "# ValCostPerEpoch\n",
        "\n",
        "def main():\n",
        "   '''\n",
        "   main function which runs sweep agent\n",
        "   '''\n",
        "   wandb.init(project = 'deep-learning')\n",
        "   config = wandb.config\n",
        "   NN = NeuralNetwork(mode_of_initialization = config.mode_of_initialization,number_of_hidden_layers = config.number_of_hidden_layers , num_neurons_in_hidden_layers = config.num_neurons_in_hidden_layers,activation = config.activation ,TrainInput = x_trainT,TrainOutput = y_trainT,ValInput = x_valT,ValOutput = y_valT)\n",
        "   NN.fit(learning_rate = config.learning_rate, beta = 0.9 , beta1 = 0.9 , beta2 = 0.999 , epsilon = 1e-6, optimizer = config.optimizer , batch_size = config.batch_size , loss = config.loss , epochs = config.epochs,wd = config.weight_decay)\n",
        "\n",
        "sweep_configuration = {\n",
        "    'method': 'bayes',\n",
        "    'name': 'ACCURACY VS EPOCH',\n",
        "    'metric': {\n",
        "        'goal': 'maximize',\n",
        "        'name': 'validation_accuracy'\n",
        "        },\n",
        "    'parameters': {\n",
        "        'mode_of_initialization': {'values': ['xavier','random']},\n",
        "        'number_of_hidden_layers' : {'values' : [3,4,5]},\n",
        "        'num_neurons_in_hidden_layers' : {'values' : [32,64,128]},\n",
        "\n",
        "        'learning_rate': {'values':[0.001,0.0001]},\n",
        "        'beta' : {'values' : [0.9,0.99,0.999]},\n",
        "        'optimizer' : {'values' : ['sgd','momentum','RMSprop','adam','nadam','nesterov_accelerated_gradient']},\n",
        "\n",
        "        'batch_size': {'values': [16,32,64]},\n",
        "        'epochs': {'values': [5,10]},\n",
        "        'loss' : {'values' : ['cross_entropy']},\n",
        "        'activation' : {'values' : ['sigmoid','relu','tanh']},\n",
        "        'weight_decay' : {'values' : [0, 0.0005,0.5]}\n",
        "       }\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"The below one is the best configuration\"\"\"\n",
        "\n",
        "NN = NeuralNetwork(mode_of_initialization = 'xavier',number_of_hidden_layers = 5 , num_neurons_in_hidden_layers = 128,activation = 'tanh' ,TrainInput = x_trainT,TrainOutput = y_trainT,ValInput = x_valT,ValOutput = y_valT)\n",
        "NN.fitWithoutWandb(learning_rate = 0.0001, beta = 0.9 , beta1 = 0.9 , beta2 = 0.999 , epsilon = 1e-6, optimizer = 'adam' , batch_size = 64 , loss = 'cross_entropy' , epochs = 10,wd = 0)\n",
        "\n",
        "yPredictedOnTest = NN.predict(x_testT)\n",
        "y_testT_e_y = np.transpose(np.eye(len(ImageClasses))[y_testT[0,:]])\n",
        "test_acc = NN.accuracy(x_testT, y_testT,yPredictedOnTest)\n",
        "print(\"test accuracy for the best model = {}\".format(test_acc))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "y1 = y_testT[0]\n",
        "y2 = np.argmax(yPredictedOnTest , axis = 0)\n",
        "new_y1 = []\n",
        "new_y2 = []\n",
        "for i in range(len(y1)):\n",
        "  new_y1.append(ImageClasses[y1[i]])\n",
        "for i in range(len(y2)):\n",
        "  new_y2.append(ImageClasses[y2[i]])\n",
        "\n",
        "wandb.init(project = 'Assignment 1')\n",
        "wandb.sklearn.plot_confusion_matrix(new_y1,new_y2,ImageClasses)"
      ],
      "metadata": {
        "id": "DaMub0m3GH2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install wandb\n",
        "\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score,ConfusionMatrixDisplay\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "'''normalizing the data'''\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "x_train_T = x_train.reshape(-1, x_train.shape[1]*x_train.shape[2]).T\n",
        "x_val_T = x_val.reshape(-1, x_val.shape[1]*x_val.shape[2]).T\n",
        "x_test_T = x_test.reshape(-1, x_test.shape[1]*x_test.shape[2]).T\n",
        "y_train_T, y_val_T, y_test_T = y_train.reshape(1, -1), y_val.reshape(1, -1), y_test.reshape(1, -1)\n",
        "\n",
        "class Util:\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_activation(A, activation):\n",
        "            if activation == 'sigmoid':\n",
        "                return Compute.sigmoid(A)\n",
        "            elif activation == 'relu':\n",
        "                return Compute.Relu(A)\n",
        "            elif activation == 'tanh':\n",
        "                return Compute.tanh(A)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(input, true_output, predicted_output, loss, batch_size,n_output):\n",
        "        if loss == 'cross_entropy':\n",
        "            one_hot_true_output = np.eye(n_output)[true_output[0]].T\n",
        "            return -np.sum(one_hot_true_output * np.log(predicted_output + 1e-9)) / batch_size\n",
        "\n",
        "\n",
        "        if loss=='squared_loss':\n",
        "            one_hot_true_output = np.eye(n_output)[true_output[0]].T\n",
        "            loss_factor=np.square(predicted_output-one_hot_true_output)\n",
        "            return np.sum(loss_factor)/batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(input, true_output, predicted_output):\n",
        "        predicted_labels = np.argmax(predicted_output, axis=0)\n",
        "        correct_predictions = np.sum(true_output == predicted_labels)\n",
        "        total_samples = true_output.shape[1]\n",
        "        accuracy_percentage = (correct_predictions / total_samples) * 100\n",
        "        return accuracy_percentage\n",
        "\n",
        "\n",
        "class Compute:\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return  1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu(x):\n",
        "        return np.maximum(x,0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return (2 * Compute.sigmoid(2 * x)) - 1\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(x):\n",
        "        return Compute.softmax(x) * (1-Compute.softmax(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(Z):\n",
        "        s = Compute.sigmoid(Z)\n",
        "        dA = s * (1 - s)\n",
        "        return dA\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu_derivative(x):\n",
        "        return 1*(x > 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(x):\n",
        "        return (1 - (Compute.tanh(x)**2))\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size):\n",
        "            dW = Compute.calculate_dW(dA, H_prev, batch_size)\n",
        "            db = Compute.calculate_db(dA, batch_size)\n",
        "            dH_prev, dA_prev = Compute.calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev) if k > 1 else (np.zeros(H_prev.shape), np.zeros(A_prev.shape))\n",
        "\n",
        "            return dW, db, dH_prev, dA_prev\n",
        "    @staticmethod\n",
        "    def calculate_dW(dA, H_prev, batch_size):\n",
        "            return np.dot(dA, H_prev.T) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_db(dA, batch_size):\n",
        "            return np.sum(dA, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev):\n",
        "            dH_prev = np.matmul(W.T, dA)\n",
        "            dA_prev = Compute.calculate_dA_prev(dH_prev, activation, A_prev)\n",
        "            return dH_prev, dA_prev\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dA_prev(dH_prev, activation, A_prev):\n",
        "            if activation == 'sigmoid':\n",
        "                return dH_prev * Compute.sigmoid_derivative(A_prev)\n",
        "            elif activation == 'tanh':\n",
        "                return dH_prev * Compute.tanh_derivative(A_prev)\n",
        "            elif activation == 'relu':\n",
        "                return dH_prev * Compute.Relu_derivative(A_prev)\n",
        "\n",
        "\n",
        "class Update:\n",
        "    @staticmethod\n",
        "    def stochastic_gradient_descent(eta,theta,grads,n_layers):\n",
        "        for l in range(1, n_layers):\n",
        "            W, dW = theta[\"W\" + str(l)], grads[\"dW\" + str(l)]\n",
        "            b, db = theta[\"b\" + str(l)],grads[\"db\" + str(l)]\n",
        "            W -= eta * dW\n",
        "            b -= eta * db\n",
        "            theta[\"W\" + str(l)], theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def nesterov_gradient_descent(my_network,i,eta, batch_size, beta, previous_updates,loss):\n",
        "        theta = {}\n",
        "        input_data = my_network.TrainInput[:, i:i + batch_size]\n",
        "        output_data = my_network.TrainOutput[0, i:i + batch_size]\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            theta[\"W\" + str(l)] = my_network.theta[\"W\" + str(l)] - beta * previous_updates[\"W\" + str(l)]\n",
        "            theta[\"b\" + str(l)] = my_network.theta[\"b\" + str(l)] - beta * previous_updates[\"b\" + str(l)]\n",
        "        y_predicted = my_network.forward(input_data, my_network.activation_function, my_network.theta)\n",
        "        e_y = np.transpose(np.eye(my_network.n_output)[output_data])\n",
        "        my_network.backpropagation(y_predicted, e_y, batch_size, loss, my_network.activation_function, my_network.theta)\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + (1-beta)*my_network.grads[\"dW\" + str(l)]\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + (1-beta)*my_network.grads[\"db\" + str(l)]\n",
        "            my_network.theta[\"W\" + str(l)] -= eta * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= eta * my_network.grads[\"db\" + str(l)]\n",
        "        return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def momentum_gradient_descent(my_network,eta, beta, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            uW, ub = previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)]\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            uW = beta * uW + (1-beta) * dW\n",
        "            ub = beta * ub + (1-beta) * db\n",
        "            W -= eta * uW\n",
        "            b -= eta * ub\n",
        "            previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)] = uW, ub\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "            return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def rms_prop(my_network,eta, beta, epsilon, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"dW\" + str(l)])\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"db\" + str(l)])\n",
        "            factorW = eta / (np.sqrt(previous_updates[\"W\" + str(l)] + epsilon))\n",
        "            factorb = eta / (np.sqrt(previous_updates[\"b\" + str(l)] + epsilon))\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * my_network.grads[\"db\" + str(l)]\n",
        "            return previous_updates\n",
        "            '''\n",
        "            Working previously fetched an issue that the previous_updates should be returned\n",
        "            if not then it is showing validation accuracy as 9.05%\n",
        "            but after returning this slightly better\n",
        "            '''\n",
        "\n",
        "    @staticmethod\n",
        "    def nadam(my_network,eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            term1 = 1 - (beta1 ** (t))\n",
        "            term2 = (1 - beta1) * my_network.grads[\"dW\" + str(l)] / term1\n",
        "            term3 = (1 - beta1) * my_network.grads[\"db\" + str(l)] / term1\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * (beta1 * MW_corrected + term2)\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * (beta1 * Mb_corrected + term3)\n",
        "        return M, V\n",
        "\n",
        "    @staticmethod\n",
        "    def adam(my_network,eta, beta1, beta2, epsilon, M, V, t): #taken from slide-2 page 42 [cs6910]\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            MW_hat = M[\"W\" + str(l)] / (1 - np.power(beta1,t))\n",
        "            Mb_hat = M[\"b\" + str(l)] / (1 - np.power(beta1,t))\n",
        "            VW_hat = V[\"W\" + str(l)] / (1 - np.power(beta2,t))\n",
        "            Vb_hat = V[\"b\" + str(l)] / (1 - np.power(beta2,t))\n",
        "            my_network.theta[\"W\" + str(l)] -= (eta / (np.sqrt(VW_hat) + epsilon)) * MW_hat\n",
        "            my_network.theta[\"b\" + str(l)] -= (eta / (np.sqrt(Vb_hat) + epsilon)) * Mb_hat\n",
        "        return M, V\n",
        "\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  theta = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "\n",
        "\n",
        "  def __init__(self,mode_of_initialization=\"random\",number_of_hidden_layers=1,num_neurons_in_hidden_layers=4,activation=\"sigmoid\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T):\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.theta[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.theta[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.theta[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def forward(self, X, activation, theta):\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1, self.n_layers):\n",
        "        H = self.cache[\"H\" + str(l - 1)]\n",
        "        W = self.theta[\"W\" + str(l)]\n",
        "        b = self.theta[\"b\" + str(l)]\n",
        "        A = np.dot(W, H) + b\n",
        "        self.cache[\"A\" + str(l)] = A\n",
        "        H = Util.apply_activation(A, activation)\n",
        "        self.cache[\"H\" + str(l)] = H\n",
        "    Al = self.cache[\"A\" + str(self.n_layers - 1)]\n",
        "    y_hat= Compute.softmax(Al)\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "\n",
        "  def backpropagation(self, y_predicted, e_y, batch_size, loss, activation, theta):\n",
        "        if loss == 'cross_entropy':\n",
        "            dA = y_predicted - e_y\n",
        "        elif loss=='squared_loss':\n",
        "            dA=(y_predicted - e_y)*Compute.softmax_derivative(self.cache[\"A\" + str(self.n_layers - 1)])\n",
        "        m = dA.shape[1]\n",
        "        self.grads[\"dA\" + str(self.n_layers - 1)] = dA\n",
        "\n",
        "        for k in range(self.n_layers - 1, 0, -1):\n",
        "            dA = self.grads[\"dA\" + str(k)]\n",
        "            H_prev = self.cache[\"H\" + str(k - 1)]\n",
        "            A_prev = self.cache[\"A\" + str(k - 1)]\n",
        "            W = self.theta[\"W\" + str(k)]\n",
        "\n",
        "            dW, db, dH_prev, dA_prev = Compute.calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size)\n",
        "\n",
        "            self.grads[\"dA\" + str(k - 1)] = dA_prev\n",
        "            self.grads[\"dW\" + str(k)] = dW\n",
        "            self.grads[\"db\" + str(k)] = db\n",
        "            # print(\"----\"*5,k,\"-----\"*5)\n",
        "            # print(\"dw\")\n",
        "            # print(dW)\n",
        "            # print(\"db\")\n",
        "            # print(db)\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def compute(self, eta = 0.1,beta = 0.5,beta1 = 0.5,beta2 = 0.5 ,epsilon = 0.000001, optimizer = 'sgd',batch_size = 4,loss = 'cross_entropy',epochs = 1):\n",
        "    train_c_epoch, tarin_acc_per_epoch, val_c_per_epoch, val_acc_per_epoch, previous_updates, M, V = [], [], [], [], {}, {}, {}\n",
        "    for l in range(1 , self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      previous_updates[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    for l in range(1 , self.n_layers):\n",
        "      M[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      M[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "      V[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      V[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    t = 1\n",
        "    for count in range(epochs):\n",
        "      for i in range(0 , self.TrainInput.shape[1],batch_size):\n",
        "        if i + batch_size > self.TrainInput.shape[1]:\n",
        "          continue\n",
        "        theta = self.theta\n",
        "        yPredicted = self.forward(self.TrainInput[:,i:i + batch_size],self.activation_function,theta)\n",
        "        e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])\n",
        "        self.backpropagation(yPredicted,e_y,batch_size,loss,self.activation_function,theta)\n",
        "        if optimizer == 'sgd':   #referred slide page 54\n",
        "            Update.stochastic_gradient_descent(eta,self.theta,self.grads,self.n_layers) #working\n",
        "        elif optimizer == 'nag':\n",
        "            previous_updates=Update.nesterov_gradient_descent(self,i,eta, batch_size, beta, previous_updates,loss) #working\n",
        "\n",
        "        elif optimizer == 'momentum': #referred from slide 43\n",
        "          previous_updates=Update.momentum_gradient_descent(self,eta,beta,previous_updates) #working\n",
        "\n",
        "        elif optimizer == 'RMSprop':\n",
        "          previous_updates=Update.rms_prop(self,eta,beta,epsilon,previous_updates) #working\n",
        "        elif optimizer == 'adam':\n",
        "          epsilon = 1e-10\n",
        "          M , V = Update.adam(self,eta,beta1,beta2,epsilon,M , V , count+1)\n",
        "        elif optimizer == 'nadam':\n",
        "          epsilon = 1e-8\n",
        "          M , V = Update.nadam(self,eta,beta1,beta2,epsilon,M , V , count+1)\n",
        "\n",
        "      y_hat = self.forward(self.TrainInput,self.activation_function,self.theta)\n",
        "      valy_hat = self.forward(self.ValInput,self.activation_function,self.theta)\n",
        "      train_cost = Util.loss(self.TrainInput,self.TrainOutput,y_hat,loss,self.TrainInput.shape[1],self.n_output)\n",
        "      train_c_epoch.append(train_cost)\n",
        "      val_cost = Util.loss(self.ValInput,self.ValOutput,valy_hat,loss,self.ValInput.shape[1],self.n_output)\n",
        "      val_c_per_epoch.append(val_cost)\n",
        "      train_acc = Util.accuracy(self.TrainInput, self.TrainOutput,y_hat)\n",
        "      tarin_acc_per_epoch.append(train_acc)\n",
        "\n",
        "      val_acc = Util.accuracy(self.ValInput, self.ValOutput,valy_hat)\n",
        "      val_acc_per_epoch.append(val_acc)\n",
        "    #   print(np.eye(self.n_output)[self.ValOutput[0]].T.shape,valy_hat.shape)\n",
        "      conf_matrix = confusion_matrix(np.argmax(np.eye(self.n_output)[self.ValOutput[0]].T, axis=0),\n",
        "                                np.argmax(valy_hat, axis=0))\n",
        "\n",
        "      '''\n",
        "        # Print confusion matrix\n",
        "      cm_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"T-shirt/top\",\n",
        "                                                                        \"Trouser\",\n",
        "                                                                        \"Pullover\",\n",
        "                                                                        \"Dress\",\n",
        "                                                                        \"Coat\",\n",
        "                                                                        \"Sandal\",\n",
        "                                                                        \"Shirt\",\n",
        "                                                                        \"Sneaker\",\n",
        "                                                                        \"Bag\",\n",
        "                                                                        \"Ankle boot\"])\n",
        "\n",
        "        # Customize appearance\n",
        "      cm_display.plot(cmap='Oranges')\n",
        "      plt.xlabel('Predicted Label', fontsize=14)\n",
        "      plt.ylabel('True Label', fontsize=14)\n",
        "      plt.title('Confusion Matrix', fontsize=16)\n",
        "      plt.xticks(rotation=45, ha='right')\n",
        "      plt.tight_layout()\n",
        "      wandb.log({\"Confusion Matrix at epoch\"+str(count+1) : plt})\n",
        "      '''\n",
        "\n",
        "\n",
        "      print(\"---------\"*20)\n",
        "      print(f\"Epoch Number = {format(count+1)}\")\n",
        "      print(f\"Training Accuracy = {format(tarin_acc_per_epoch[-1])}\")\n",
        "      print(f\"Validation Accuracy = {format(val_acc_per_epoch[-1])}\")\n",
        "    #   if(count==1):\n",
        "    #     print(self.cache[\"A1\"])\n",
        "      wandb.log({\"training_accuracy\": train_acc,\"validation_accuracy\": val_acc,\"training_loss\":train_cost,\"validation_loss\": val_cost,\"epoch\": count})\n",
        "    return train_c_epoch,tarin_acc_per_epoch,val_c_per_epoch,val_acc_per_epoch\n",
        "\n",
        "\n",
        "# my_network = MyNeuralNetwork(mode_of_initialization=\"xavier\",number_of_hidden_layers=4,num_neurons_in_hidden_layers=32,activation=\"tanh\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T)\n",
        "# train=my_network.compute(eta = 0.01,beta = 0.9,beta1 = 0.1,beta2 = 0.1 ,epsilon = 0.05, optimizer = 'sgd',batch_size = 32,loss = 'cross_entropy',epochs = 5)\n",
        "'''\n",
        "The below code is for creating the sweep feature\n",
        "'''\n",
        "\n",
        "def train():\n",
        "    wandb.init(project = \"deep-learning-assignment-1\")\n",
        "    config = wandb.config\n",
        "    run_name=\"init_\"+(config.mode_of_initialization)+\"_l\"+str(config.number_of_hidden_layers)+\"_node_\"+str(config.num_neurons_in_hidden_layers)+\"_act_\"+config.activation+\"_eta_\"+str(config.eta)+\"_beta_\"+str(config.beta)+\"_opt_\"+config.optimizer+\"_bs_\"+str(config.batch_size)+\"_loss_\"+config.loss+\"_ep_\"+str(config.epochs)\n",
        "    with wandb.init(project=\"deep-learning-assignment-1\", name=run_name) as run:\n",
        "        my_network = MyNeuralNetwork(mode_of_initialization=config.mode_of_initialization,number_of_hidden_layers=config.number_of_hidden_layers,num_neurons_in_hidden_layers=config.num_neurons_in_hidden_layers,activation=config.activation,TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T)\n",
        "        my_network.compute(eta =config.eta,beta = config.beta,beta1 = 0.5,beta2 = 0.5 ,epsilon = 0.05, optimizer = config.optimizer,batch_size =config.batch_size,loss = config.loss,epochs = config.epochs)\n",
        "\n",
        "\n",
        "sweep_config = {\n",
        "    'method': 'random',\n",
        "    'name': 'accuracy sweep',\n",
        "    'metric': {\n",
        "        'goal': 'maximize',\n",
        "        'name': 'validation_accuracy'\n",
        "        },\n",
        "    'parameters': {\n",
        "        'mode_of_initialization': {'values': ['xavier','random']},\n",
        "        'number_of_hidden_layers' : {'values' : [3,4,5]},\n",
        "        'num_neurons_in_hidden_layers' : {'values' : [32,64,128]},\n",
        "\n",
        "        'eta': {'values':[0.01,0.1]},\n",
        "        'beta' : {'values' : [0.05,0.9,0.99]},\n",
        "        'optimizer' : {'values' : ['sgd']},\n",
        "\n",
        "        'batch_size': {'values': [16,32,64]},\n",
        "        'epochs': {'values': [5,10]},\n",
        "        'loss' : {'values' : ['cross_entropy']},\n",
        "        'activation' : {'values' : ['sigmoid','relu','tanh']},\n",
        "        'weight_decay' : {'values' : [0,0.0005,0.5]}\n",
        "       }\n",
        "    }\n",
        "\n",
        "\n",
        "sweep_id = wandb.sweep(sweep_config, project=\"deep-learning-assignment-1\")\n",
        "wandb.agent(sweep_id , function = train , count = 5)\n",
        "\n"
      ],
      "metadata": {
        "id": "EhrubtZQGz7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DkXp1JcMf0Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vkb1WqXFfwEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G-upHJi-1e2H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMz7wg6tVVZyTmZYm+GOmB6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1e1ae32c5865475e8d44fc27f4563045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_981974721d46417f93377ffa9ee70ac7",
              "IPY_MODEL_a90d5e163fdd4430aeccda5f3e6f3552"
            ],
            "layout": "IPY_MODEL_75e928fbf21343caa15dad9b0b23785e"
          }
        },
        "981974721d46417f93377ffa9ee70ac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ddbd2c66dcf48c28c3e18d6fcda8905",
            "placeholder": "​",
            "style": "IPY_MODEL_93aae286f90b4fe2be389b8fa264b996",
            "value": "Waiting for wandb.init()...\r"
          }
        },
        "a90d5e163fdd4430aeccda5f3e6f3552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0711d25ffd9b419cb02bba788482e403",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5761f3014df441b19a81484f13a2f1c2",
            "value": 0.022282265455558243
          }
        },
        "75e928fbf21343caa15dad9b0b23785e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ddbd2c66dcf48c28c3e18d6fcda8905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93aae286f90b4fe2be389b8fa264b996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0711d25ffd9b419cb02bba788482e403": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5761f3014df441b19a81484f13a2f1c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}