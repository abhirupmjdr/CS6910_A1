{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgXiQq+rfV7zlKmS0jRhrZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "67c3bdd60aa24a959a627cea76711874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e8ec92c40ac24dccb4cba596b3569ee0",
              "IPY_MODEL_e891ab6d917c4cf190b3dac70a8c158b"
            ],
            "layout": "IPY_MODEL_7e128612481d4aa8b5865e30608f24ae"
          }
        },
        "e8ec92c40ac24dccb4cba596b3569ee0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_738e1b309dba41e5b9a8fb525182a6eb",
            "placeholder": "​",
            "style": "IPY_MODEL_9fce7205f4a448c8a8081ea7506b328c",
            "value": "0.014 MB of 0.014 MB uploaded\r"
          }
        },
        "e891ab6d917c4cf190b3dac70a8c158b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ff34774bf8c40adafc29dcb0061f85e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b0b6fcc435c40198ee13c8ad017d9c7",
            "value": 1
          }
        },
        "7e128612481d4aa8b5865e30608f24ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "738e1b309dba41e5b9a8fb525182a6eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9fce7205f4a448c8a8081ea7506b328c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3ff34774bf8c40adafc29dcb0061f85e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b0b6fcc435c40198ee13c8ad017d9c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhirupmjdr/CS6910_A1/blob/main/dl_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJFmZOitTGsi",
        "outputId": "fd70488f-2724-46fd-908d-a95b94435605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7917198301022377 -2.2821180729166732\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Gradinat Descent (Mitesh Khapra)\n",
        "import numpy as np\n",
        "X = [0.5,2.5]\n",
        "Y = [0.2,0.9]\n",
        "\n",
        "def fun(x,w,b):\n",
        "    return 1/(1+np.exp(-(w*x+b)))\n",
        "\n",
        "def grad_w(x,y,w,b):\n",
        "    fx=fun(x,w,b)\n",
        "    return (fx-y)*fx*(1-fx)*x\n",
        "\n",
        "def grad_b(x,y,w,b):\n",
        "    fx=fun(x,w,b)\n",
        "    return (fx-y)*fx*(1-fx)\n",
        "\n",
        "def error(w,b):\n",
        "    err=0.0\n",
        "    n=0\n",
        "    for x,y in zip(X,Y):\n",
        "        err+=(fun(x,w,b)-y)**2\n",
        "        n+=1\n",
        "    return err/n\n",
        "\n",
        "def gradiant_descent():\n",
        "    w,b,eta,max_itr=-2,-2,1.0,1000\n",
        "    for i in range (max_itr):\n",
        "        dw,db=0,0\n",
        "        for x,y in zip(X,Y):\n",
        "            dw+=grad_w(x,y,w,b)\n",
        "            db+=grad_b(x,y,w,b)\n",
        "        w=w-eta*dw\n",
        "        b=b-eta*db\n",
        "    if i==999:\n",
        "        print(w,b)\n",
        "        print(error(w,b))\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    gradiant_descent()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2meJgX2p700u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHrEpPC5XKHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5824fc-8aab-4c7f-bee7-7a9e421d44d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.11994532 0.13682959 0.06105959 ... 0.13723868 0.59235629 0.13689276]\n",
            " [0.25163098 0.25992294 0.28016269 ... 0.25403603 0.12895459 0.57248959]\n",
            " [0.03669064 0.03684221 0.03513156 ... 0.03789504 0.01426023 0.0039766 ]\n",
            " ...\n",
            " [0.02844356 0.0500063  0.02622639 ... 0.02750218 0.04976222 0.0100033 ]\n",
            " [0.19163651 0.14171315 0.19899944 ... 0.19362844 0.04255918 0.1139606 ]\n",
            " [0.04519285 0.07596379 0.06984453 ... 0.04543387 0.0215163  0.02550157]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Question5.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/11gmNfKBG39ABwUVhkJr-KnagzUBnwk4z\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import seaborn as sns\n",
        "'''\n",
        "10 distinct images of fashion_mnist class is written and then wandb is initialized and the images are plotted\n",
        "'''\n",
        "ImageClasses = [\"Pullover\",\"Shirt\",\"Coat\",\"Trouser\",\"Dress\",\"Sandal\",\"Bag\",\"Sneaker\",\"Ankle boot\",\"T-shirt/top\"]\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "'''normalizing the data'''\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "x_valT = np.transpose(x_val.reshape(x_val.shape[0] , x_val.shape[1] * x_val.shape[2]))\n",
        "x_trainT = np.transpose(x_train.reshape(x_train.shape[0] , x_train.shape[1] * x_train.shape[2]))\n",
        "x_testT = np.transpose(x_test.reshape(x_test.shape[0] , x_test.shape[1] * x_test.shape[2]))\n",
        "y_trainT = y_train.reshape(1 , y_train.shape[0])\n",
        "y_valT = y_val.reshape(1 , y_val.shape[0])\n",
        "y_testT = y_test.reshape(1 , y_test.shape[0])\n",
        "\n",
        "class ActivationFunction:\n",
        "  '''all activation functions are defined here'''\n",
        "  def sigmoid(x):\n",
        "    return  1 / (1 + np.exp(-x))\n",
        "  def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "  def Relu(x):\n",
        "    return np.maximum(x,0)\n",
        "  def tanh(x):\n",
        "    return np.tanh(x)\n",
        "  def softmax_derivative(x):\n",
        "    return ActivationFunction.softmax(x) * (1-ActivationFunction.softmax(x))\n",
        "  def sigmoid_derivative(Z):\n",
        "    s = 1 /(1 + np.exp(-Z))\n",
        "    dA = s * (1 - s)\n",
        "    return dA\n",
        "  def Relu_derivative(x):\n",
        "    return 1*(x > 0)\n",
        "  def tanh_derivative(x):\n",
        "    return (1 - (np.tanh(x)**2))\n",
        "\n",
        "class NeuralNetwork:\n",
        "  '''\n",
        "  mode_of_initialization =str , The way the weights are initialized\n",
        "  n_layers = int ,total number of layers\n",
        "  activation_function = str , activation function of the neural network,default is sigmoid activation function\n",
        "  n_input = int ,number of inputs\n",
        "  n_outputs = int , number of outputs\n",
        "  num_neurns_in_hidden_layer = int ,how many neurons are in each hidden layer\n",
        "  n_neurons = list having number of neurons in each layer\n",
        "  TrainInput = input layer's input\n",
        "  TrainOutput = output layer's output\n",
        "  ValInput = Validation Input\n",
        "  ValOutput = validation output\n",
        "  parameters = dict ,stores the parameters of the model\n",
        "  cache = dict , stores the H and A for each layer which help in gradient computation\n",
        "  grads = dict ,store the gradients\n",
        "    '''\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  num_neurons_in_hidden_layers = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  parameters = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "  def __init__(self,mode_of_initialization = 'random',number_of_hidden_layers = 1,num_neurons_in_hidden_layers = 4,activation = 'sigmoid',TrainInput = x_trainT,TrainOutput = y_trainT,ValInput = x_valT,ValOutput = y_valT):\n",
        "\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    self.num_neurons_in_hidden_layers = num_neurons_in_hidden_layers\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.parameters[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.parameters[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.parameters[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def output(self,A):\n",
        "    '''\n",
        "    uses softmax function in the last layer for calculating the prediction\n",
        "    '''\n",
        "    return ActivationFunction.softmax(A)\n",
        "\n",
        "\n",
        "  def forward(self,X,activation,parameters):\n",
        "    '''\n",
        "    calculates H and A for each layer and stores them into cache dictionary. It uses the parameter dictionary .\n",
        "    '''\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1 , self.n_layers):\n",
        "      H = self.cache[\"H\" + str(l - 1)]\n",
        "      # print(H.shape)\n",
        "      W = self.parameters[\"W\" + str(l)]\n",
        "      b = self.parameters[\"b\" + str(l)]\n",
        "      A = np.dot(W , H) + b\n",
        "      # print(W.shape)\n",
        "      # print(b.shape)\n",
        "      self.cache[\"A\" + str(l)] = A\n",
        "      if activation == 'sigmoid':\n",
        "         H = ActivationFunction.sigmoid(A)\n",
        "      elif activation == 'relu':\n",
        "         H = ActivationFunction.Relu(A)\n",
        "      elif activation == 'tanh':\n",
        "         H = ActivationFunction.tanh(A)\n",
        "      self.cache[\"H\" + str(l)] = H\n",
        "    yPredicted = self.output(self.cache[\"A\" + str(self.n_layers - 1)])\n",
        "    return yPredicted\n",
        "\n",
        "  def predict(self,input):\n",
        "    '''\n",
        "    predicts the class of the image , although result is not int , it is probability\n",
        "    '''\n",
        "    H = input\n",
        "    activation = self.activation_function\n",
        "    for l in range(1 , self.n_layers - 1):\n",
        "      # print(H.shape)\n",
        "      W = self.parameters[\"W\" + str(l)]\n",
        "      b = self.parameters[\"b\" + str(l)]\n",
        "      A = np.dot(W , H) + b\n",
        "      # print(W.shape)\n",
        "      # print(b.shape)\n",
        "      if activation == 'sigmoid':\n",
        "         H = ActivationFunction.sigmoid(A)\n",
        "      elif activation == 'relu':\n",
        "         H = ActivationFunction.Relu(A)\n",
        "      elif activation == 'tanh':\n",
        "         H = ActivationFunction.tanh(A)\n",
        "\n",
        "\n",
        "    W = self.parameters[\"W\" + str(self.n_layers - 1)]\n",
        "    b = self.parameters[\"b\" + str(self.n_layers - 1)]\n",
        "    A = np.dot(W , H) + b\n",
        "    y_predicted = self.output(A)\n",
        "    return y_predicted\n",
        "\n",
        "NN = NeuralNetwork()\n",
        "\n",
        "print(NN.predict(x_trainT))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gzb7eJ0k3X4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "(train_input, train_output), (test_input, test_output) = fashion_mnist.load_data()\n",
        "train_input = train_input / 255.0\n",
        "test_input = test_input / 255.0\n",
        "\n",
        "# Assuming 'data' is your dataset and 'labels' are corresponding labels/targets\n",
        "train_input, test_input, train_output, test_output = train_test_split(\n",
        "    train_input, train_output, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_inputT = np.transpose(train_input.reshape(train_input.shape[0], -1))\n",
        "\n",
        "class Activation:\n",
        "\n",
        "    def sigmoid(x):\n",
        "        x = np.clip(x, -500, 500)\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(x):\n",
        "        exp_values = np.exp(x - np.max(x))\n",
        "        return exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
        "\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "\n",
        "class Neural_Network:\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers=1,\n",
        "        hidden_layer_nodes=4,\n",
        "        activation_function=\"sigmoid\",\n",
        "        x_train=train_inputT,\n",
        "        y_train=train_output,\n",
        "        x_test=test_input,\n",
        "        y_test=test_output,\n",
        "    ):\n",
        "        self.layers = layers\n",
        "        self.hidden_layer_nodes = hidden_layer_nodes\n",
        "        self.activation_function = activation_function\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        self.decode = {\"h0\": x_train}\n",
        "        self.decode[\"A0\"] = x_train\n",
        "        self.n_input = x_train.shape[0]\n",
        "        self.n_output = 10  # Assuming you have 10 classes in Fashion MNIST\n",
        "        self.nodes_in_level = [self.n_input]  # Initialize with input layer nodes\n",
        "        self.thetas = {}\n",
        "        for l in range(1, self.layers + 2):\n",
        "            self.nodes_in_level.append(hidden_layer_nodes) if l < self.layers + 1 else self.nodes_in_level.append(\n",
        "                self.n_output\n",
        "            )\n",
        "            self.thetas[\"W\" + str(l)] = np.random.randn(\n",
        "                self.nodes_in_level[l], self.nodes_in_level[l - 1]\n",
        "            )\n",
        "            self.thetas[\"b\" + str(l)] = np.zeros((self.nodes_in_level[l], 1))\n",
        "\n",
        "    def forward_propagation(self):\n",
        "        for k in range(1, self.layers + 2):\n",
        "            W = self.thetas[\"W\" + str(k)]\n",
        "            b = self.thetas[\"b\" + str(k)]\n",
        "            h = self.decode[\"h\" + str(k - 1)]\n",
        "            A = b + np.dot(W, h)\n",
        "            if self.activation_function == \"sigmoid\":\n",
        "                h = Activation.sigmoid(A)\n",
        "            elif self.activation_function == \"tanh\":\n",
        "                h = Activation.tanh(A)\n",
        "            self.decode[\"h\" + str(k)] = h\n",
        "            self.decode[\"A\" + str(k)] = A\n",
        "\n",
        "        y_hat = Activation.softmax(self.decode[\"A\" + str(self.layers + 1)])\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "neural_network = Neural_Network()\n",
        "result = neural_network.forward_propagation()\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhYfErKwx7OU",
        "outputId": "34cf77d4-1ef7-4092-a4ba-35c5a03b7451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.10013348 0.00676222 0.02234989 ... 0.0397687  0.02069737 0.00749427]\n",
            " [0.10013738 0.15123605 0.39413149 ... 0.03664666 0.19659424 0.30401039]\n",
            " [0.09979417 0.0482745  0.00955078 ... 0.05159459 0.00698927 0.05777833]\n",
            " ...\n",
            " [0.09991326 0.14251926 0.0278301  ... 0.28297623 0.12459664 0.03887115]\n",
            " [0.10009272 0.04417972 0.182001   ... 0.04951272 0.13784489 0.06064374]\n",
            " [0.10002165 0.07634184 0.08359702 ... 0.05792878 0.07231635 0.08949334]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question 3\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "(train_input, train_output), (test_input, test_output) = fashion_mnist.load_data()\n",
        "\n",
        "train_input = train_input / 255.0\n",
        "test_input = test_input / 255.0\n",
        "\n",
        "# Assuming 'data' is your dataset and 'labels' are corresponding labels/targets\n",
        "train_input, test_input, train_output, test_output = train_test_split(\n",
        "    train_input, train_output, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_inputT = np.transpose(train_input.reshape(train_input.shape[0], -1))\n",
        "\n",
        "class Activation:\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        # Use np.clip to prevent overflow issues\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        exp_values = np.exp(x - np.max(x))\n",
        "        return exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "\n",
        "\n",
        "class Neural_Network:\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers=10,\n",
        "        hidden_layer_nodes=8,\n",
        "        activation_function=\"sigmoid\",\n",
        "        x_train=train_inputT,\n",
        "        y_train=train_output,\n",
        "        x_test=test_input,\n",
        "        y_test=test_output,\n",
        "    ):\n",
        "        self.layers = layers\n",
        "        self.hidden_layer_nodes = hidden_layer_nodes\n",
        "        self.activation_function = activation_function\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        self.decode = {\"h0\": x_train}\n",
        "        self.decode[\"A0\"] = x_train\n",
        "        self.n_input = x_train.shape[0]\n",
        "        self.n_output = 10  # Assuming you have 10 classes in Fashion MNIST\n",
        "        self.nodes_in_level = [self.n_input]  # Initialize with input layer nodes\n",
        "        self.thetas = {}\n",
        "        self.derivatives={}\n",
        "        self.previous_updates={}\n",
        "        for l in range(1,self.layers+2):\n",
        "            self.previous_updates[\"W\"+str(l)]=np.zeros(self.nodes_in_level[l], self.nodes_in_level[l - 1])\n",
        "            self.previous_updates[\"b\"+str(l)]=np.zeros(self.nodes_in_level[l],1)\n",
        "        self.y_hat=[]\n",
        "        for l in range(1, self.layers + 2):\n",
        "            self.nodes_in_level.append(hidden_layer_nodes) if l < self.layers + 1 else self.nodes_in_level.append(\n",
        "                self.n_output\n",
        "            )\n",
        "            self.thetas[\"W\" + str(l)] = np.random.randn(\n",
        "                self.nodes_in_level[l], self.nodes_in_level[l - 1]\n",
        "            )\n",
        "            self.thetas[\"b\" + str(l)] = np.zeros((self.nodes_in_level[l], 1))\n",
        "            # printf(self.thetas[\"W1\"])\n",
        "\n",
        "    def forward_propagation(self):\n",
        "        for k in range(1, self.layers +  2):\n",
        "            W = self.thetas[\"W\" + str(k)]\n",
        "            b = self.thetas[\"b\" + str(k)]\n",
        "            h = self.decode[\"h\" + str(k - 1)]\n",
        "            A = b + np.dot(W, h)\n",
        "            # print(\"dim of A = \",A.shape)\n",
        "            if self.activation_function == \"sigmoid\":\n",
        "                h = Activation.sigmoid(A)\n",
        "            elif self.activation_function == \"tanh\":\n",
        "                h = Activation.tanh(A)\n",
        "            # print(\"printing h in\",k)\n",
        "            # print(h)\n",
        "            self.decode[\"h\" + str(k)] = h\n",
        "            self.decode[\"A\" + str(k)] = A\n",
        "\n",
        "        self.y_hat = Activation.softmax(self.decode[\"A\" + str(self.layers + 1)])\n",
        "        return self.y_hat\n",
        "\n",
        "\n",
        "    def backward_propagation(self):\n",
        "        e_l = np.transpose(np.eye(10)[self.y_train[:]])\n",
        "        # print(\"dim of e_l\",e_l.shape)\n",
        "        # print(\"dim of y_hat\",self.y_hat.shape)\n",
        "        dA = (self.y_hat - e_l)\n",
        "        # print(\"dim of dA\", dA.shape)\n",
        "        self.derivatives[\"dA\" + str(self.layers + 1)] = dA\n",
        "        for k in range(self.layers +1, 0, -1):\n",
        "            dA = self.derivatives[\"dA\" + str(k)]\n",
        "            h_prev = self.decode[\"h\" + str(k - 1)]\n",
        "            A_prev = self.decode[\"A\" + str(k - 1)]\n",
        "            W = self.thetas[\"W\" + str(k)]\n",
        "            dW = np.zeros(W.shape)\n",
        "            db = np.zeros((W.shape[0],1))\n",
        "            dh_prev = np.zeros(h_prev.shape)\n",
        "            dA_prev = np.zeros(A_prev.shape)\n",
        "            dW = np.dot(dA, h_prev.T)\n",
        "            self.derivatives[\"dW\" + str(k)] = dW\n",
        "            db = np.sum(dA, axis=1, keepdims=True)\n",
        "            self.derivatives[\"db\" + str(k)] = db\n",
        "            if k >= 2:\n",
        "                dh_prev = np.matmul(W.T,dA)\n",
        "                if self.activation_function == \"sigmoid\":\n",
        "                    dA_prev = np.multiply(dh_prev, (Activation.sigmoid(A_prev) * (1 - Activation.sigmoid(A_prev))))\n",
        "            self.derivatives[\"dA\" + str(k - 1)] = dA_prev\n",
        "\n",
        "    def do_mgd(self,eta,beta):\n",
        "        for l in range(1,self.layers+2):\n",
        "            self.previous_updates[\"W\"+str(l)]=beta*self.previous_updates[\"W\"+str(l)]+self.derivatives[\"dW\"+str(l)]\n",
        "            self.previous_updates[\"b\"+str(l)]=beta*self.previous_updates[\"b\"+str(l)]+self.derivatives[\"db\"+str(l)]\n",
        "            self.thetas[\"W\"+str(l)]=self.thetas[\"W\"+str(l)]-eta*self.previous_updates[\"W\"+str(l)]\n",
        "            self.thetas[\"b\"+str(l)]=self.thetas[\"b\"+str(l)]-eta*self.previous_updates[\"b\"+str(l)]\n",
        "\n",
        "\n",
        "\n",
        "    def compute(self):\n",
        "        epoch = 100\n",
        "        eta = 0.01\n",
        "        for iter in range(epoch):\n",
        "            self.forward_propagation()\n",
        "            self.backward_propagation()\n",
        "            # Update weights and biases\n",
        "            for l in range(1,self.layers):\n",
        "                W = self.thetas[\"W\" + str(l)]\n",
        "                dW = self.derivatives[\"dW\" + str(l)]\n",
        "                W = W - eta * dW\n",
        "                # print(eta*dW)\n",
        "                self.thetas[\"W\" + str(l)] = W\n",
        "                b = self.thetas[\"b\" + str(l)]\n",
        "                db = self.derivatives[\"db\" + str(l)]\n",
        "                b = b - eta * db\n",
        "                self.thetas[\"b\" + str(l)] = b\n",
        "            if iter >=0 :\n",
        "                OneHotOfTrueOutput = np.transpose(np.eye(10)[self.y_train[:]])\n",
        "                sum = -np.sum(OneHotOfTrueOutput * np.log(self.y_hat), axis=0)\n",
        "                print(sum)\n",
        "\n",
        "\n",
        "\n",
        "neural_network = Neural_Network()\n",
        "neural_network.compute()\n",
        "\n"
      ],
      "metadata": {
        "id": "so5OWDZD3Yjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#question 1\n",
        "# !pip install wandb\n",
        "# import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# wandb.login()\n",
        "# wandb.init(\n",
        "#       # Set the project where this run will be logged\n",
        "#       project=\"deep-learning\",\n",
        "#       # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "#       # Track hyperparameters and run metadata\n",
        "#       config={\n",
        "#       \"epochs\": 1\n",
        "#       })\n",
        "\n",
        "(train_input, train_output), (test_input,test_output) = fashion_mnist.load_data()\n",
        "\n",
        "class_names = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\"\n",
        "}\n",
        "\n",
        "# Plot one sample image for each class\n",
        "class_counts = np.bincount(train_output)\n",
        "total=len(train_output)\n",
        "for i in range(len(class_names)):\n",
        "    # Find the first image with the corresponding class label\n",
        "    idx=np.where(train_output==i)[0][0]\n",
        "    # plotting the image\n",
        "    plt.subplot(2,5,i+1)\n",
        "    plt.imshow(train_input[idx],cmap='gray')\n",
        "    plt.title(class_names[i])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "# wandb.log({\"Sample Images\": plt})"
      ],
      "metadata": {
        "id": "1MCZ5quDV-9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#question 2\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Reshape the data for the neural network\n",
        "x_val_T = np.transpose(x_val.reshape(x_val.shape[0], -1))\n",
        "x_train_T = np.transpose(x_train.reshape(x_train.shape[0], -1))\n",
        "x_test_T = np.transpose(x_test.reshape(x_test.shape[0], -1))\n",
        "y_train_T = y_train.reshape(1, -1)\n",
        "y_val_T = y_val.reshape(1, -1)\n",
        "y_test_T = y_test.reshape(1, -1)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, init_mode='random', num_hidden_layers=6, neurons_per_hidden_layer=32,\n",
        "                 activation_function='sigmoid', train_input=x_train_T, train_output=y_train_T, val_input=x_val_T,\n",
        "                 val_output=y_val_T):\n",
        "        self.init_mode = init_mode\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.neurons_per_hidden_layer = neurons_per_hidden_layer\n",
        "        self.activation_function = activation_function\n",
        "        self.train_input = train_input\n",
        "        self.train_output = train_output\n",
        "        self.val_input = val_input\n",
        "        self.val_output = val_output\n",
        "        self.n_layers = num_hidden_layers + 2\n",
        "        self.n_input = train_input.shape[0]\n",
        "        self.n_output = np.max(train_output) + 1\n",
        "        self.n_neurons = [self.n_input] + [neurons_per_hidden_layer] * num_hidden_layers + [self.n_output]\n",
        "        self.cache = {\"H0\": train_input, \"A0\": train_input}\n",
        "        self.theta = {}\n",
        "        self.grads = {}\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        for l in range(1, self.n_layers):\n",
        "            if self.init_mode == \"random\":\n",
        "                self.theta[f\"W{l}\"] = np.random.randn(self.n_neurons[l], self.n_neurons[l - 1])\n",
        "            self.theta[f\"b{l}\"] = np.zeros((self.n_neurons[l], 1))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(x, 0)\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def feedforward(self, input_data):\n",
        "        H = input_data\n",
        "        activation = self.activation_function\n",
        "        for l in range(1, self.n_layers - 1):\n",
        "            W = self.theta[f\"W{l}\"]\n",
        "            b = self.theta[f\"b{l}\"]\n",
        "            A = np.dot(W, H) + b\n",
        "            H = getattr(self, activation)(A)\n",
        "        W = self.theta[f\"W{self.n_layers - 1}\"]\n",
        "        b = self.theta[f\"b{self.n_layers - 1}\"]\n",
        "        A = np.dot(W, H) + b\n",
        "        y_predicted = np.exp(A) / np.sum(np.exp(A), axis=0)\n",
        "        return y_predicted\n",
        "\n",
        "# Create an instance of the NeuralNetwork class\n",
        "my_model = NeuralNetwork()\n",
        "\n",
        "# Perform a feedforward pass on the training data\n",
        "my_model.feedforward(x_train_T)"
      ],
      "metadata": {
        "id": "J5lGVSyLWPGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip install wandb\n",
        "\n",
        "# import wandb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# wandb.login()\n",
        "\n",
        "# wandb.init(\n",
        "#     # set the wandb project where this run will be logged\n",
        "#     project=\"deep-learning\",\n",
        "\n",
        "#     # track hyperparameters and run metadata\n",
        "#     config={\n",
        "#     \"eta\": 0.02\n",
        "#     }\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.datasets import m\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "'''normalizing the data'''\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "x_train_T = x_train.reshape(-1, x_train.shape[1]*x_train.shape[2]).T\n",
        "x_val_T = x_val.reshape(-1, x_val.shape[1]*x_val.shape[2]).T\n",
        "x_test_T = x_test.reshape(-1, x_test.shape[1]*x_test.shape[2]).T\n",
        "y_train_T, y_val_T, y_test_T = y_train.reshape(1, -1), y_val.reshape(1, -1), y_test.reshape(1, -1)\n",
        "\n",
        "\n",
        "\n",
        "class Compute:\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return  1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu(x):\n",
        "        return np.maximum(x,0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(x):\n",
        "        return Compute.softmax(x) * (1-Compute.softmax(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(Z):\n",
        "        s = Compute.sigmoid(Z)\n",
        "        dA = s * (1 - s)\n",
        "        return dA\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu_derivative(x):\n",
        "        return 1*(x > 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(x):\n",
        "        return (1 - (np.tanh(x)**2))\n",
        "\n",
        "\n",
        "class Update:\n",
        "    @staticmethod\n",
        "    def simple_gradient_descent(eta):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            W -= eta * dW\n",
        "            b -= eta * db\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss):\n",
        "        theta = {}\n",
        "        input_data = my_network.TrainInput[:, i:i + batch_size]\n",
        "        output_data = my_network.TrainOutput[0, i:i + batch_size]\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            theta[\"W\" + str(l)] = my_network.theta[\"W\" + str(l)] - beta * previous_updates[\"W\" + str(l)]\n",
        "            theta[\"b\" + str(l)] = my_network.theta[\"b\" + str(l)] - beta * previous_updates[\"b\" + str(l)]\n",
        "        y_predicted = my_network.forward(input_data, my_network.activation_function, my_network.theta)\n",
        "        e_y = np.transpose(np.eye(my_network.n_output)[output_data])\n",
        "        my_network.backpropagation(y_predicted, e_y, batch_size, loss, my_network.activation_function, my_network.theta)\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + my_network.grads[\"dW\" + str(l)]\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + my_network.grads[\"db\" + str(l)]\n",
        "            my_network.theta[\"W\" + str(l)] -= eta * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= eta * my_network.grads[\"db\" + str(l)]\n",
        "        return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def momentum_gradient_descent(eta, beta, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            uW, ub = previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)]\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            uW = beta * uW + dW\n",
        "            ub = beta * ub + db\n",
        "            W -= eta * uW\n",
        "            b -= eta * ub\n",
        "            previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)] = uW, ub\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def rms_prop(eta, beta, epsilon, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"dW\" + str(l)])\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"db\" + str(l)])\n",
        "            factorW = eta / (np.sqrt(previous_updates[\"W\" + str(l)] + epsilon))\n",
        "            factorb = eta / (np.sqrt(previous_updates[\"b\" + str(l)] + epsilon))\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * my_network.grads[\"db\" + str(l)]\n",
        "\n",
        "    @staticmethod\n",
        "    def nadam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            term1 = 1 - (beta1 ** (t))\n",
        "            term2 = (1 - beta1) * my_network.grads[\"dW\" + str(l)] / term1\n",
        "            term3 = (1 - beta1) * my_network.grads[\"db\" + str(l)] / term1\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * (beta1 * MW_corrected + term2)\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * (beta1 * Mb_corrected + term3)\n",
        "        return M, V, t + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def adam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** t))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** t))\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** t))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** t))\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * MW_corrected\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * Mb_corrected\n",
        "        return M, V, t + 1\n",
        "\n",
        "\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  theta = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "\n",
        "\n",
        "  def __init__(self,mode_of_initialization=\"random\",number_of_hidden_layers=6,num_neurons_in_hidden_layers=32,activation=\"sigmoid\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T):\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.theta[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.theta[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.theta[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def forward(self, X, activation, theta):\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1, self.n_layers):\n",
        "        H = self.cache[\"H\" + str(l - 1)]\n",
        "        W = self.theta[\"W\" + str(l)]\n",
        "        b = self.theta[\"b\" + str(l)]\n",
        "        A = np.dot(W, H) + b\n",
        "        self.cache[\"A\" + str(l)] = A\n",
        "        H = self.apply_activation(A, activation)\n",
        "        self.cache[\"H\" + str(l)] = H\n",
        "    Al = self.cache[\"A\" + str(self.n_layers - 1)]\n",
        "    y_hat= Compute.softmax(Al)\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "  def apply_activation(self, A, activation):\n",
        "        if activation == 'sigmoid':\n",
        "            return Compute.sigmoid(A)\n",
        "        elif activation == 'relu':\n",
        "            return Compute.Relu(A)\n",
        "        elif activation == 'tanh':\n",
        "            return Compute.tanh(A)\n",
        "\n",
        "  def loss(self, input, true_output, predicted_output, loss, batch_size):\n",
        "    if loss == 'cross_entropy':\n",
        "        one_hot_true_output = np.eye(self.n_output)[true_output[0]].T\n",
        "        loss_value = -np.sum(one_hot_true_output * np.log(predicted_output + 1e-9)) / batch_size\n",
        "    return loss_value\n",
        "\n",
        "  def accuracy(self, input, true_output, predicted_output):\n",
        "    predicted_labels = np.argmax(predicted_output, axis=0)\n",
        "    correct_predictions = np.sum(true_output == predicted_labels)\n",
        "    total_samples = true_output.shape[1]\n",
        "    accuracy_percentage = (correct_predictions / total_samples) * 100\n",
        "    return accuracy_percentage\n",
        "\n",
        "  def backpropagation(self, y_predicted, e_y, batch_size, loss, activation, theta):\n",
        "        if loss == 'cross_entropy':\n",
        "            dA = y_predicted - e_y\n",
        "        m = dA.shape[1]\n",
        "        self.grads[\"dA\" + str(self.n_layers - 1)] = dA\n",
        "\n",
        "        for k in range(self.n_layers - 1, 0, -1):\n",
        "            dA = self.grads[\"dA\" + str(k)]\n",
        "            H_prev = self.cache[\"H\" + str(k - 1)]\n",
        "            A_prev = self.cache[\"A\" + str(k - 1)]\n",
        "            W = self.theta[\"W\" + str(k)]\n",
        "\n",
        "            dW, db, dH_prev, dA_prev = self.calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size)\n",
        "\n",
        "            self.grads[\"dA\" + str(k - 1)] = dA_prev\n",
        "            self.grads[\"dW\" + str(k)] = dW\n",
        "            self.grads[\"db\" + str(k)] = db\n",
        "\n",
        "        return\n",
        "\n",
        "  def calculate_gradients(self, k, dA, H_prev, A_prev, W, activation, batch_size):\n",
        "        dW = self.calculate_dW(dA, H_prev, batch_size)\n",
        "        db = self.calculate_db(dA, batch_size)\n",
        "        dH_prev, dA_prev = self.calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev) if k > 1 else (np.zeros(H_prev.shape), np.zeros(A_prev.shape))\n",
        "\n",
        "        return dW, db, dH_prev, dA_prev\n",
        "\n",
        "  def calculate_dW(self, dA, H_prev, batch_size):\n",
        "        return np.dot(dA, H_prev.T) / batch_size\n",
        "\n",
        "  def calculate_db(self, dA, batch_size):\n",
        "        return np.sum(dA, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "  def calculate_dH_prev_dA_prev(self, k, W, dA, activation, A_prev):\n",
        "        dH_prev = np.matmul(W.T, dA)\n",
        "        dA_prev = self.calculate_dA_prev(dH_prev, activation, A_prev)\n",
        "        return dH_prev, dA_prev\n",
        "\n",
        "  def calculate_dA_prev(self, dH_prev, activation, A_prev):\n",
        "        if activation == 'sigmoid':\n",
        "            return dH_prev * Compute.sigmoid_derivative(A_prev)\n",
        "        elif activation == 'tanh':\n",
        "            return dH_prev * Compute.tanh_derivative(A_prev)\n",
        "        elif activation == 'relu':\n",
        "            return dH_prev * Compute.Relu_derivative(A_prev)\n",
        "\n",
        "\n",
        "  def compute(self, eta = 0.001,beta = 0.9,beta1 = 0.9,beta2 = 0.999 ,epsilon = 1e-6, optimizer = 'sgd',batch_size = 100,loss = 'cross_entropy',epochs = 20):\n",
        "    train_c_epoch, tarin_acc_per_epoch, val_c_per_epoch, val_acc_per_epoch, previous_updates, M, V = [], [], [], [], {}, {}, {}\n",
        "    for l in range(1 , self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      previous_updates[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    for l in range(1 , self.n_layers):\n",
        "      M[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      M[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "      V[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      V[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    t = 1\n",
        "    for count in range(epochs):\n",
        "      for i in range(0 , self.TrainInput.shape[1],batch_size):\n",
        "        if i + batch_size > self.TrainInput.shape[1]:\n",
        "          continue\n",
        "        theta = self.theta\n",
        "        yPredicted = self.forward(self.TrainInput[:,i:i + batch_size],self.activation_function,theta)\n",
        "        e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])\n",
        "        self.backpropagation(yPredicted,e_y,batch_size,loss,self.activation_function,theta)\n",
        "        if optimizer == 'sgd':   #referred slide page 54\n",
        "            Update.simple_gradient_descent(eta)\n",
        "        elif optimizer == 'nag':\n",
        "            previous_updates=Update.nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss)\n",
        "\n",
        "        elif optimizer == 'momentum': #referred from slide 43\n",
        "          Update.momentum_gradient_descent(eta,beta,previous_updates)\n",
        "\n",
        "        elif optimizer == 'RMSprop':\n",
        "          Update.rms_prop(eta,beta,epsilon,previous_updates)\n",
        "        elif optimizer == 'adam':\n",
        "          epsilon = 1e-8\n",
        "          M , V , t= Update.adam(eta,beta1,beta2,epsilon,M , V , t)\n",
        "        elif optimizer == 'nadam':\n",
        "          epsilon = 1e-8\n",
        "          M , V , t= Update.nadam(eta,beta1,beta2,epsilon,M , V , t)\n",
        "\n",
        "      y_hat = self.forward(self.TrainInput,self.activation_function,self.theta)\n",
        "      valy_hat = self.forward(self.ValInput,self.activation_function,self.theta)\n",
        "      train_cost = self.loss(self.TrainInput,self.TrainOutput,y_hat,loss,self.TrainInput.shape[1])\n",
        "      train_c_epoch.append(train_cost)\n",
        "      val_cost = self.loss(self.ValInput,self.ValOutput,valy_hat,loss,self.ValInput.shape[1])\n",
        "      val_c_per_epoch.append(val_cost)\n",
        "      train_acc = self.accuracy(self.TrainInput, self.TrainOutput,y_hat)\n",
        "      tarin_acc_per_epoch.append(train_acc)\n",
        "\n",
        "      val_acc = self.accuracy(self.ValInput, self.ValOutput,valy_hat)\n",
        "      val_acc_per_epoch.append(val_acc)\n",
        "\n",
        "    #   if count==epochs-1:\n",
        "        # conf_data=confusion_matrix(self.ValOutput[0], valy_hat[0])\n",
        "        # seaborn.heatmap(conf_data, annot=True, cmap='Blues', fmt='g')\n",
        "        # plt.xlabel('Predicted labels')\n",
        "        # plt.ylabel('True labels')\n",
        "        # plt.title('Confusion Matrix')\n",
        "        # plt.show()\n",
        "\n",
        "    #   print(\"---------\"*20)\n",
        "    #   print(f\"Epoch Number = {format(count+1)}\")\n",
        "    #   print(f\"Training Accuracy = {format(tarin_acc_per_epoch[-1])}\")\n",
        "    #   print(f\"Validation Accuracy = {format(val_acc_per_epoch[-1])}\")\n",
        "      # wandb.log({\"training_accuracy\": train_acc,\"validation_accuracy\": val_acc,\"training_loss\":train_cost,\"validation_loss\": val_cost,\"epoch\": count})\n",
        "    return train_c_epoch,tarin_acc_per_epoch,val_c_per_epoch,val_acc_per_epoch\n",
        "\n",
        "\n",
        "my_network = MyNeuralNetwork()\n",
        "my_network.compute(eta = 0.01, beta = 0.9 , beta1 = 0.9 , beta2 = 0.999 , epsilon = 0.001, optimizer = 'sgd' , batch_size = 16 , loss = 'cross_entropy' , epochs = 1)\n"
      ],
      "metadata": {
        "id": "K33zpd9y716C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip install wandb\n",
        "\n",
        "# import wandb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score,ConfusionMatrixDisplay\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# wandb.login()\n",
        "\n",
        "# wandb.init(\n",
        "#     # set the wandb project where this run will be logged\n",
        "#     project=\"deep-learning\",\n",
        "\n",
        "#     # track hyperparameters and run metadata\n",
        "#     config={\n",
        "#     \"eta\": 0.02\n",
        "#     }\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "'''normalizing the data'''\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "x_train_T = x_train.reshape(-1, x_train.shape[1]*x_train.shape[2]).T\n",
        "x_val_T = x_val.reshape(-1, x_val.shape[1]*x_val.shape[2]).T\n",
        "x_test_T = x_test.reshape(-1, x_test.shape[1]*x_test.shape[2]).T\n",
        "y_train_T, y_val_T, y_test_T = y_train.reshape(1, -1), y_val.reshape(1, -1), y_test.reshape(1, -1)\n",
        "\n",
        "class Util:\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_activation(A, activation):\n",
        "            if activation == 'sigmoid':\n",
        "                return Compute.sigmoid(A)\n",
        "            elif activation == 'relu':\n",
        "                return Compute.Relu(A)\n",
        "            elif activation == 'tanh':\n",
        "                return Compute.tanh(A)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(input, true_output, predicted_output, loss, batch_size):\n",
        "        if loss == 'cross_entropy':\n",
        "            one_hot_true_output = np.eye(my_network.n_output)[true_output[0]].T\n",
        "            return -np.sum(one_hot_true_output * np.log(predicted_output + 1e-9)) / batch_size\n",
        "\n",
        "\n",
        "        if loss=='squared_loss':\n",
        "            one_hot_true_output = np.eye(my_network.n_output)[true_output[0]].T\n",
        "            loss_factor=np.square(predicted_output-one_hot_true_output)\n",
        "            return np.sum(loss_factor)/batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(input, true_output, predicted_output):\n",
        "        predicted_labels = np.argmax(predicted_output, axis=0)\n",
        "        correct_predictions = np.sum(true_output == predicted_labels)\n",
        "        total_samples = true_output.shape[1]\n",
        "        accuracy_percentage = (correct_predictions / total_samples) * 100\n",
        "        return accuracy_percentage\n",
        "\n",
        "class Compute:\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return  1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu(x):\n",
        "        return np.maximum(x,0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return (2 * Compute.sigmoid(2 * x)) - 1\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(x):\n",
        "        return Compute.softmax(x) * (1-Compute.softmax(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(Z):\n",
        "        s = Compute.sigmoid(Z)\n",
        "        dA = s * (1 - s)\n",
        "        return dA\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu_derivative(x):\n",
        "        return 1*(x > 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(x):\n",
        "        return (1 - (Compute.tanh(x)**2))\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size):\n",
        "            dW = Compute.calculate_dW(dA, H_prev, batch_size)\n",
        "            db = Compute.calculate_db(dA, batch_size)\n",
        "            dH_prev, dA_prev = Compute.calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev) if k > 1 else (np.zeros(H_prev.shape), np.zeros(A_prev.shape))\n",
        "\n",
        "            return dW, db, dH_prev, dA_prev\n",
        "    @staticmethod\n",
        "    def calculate_dW(dA, H_prev, batch_size):\n",
        "            return np.dot(dA, H_prev.T) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_db(dA, batch_size):\n",
        "            return np.sum(dA, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev):\n",
        "            dH_prev = np.matmul(W.T, dA)\n",
        "            dA_prev = Compute.calculate_dA_prev(dH_prev, activation, A_prev)\n",
        "            return dH_prev, dA_prev\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dA_prev(dH_prev, activation, A_prev):\n",
        "            if activation == 'sigmoid':\n",
        "                return dH_prev * Compute.sigmoid_derivative(A_prev)\n",
        "            elif activation == 'tanh':\n",
        "                return dH_prev * Compute.tanh_derivative(A_prev)\n",
        "            elif activation == 'relu':\n",
        "                return dH_prev * Compute.Relu_derivative(A_prev)\n",
        "\n",
        "\n",
        "class Update:\n",
        "    @staticmethod\n",
        "    def simple_gradient_descent(eta):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            W -= eta * dW\n",
        "            b -= eta * db\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss):\n",
        "        theta = {}\n",
        "        input_data = my_network.TrainInput[:, i:i + batch_size]\n",
        "        output_data = my_network.TrainOutput[0, i:i + batch_size]\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            theta[\"W\" + str(l)] = my_network.theta[\"W\" + str(l)] - beta * previous_updates[\"W\" + str(l)]\n",
        "            theta[\"b\" + str(l)] = my_network.theta[\"b\" + str(l)] - beta * previous_updates[\"b\" + str(l)]\n",
        "        y_predicted = my_network.forward(input_data, my_network.activation_function, my_network.theta)\n",
        "        e_y = np.transpose(np.eye(my_network.n_output)[output_data])\n",
        "        my_network.backpropagation(y_predicted, e_y, batch_size, loss, my_network.activation_function, my_network.theta)\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + my_network.grads[\"dW\" + str(l)]\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + my_network.grads[\"db\" + str(l)]\n",
        "            my_network.theta[\"W\" + str(l)] -= eta * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= eta * my_network.grads[\"db\" + str(l)]\n",
        "        return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def momentum_gradient_descent(eta, beta, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            uW, ub = previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)]\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            uW = beta * uW + dW\n",
        "            ub = beta * ub + db\n",
        "            W -= eta * uW\n",
        "            b -= eta * ub\n",
        "            previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)] = uW, ub\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def rms_prop(eta, beta, epsilon, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"dW\" + str(l)])\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"db\" + str(l)])\n",
        "            factorW = eta / (np.sqrt(previous_updates[\"W\" + str(l)] + epsilon))\n",
        "            factorb = eta / (np.sqrt(previous_updates[\"b\" + str(l)] + epsilon))\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * my_network.grads[\"db\" + str(l)]\n",
        "\n",
        "    @staticmethod\n",
        "    def nadam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            term1 = 1 - (beta1 ** (t))\n",
        "            term2 = (1 - beta1) * my_network.grads[\"dW\" + str(l)] / term1\n",
        "            term3 = (1 - beta1) * my_network.grads[\"db\" + str(l)] / term1\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * (beta1 * MW_corrected + term2)\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * (beta1 * Mb_corrected + term3)\n",
        "        return M, V, t + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def adam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** t))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** t))\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** t))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** t))\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * MW_corrected\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * Mb_corrected\n",
        "        return M, V, t + 1\n",
        "\n",
        "\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  theta = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "\n",
        "\n",
        "  def __init__(self,mode_of_initialization=\"random\",number_of_hidden_layers=1,num_neurons_in_hidden_layers=4,activation=\"sigmoid\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T):\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.theta[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.theta[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.theta[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def forward(self, X, activation, theta):\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1, self.n_layers):\n",
        "        H = self.cache[\"H\" + str(l - 1)]\n",
        "        W = self.theta[\"W\" + str(l)]\n",
        "        b = self.theta[\"b\" + str(l)]\n",
        "        A = np.dot(W, H) + b\n",
        "        self.cache[\"A\" + str(l)] = A\n",
        "        H = Util.apply_activation(A, activation)\n",
        "        self.cache[\"H\" + str(l)] = H\n",
        "    Al = self.cache[\"A\" + str(self.n_layers - 1)]\n",
        "    y_hat= Compute.softmax(Al)\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "\n",
        "  def backpropagation(self, y_predicted, e_y, batch_size, loss, activation, theta):\n",
        "        if loss == 'cross_entropy':\n",
        "            dA = y_predicted - e_y\n",
        "        elif loss=='squared_loss':\n",
        "            dA=(y_predicted - e_y)*Compute.softmax_derivative(self.cache[\"A\" + str(self.n_layers - 1)])\n",
        "        m = dA.shape[1]\n",
        "        self.grads[\"dA\" + str(self.n_layers - 1)] = dA\n",
        "\n",
        "        for k in range(self.n_layers - 1, 0, -1):\n",
        "            dA = self.grads[\"dA\" + str(k)]\n",
        "            H_prev = self.cache[\"H\" + str(k - 1)]\n",
        "            A_prev = self.cache[\"A\" + str(k - 1)]\n",
        "            W = self.theta[\"W\" + str(k)]\n",
        "\n",
        "            dW, db, dH_prev, dA_prev = Compute.calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size)\n",
        "\n",
        "            self.grads[\"dA\" + str(k - 1)] = dA_prev\n",
        "            self.grads[\"dW\" + str(k)] = dW\n",
        "            self.grads[\"db\" + str(k)] = db\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def compute(self, eta = 0.1,beta = 0.5,beta1 = 0.5,beta2 = 0.5 ,epsilon = 0.000001, optimizer = 'sgd',batch_size = 4,loss = 'cross_entropy',epochs = 1):\n",
        "    train_c_epoch, tarin_acc_per_epoch, val_c_per_epoch, val_acc_per_epoch, previous_updates, M, V = [], [], [], [], {}, {}, {}\n",
        "    for l in range(1 , self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      previous_updates[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    for l in range(1 , self.n_layers):\n",
        "      M[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      M[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "      V[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      V[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    t = 1\n",
        "    for count in range(epochs):\n",
        "      for i in range(0 , self.TrainInput.shape[1],batch_size):\n",
        "        if i + batch_size > self.TrainInput.shape[1]:\n",
        "          continue\n",
        "        theta = self.theta\n",
        "        yPredicted = self.forward(self.TrainInput[:,i:i + batch_size],self.activation_function,theta)\n",
        "        e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])\n",
        "        self.backpropagation(yPredicted,e_y,batch_size,loss,self.activation_function,theta)\n",
        "        if optimizer == 'sgd':   #referred slide page 54\n",
        "            Update.simple_gradient_descent(eta)\n",
        "        elif optimizer == 'nag':\n",
        "            previous_updates=Update.nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss)\n",
        "\n",
        "        elif optimizer == 'momentum': #referred from slide 43\n",
        "          Update.momentum_gradient_descent(eta,beta,previous_updates)\n",
        "\n",
        "        elif optimizer == 'RMSprop':\n",
        "          Update.rms_prop(eta,beta,epsilon,previous_updates)\n",
        "        elif optimizer == 'adam':\n",
        "          epsilon = 1e-8\n",
        "          M , V , t= Update.adam(eta,beta1,beta2,epsilon,M , V , t)\n",
        "        elif optimizer == 'nadam':\n",
        "          epsilon = 1e-8\n",
        "          M , V , t= Update.nadam(eta,beta1,beta2,epsilon,M , V , t)\n",
        "\n",
        "      y_hat = self.forward(self.TrainInput,self.activation_function,self.theta)\n",
        "      valy_hat = self.forward(self.ValInput,self.activation_function,self.theta)\n",
        "      train_cost = Util.loss(self.TrainInput,self.TrainOutput,y_hat,loss,self.TrainInput.shape[1])\n",
        "      train_c_epoch.append(train_cost)\n",
        "      val_cost = Util.loss(self.ValInput,self.ValOutput,valy_hat,loss,self.ValInput.shape[1])\n",
        "      val_c_per_epoch.append(val_cost)\n",
        "      train_acc = Util.accuracy(self.TrainInput, self.TrainOutput,y_hat)\n",
        "      tarin_acc_per_epoch.append(train_acc)\n",
        "\n",
        "      val_acc = Util.accuracy(self.ValInput, self.ValOutput,valy_hat)\n",
        "      val_acc_per_epoch.append(val_acc)\n",
        "    #   print(np.eye(self.n_output)[self.ValOutput[0]].T.shape,valy_hat.shape)\n",
        "      conf_matrix = confusion_matrix(np.argmax(np.eye(self.n_output)[self.ValOutput[0]].T, axis=0),\n",
        "                                np.argmax(valy_hat, axis=0))\n",
        "\n",
        "      '''\n",
        "        # Print confusion matrix\n",
        "      cm_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"T-shirt/top\",\n",
        "                                                                        \"Trouser\",\n",
        "                                                                        \"Pullover\",\n",
        "                                                                        \"Dress\",\n",
        "                                                                        \"Coat\",\n",
        "                                                                        \"Sandal\",\n",
        "                                                                        \"Shirt\",\n",
        "                                                                        \"Sneaker\",\n",
        "                                                                        \"Bag\",\n",
        "                                                                        \"Ankle boot\"])\n",
        "\n",
        "        # Customize appearance\n",
        "      cm_display.plot(cmap='Oranges')\n",
        "      plt.xlabel('Predicted Label', fontsize=14)\n",
        "      plt.ylabel('True Label', fontsize=14)\n",
        "      plt.title('Confusion Matrix', fontsize=16)\n",
        "      plt.xticks(rotation=45, ha='right')\n",
        "      plt.tight_layout()\n",
        "      wandb.log({\"Confusion Matrix at epoch\"+str(count+1) : plt})\n",
        "      '''\n",
        "\n",
        "\n",
        "      print(\"---------\"*20)\n",
        "      print(f\"Epoch Number = {format(count+1)}\")\n",
        "      print(f\"Training Accuracy = {format(tarin_acc_per_epoch[-1])}\")\n",
        "      print(f\"Validation Accuracy = {format(val_acc_per_epoch[-1])}\")\n",
        "    #   wandb.log({\"training_accuracy\": train_acc,\"validation_accuracy\": val_acc,\"training_loss\":train_cost,\"validation_loss\": val_cost,\"epoch\": count})\n",
        "    return train_c_epoch,tarin_acc_per_epoch,val_c_per_epoch,val_acc_per_epoch\n",
        "\n",
        "\n",
        "my_network = MyNeuralNetwork()\n",
        "my_network.compute(epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "67c3bdd60aa24a959a627cea76711874",
            "e8ec92c40ac24dccb4cba596b3569ee0",
            "e891ab6d917c4cf190b3dac70a8c158b",
            "7e128612481d4aa8b5865e30608f24ae",
            "738e1b309dba41e5b9a8fb525182a6eb",
            "9fce7205f4a448c8a8081ea7506b328c",
            "3ff34774bf8c40adafc29dcb0061f85e",
            "2b0b6fcc435c40198ee13c8ad017d9c7"
          ]
        },
        "id": "pAS--NwnqTa5",
        "outputId": "861fdfa2-7bbd-4e8f-9813-fb69f6a25997"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.16.3)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.42)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.40.5)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:9w9lj1qp) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67c3bdd60aa24a959a627cea76711874"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>training_accuracy</td><td>▁▃▄▆█</td></tr><tr><td>training_loss</td><td>█▆▅▃▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▅▆█</td></tr><tr><td>validation_loss</td><td>█▆▅▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>training_accuracy</td><td>40.81111</td></tr><tr><td>training_loss</td><td>0.75997</td></tr><tr><td>validation_accuracy</td><td>39.73333</td></tr><tr><td>validation_loss</td><td>0.76405</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">glowing-envelope-3</strong> at: <a href='https://wandb.ai/abhirupmjdr_dl/deep-learning/runs/9w9lj1qp' target=\"_blank\">https://wandb.ai/abhirupmjdr_dl/deep-learning/runs/9w9lj1qp</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240223_100816-9w9lj1qp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:9w9lj1qp). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.3"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240223_100946-523friiz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abhirupmjdr_dl/deep-learning/runs/523friiz' target=\"_blank\">golden-wonton-4</a></strong> to <a href='https://wandb.ai/abhirupmjdr_dl/deep-learning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abhirupmjdr_dl/deep-learning' target=\"_blank\">https://wandb.ai/abhirupmjdr_dl/deep-learning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abhirupmjdr_dl/deep-learning/runs/523friiz' target=\"_blank\">https://wandb.ai/abhirupmjdr_dl/deep-learning/runs/523friiz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 1\n",
            "Training Accuracy = 48.62222222222222\n",
            "Validation Accuracy = 48.18333333333334\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 2\n",
            "Training Accuracy = 58.031481481481485\n",
            "Validation Accuracy = 57.18333333333333\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 3\n",
            "Training Accuracy = 64.23333333333333\n",
            "Validation Accuracy = 63.083333333333336\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 4\n",
            "Training Accuracy = 68.01296296296296\n",
            "Validation Accuracy = 67.21666666666667\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 5\n",
            "Training Accuracy = 70.28703703703704\n",
            "Validation Accuracy = 69.66666666666667\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1.383659175677465,\n",
              "  1.1403124093123735,\n",
              "  0.9845121335048668,\n",
              "  0.8847753726818004,\n",
              "  0.8194863984674912],\n",
              " [48.62222222222222,\n",
              "  58.031481481481485,\n",
              "  64.23333333333333,\n",
              "  68.01296296296296,\n",
              "  70.28703703703704],\n",
              " [1.3821989744612944,\n",
              "  1.142778123472919,\n",
              "  0.992954518823647,\n",
              "  0.8952897325716765,\n",
              "  0.8289281525758702],\n",
              " [48.18333333333334,\n",
              "  57.18333333333333,\n",
              "  63.083333333333336,\n",
              "  67.21666666666667,\n",
              "  69.66666666666667])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qgu3vx9MV6zN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}