{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMHOmzZY3JtN0oWHDnrfElc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abhirupmjdr/CS6910_A1/blob/main/dl_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJFmZOitTGsi",
        "outputId": "fd70488f-2724-46fd-908d-a95b94435605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.7917198301022377 -2.2821180729166732\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# Gradinat Descent (Mitesh Khapra)\n",
        "import numpy as np\n",
        "X = [0.5,2.5]\n",
        "Y = [0.2,0.9]\n",
        "\n",
        "def fun(x,w,b):\n",
        "    return 1/(1+np.exp(-(w*x+b)))\n",
        "\n",
        "def grad_w(x,y,w,b):\n",
        "    fx=fun(x,w,b)\n",
        "    return (fx-y)*fx*(1-fx)*x\n",
        "\n",
        "def grad_b(x,y,w,b):\n",
        "    fx=fun(x,w,b)\n",
        "    return (fx-y)*fx*(1-fx)\n",
        "\n",
        "def error(w,b):\n",
        "    err=0.0\n",
        "    n=0\n",
        "    for x,y in zip(X,Y):\n",
        "        err+=(fun(x,w,b)-y)**2\n",
        "        n+=1\n",
        "    return err/n\n",
        "\n",
        "def gradiant_descent():\n",
        "    w,b,eta,max_itr=-2,-2,1.0,1000\n",
        "    for i in range (max_itr):\n",
        "        dw,db=0,0\n",
        "        for x,y in zip(X,Y):\n",
        "            dw+=grad_w(x,y,w,b)\n",
        "            db+=grad_b(x,y,w,b)\n",
        "        w=w-eta*dw\n",
        "        b=b-eta*db\n",
        "    if i==999:\n",
        "        print(w,b)\n",
        "        print(error(w,b))\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    gradiant_descent()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2meJgX2p700u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHrEpPC5XKHv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df5824fc-8aab-4c7f-bee7-7a9e421d44d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.11994532 0.13682959 0.06105959 ... 0.13723868 0.59235629 0.13689276]\n",
            " [0.25163098 0.25992294 0.28016269 ... 0.25403603 0.12895459 0.57248959]\n",
            " [0.03669064 0.03684221 0.03513156 ... 0.03789504 0.01426023 0.0039766 ]\n",
            " ...\n",
            " [0.02844356 0.0500063  0.02622639 ... 0.02750218 0.04976222 0.0100033 ]\n",
            " [0.19163651 0.14171315 0.19899944 ... 0.19362844 0.04255918 0.1139606 ]\n",
            " [0.04519285 0.07596379 0.06984453 ... 0.04543387 0.0215163  0.02550157]]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"Question5.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/11gmNfKBG39ABwUVhkJr-KnagzUBnwk4z\n",
        "\"\"\"\n",
        "\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import seaborn as sns\n",
        "'''\n",
        "10 distinct images of fashion_mnist class is written and then wandb is initialized and the images are plotted\n",
        "'''\n",
        "ImageClasses = [\"Pullover\",\"Shirt\",\"Coat\",\"Trouser\",\"Dress\",\"Sandal\",\"Bag\",\"Sneaker\",\"Ankle boot\",\"T-shirt/top\"]\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "'''normalizing the data'''\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "x_valT = np.transpose(x_val.reshape(x_val.shape[0] , x_val.shape[1] * x_val.shape[2]))\n",
        "x_trainT = np.transpose(x_train.reshape(x_train.shape[0] , x_train.shape[1] * x_train.shape[2]))\n",
        "x_testT = np.transpose(x_test.reshape(x_test.shape[0] , x_test.shape[1] * x_test.shape[2]))\n",
        "y_trainT = y_train.reshape(1 , y_train.shape[0])\n",
        "y_valT = y_val.reshape(1 , y_val.shape[0])\n",
        "y_testT = y_test.reshape(1 , y_test.shape[0])\n",
        "\n",
        "class ActivationFunction:\n",
        "  '''all activation functions are defined here'''\n",
        "  def sigmoid(x):\n",
        "    return  1 / (1 + np.exp(-x))\n",
        "  def softmax(x):\n",
        "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "  def Relu(x):\n",
        "    return np.maximum(x,0)\n",
        "  def tanh(x):\n",
        "    return np.tanh(x)\n",
        "  def softmax_derivative(x):\n",
        "    return ActivationFunction.softmax(x) * (1-ActivationFunction.softmax(x))\n",
        "  def sigmoid_derivative(Z):\n",
        "    s = 1 /(1 + np.exp(-Z))\n",
        "    dA = s * (1 - s)\n",
        "    return dA\n",
        "  def Relu_derivative(x):\n",
        "    return 1*(x > 0)\n",
        "  def tanh_derivative(x):\n",
        "    return (1 - (np.tanh(x)**2))\n",
        "\n",
        "class NeuralNetwork:\n",
        "  '''\n",
        "  mode_of_initialization =str , The way the weights are initialized\n",
        "  n_layers = int ,total number of layers\n",
        "  activation_function = str , activation function of the neural network,default is sigmoid activation function\n",
        "  n_input = int ,number of inputs\n",
        "  n_outputs = int , number of outputs\n",
        "  num_neurns_in_hidden_layer = int ,how many neurons are in each hidden layer\n",
        "  n_neurons = list having number of neurons in each layer\n",
        "  TrainInput = input layer's input\n",
        "  TrainOutput = output layer's output\n",
        "  ValInput = Validation Input\n",
        "  ValOutput = validation output\n",
        "  parameters = dict ,stores the parameters of the model\n",
        "  cache = dict , stores the H and A for each layer which help in gradient computation\n",
        "  grads = dict ,store the gradients\n",
        "    '''\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  num_neurons_in_hidden_layers = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  parameters = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "  def __init__(self,mode_of_initialization = 'random',number_of_hidden_layers = 1,num_neurons_in_hidden_layers = 4,activation = 'sigmoid',TrainInput = x_trainT,TrainOutput = y_trainT,ValInput = x_valT,ValOutput = y_valT):\n",
        "\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    self.num_neurons_in_hidden_layers = num_neurons_in_hidden_layers\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.parameters[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.parameters[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.parameters[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def output(self,A):\n",
        "    '''\n",
        "    uses softmax function in the last layer for calculating the prediction\n",
        "    '''\n",
        "    return ActivationFunction.softmax(A)\n",
        "\n",
        "\n",
        "  def forward(self,X,activation,parameters):\n",
        "    '''\n",
        "    calculates H and A for each layer and stores them into cache dictionary. It uses the parameter dictionary .\n",
        "    '''\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1 , self.n_layers):\n",
        "      H = self.cache[\"H\" + str(l - 1)]\n",
        "      # print(H.shape)\n",
        "      W = self.parameters[\"W\" + str(l)]\n",
        "      b = self.parameters[\"b\" + str(l)]\n",
        "      A = np.dot(W , H) + b\n",
        "      # print(W.shape)\n",
        "      # print(b.shape)\n",
        "      self.cache[\"A\" + str(l)] = A\n",
        "      if activation == 'sigmoid':\n",
        "         H = ActivationFunction.sigmoid(A)\n",
        "      elif activation == 'relu':\n",
        "         H = ActivationFunction.Relu(A)\n",
        "      elif activation == 'tanh':\n",
        "         H = ActivationFunction.tanh(A)\n",
        "      self.cache[\"H\" + str(l)] = H\n",
        "    yPredicted = self.output(self.cache[\"A\" + str(self.n_layers - 1)])\n",
        "    return yPredicted\n",
        "\n",
        "  def predict(self,input):\n",
        "    '''\n",
        "    predicts the class of the image , although result is not int , it is probability\n",
        "    '''\n",
        "    H = input\n",
        "    activation = self.activation_function\n",
        "    for l in range(1 , self.n_layers - 1):\n",
        "      # print(H.shape)\n",
        "      W = self.parameters[\"W\" + str(l)]\n",
        "      b = self.parameters[\"b\" + str(l)]\n",
        "      A = np.dot(W , H) + b\n",
        "      # print(W.shape)\n",
        "      # print(b.shape)\n",
        "      if activation == 'sigmoid':\n",
        "         H = ActivationFunction.sigmoid(A)\n",
        "      elif activation == 'relu':\n",
        "         H = ActivationFunction.Relu(A)\n",
        "      elif activation == 'tanh':\n",
        "         H = ActivationFunction.tanh(A)\n",
        "\n",
        "\n",
        "    W = self.parameters[\"W\" + str(self.n_layers - 1)]\n",
        "    b = self.parameters[\"b\" + str(self.n_layers - 1)]\n",
        "    A = np.dot(W , H) + b\n",
        "    y_predicted = self.output(A)\n",
        "    return y_predicted\n",
        "\n",
        "NN = NeuralNetwork()\n",
        "\n",
        "print(NN.predict(x_trainT))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gzb7eJ0k3X4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "(train_input, train_output), (test_input, test_output) = fashion_mnist.load_data()\n",
        "train_input = train_input / 255.0\n",
        "test_input = test_input / 255.0\n",
        "\n",
        "# Assuming 'data' is your dataset and 'labels' are corresponding labels/targets\n",
        "train_input, test_input, train_output, test_output = train_test_split(\n",
        "    train_input, train_output, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_inputT = np.transpose(train_input.reshape(train_input.shape[0], -1))\n",
        "\n",
        "class Activation:\n",
        "\n",
        "    def sigmoid(x):\n",
        "        x = np.clip(x, -500, 500)\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def softmax(x):\n",
        "        exp_values = np.exp(x - np.max(x))\n",
        "        return exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
        "\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "\n",
        "class Neural_Network:\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers=1,\n",
        "        hidden_layer_nodes=4,\n",
        "        activation_function=\"sigmoid\",\n",
        "        x_train=train_inputT,\n",
        "        y_train=train_output,\n",
        "        x_test=test_input,\n",
        "        y_test=test_output,\n",
        "    ):\n",
        "        self.layers = layers\n",
        "        self.hidden_layer_nodes = hidden_layer_nodes\n",
        "        self.activation_function = activation_function\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        self.decode = {\"h0\": x_train}\n",
        "        self.decode[\"A0\"] = x_train\n",
        "        self.n_input = x_train.shape[0]\n",
        "        self.n_output = 10  # Assuming you have 10 classes in Fashion MNIST\n",
        "        self.nodes_in_level = [self.n_input]  # Initialize with input layer nodes\n",
        "        self.thetas = {}\n",
        "        for l in range(1, self.layers + 2):\n",
        "            self.nodes_in_level.append(hidden_layer_nodes) if l < self.layers + 1 else self.nodes_in_level.append(\n",
        "                self.n_output\n",
        "            )\n",
        "            self.thetas[\"W\" + str(l)] = np.random.randn(\n",
        "                self.nodes_in_level[l], self.nodes_in_level[l - 1]\n",
        "            )\n",
        "            self.thetas[\"b\" + str(l)] = np.zeros((self.nodes_in_level[l], 1))\n",
        "\n",
        "    def forward_propagation(self):\n",
        "        for k in range(1, self.layers + 2):\n",
        "            W = self.thetas[\"W\" + str(k)]\n",
        "            b = self.thetas[\"b\" + str(k)]\n",
        "            h = self.decode[\"h\" + str(k - 1)]\n",
        "            A = b + np.dot(W, h)\n",
        "            if self.activation_function == \"sigmoid\":\n",
        "                h = Activation.sigmoid(A)\n",
        "            elif self.activation_function == \"tanh\":\n",
        "                h = Activation.tanh(A)\n",
        "            self.decode[\"h\" + str(k)] = h\n",
        "            self.decode[\"A\" + str(k)] = A\n",
        "\n",
        "        y_hat = Activation.softmax(self.decode[\"A\" + str(self.layers + 1)])\n",
        "        return y_hat\n",
        "\n",
        "\n",
        "neural_network = Neural_Network()\n",
        "result = neural_network.forward_propagation()\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhYfErKwx7OU",
        "outputId": "34cf77d4-1ef7-4092-a4ba-35c5a03b7451"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.10013348 0.00676222 0.02234989 ... 0.0397687  0.02069737 0.00749427]\n",
            " [0.10013738 0.15123605 0.39413149 ... 0.03664666 0.19659424 0.30401039]\n",
            " [0.09979417 0.0482745  0.00955078 ... 0.05159459 0.00698927 0.05777833]\n",
            " ...\n",
            " [0.09991326 0.14251926 0.0278301  ... 0.28297623 0.12459664 0.03887115]\n",
            " [0.10009272 0.04417972 0.182001   ... 0.04951272 0.13784489 0.06064374]\n",
            " [0.10002165 0.07634184 0.08359702 ... 0.05792878 0.07231635 0.08949334]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#question 3\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "(train_input, train_output), (test_input, test_output) = fashion_mnist.load_data()\n",
        "\n",
        "train_input = train_input / 255.0\n",
        "test_input = test_input / 255.0\n",
        "\n",
        "# Assuming 'data' is your dataset and 'labels' are corresponding labels/targets\n",
        "train_input, test_input, train_output, test_output = train_test_split(\n",
        "    train_input, train_output, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "train_inputT = np.transpose(train_input.reshape(train_input.shape[0], -1))\n",
        "\n",
        "class Activation:\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        # Use np.clip to prevent overflow issues\n",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        exp_values = np.exp(x - np.max(x))\n",
        "        return exp_values / np.sum(exp_values, axis=0, keepdims=True)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "\n",
        "\n",
        "class Neural_Network:\n",
        "    def __init__(\n",
        "        self,\n",
        "        layers=10,\n",
        "        hidden_layer_nodes=8,\n",
        "        activation_function=\"sigmoid\",\n",
        "        x_train=train_inputT,\n",
        "        y_train=train_output,\n",
        "        x_test=test_input,\n",
        "        y_test=test_output,\n",
        "    ):\n",
        "        self.layers = layers\n",
        "        self.hidden_layer_nodes = hidden_layer_nodes\n",
        "        self.activation_function = activation_function\n",
        "        self.x_train = x_train\n",
        "        self.y_train = y_train\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        self.decode = {\"h0\": x_train}\n",
        "        self.decode[\"A0\"] = x_train\n",
        "        self.n_input = x_train.shape[0]\n",
        "        self.n_output = 10  # Assuming you have 10 classes in Fashion MNIST\n",
        "        self.nodes_in_level = [self.n_input]  # Initialize with input layer nodes\n",
        "        self.thetas = {}\n",
        "        self.derivatives={}\n",
        "        self.previous_updates={}\n",
        "        for l in range(1,self.layers+2):\n",
        "            self.previous_updates[\"W\"+str(l)]=np.zeros(self.nodes_in_level[l], self.nodes_in_level[l - 1])\n",
        "            self.previous_updates[\"b\"+str(l)]=np.zeros(self.nodes_in_level[l],1)\n",
        "        self.y_hat=[]\n",
        "        for l in range(1, self.layers + 2):\n",
        "            self.nodes_in_level.append(hidden_layer_nodes) if l < self.layers + 1 else self.nodes_in_level.append(\n",
        "                self.n_output\n",
        "            )\n",
        "            self.thetas[\"W\" + str(l)] = np.random.randn(\n",
        "                self.nodes_in_level[l], self.nodes_in_level[l - 1]\n",
        "            )\n",
        "            self.thetas[\"b\" + str(l)] = np.zeros((self.nodes_in_level[l], 1))\n",
        "            # printf(self.thetas[\"W1\"])\n",
        "\n",
        "    def forward_propagation(self):\n",
        "        for k in range(1, self.layers +  2):\n",
        "            W = self.thetas[\"W\" + str(k)]\n",
        "            b = self.thetas[\"b\" + str(k)]\n",
        "            h = self.decode[\"h\" + str(k - 1)]\n",
        "            A = b + np.dot(W, h)\n",
        "            # print(\"dim of A = \",A.shape)\n",
        "            if self.activation_function == \"sigmoid\":\n",
        "                h = Activation.sigmoid(A)\n",
        "            elif self.activation_function == \"tanh\":\n",
        "                h = Activation.tanh(A)\n",
        "            # print(\"printing h in\",k)\n",
        "            # print(h)\n",
        "            self.decode[\"h\" + str(k)] = h\n",
        "            self.decode[\"A\" + str(k)] = A\n",
        "\n",
        "        self.y_hat = Activation.softmax(self.decode[\"A\" + str(self.layers + 1)])\n",
        "        return self.y_hat\n",
        "\n",
        "\n",
        "    def backward_propagation(self):\n",
        "        e_l = np.transpose(np.eye(10)[self.y_train[:]])\n",
        "        # print(\"dim of e_l\",e_l.shape)\n",
        "        # print(\"dim of y_hat\",self.y_hat.shape)\n",
        "        dA = (self.y_hat - e_l)\n",
        "        # print(\"dim of dA\", dA.shape)\n",
        "        self.derivatives[\"dA\" + str(self.layers + 1)] = dA\n",
        "        for k in range(self.layers +1, 0, -1):\n",
        "            dA = self.derivatives[\"dA\" + str(k)]\n",
        "            h_prev = self.decode[\"h\" + str(k - 1)]\n",
        "            A_prev = self.decode[\"A\" + str(k - 1)]\n",
        "            W = self.thetas[\"W\" + str(k)]\n",
        "            dW = np.zeros(W.shape)\n",
        "            db = np.zeros((W.shape[0],1))\n",
        "            dh_prev = np.zeros(h_prev.shape)\n",
        "            dA_prev = np.zeros(A_prev.shape)\n",
        "            dW = np.dot(dA, h_prev.T)\n",
        "            self.derivatives[\"dW\" + str(k)] = dW\n",
        "            db = np.sum(dA, axis=1, keepdims=True)\n",
        "            self.derivatives[\"db\" + str(k)] = db\n",
        "            if k >= 2:\n",
        "                dh_prev = np.matmul(W.T,dA)\n",
        "                if self.activation_function == \"sigmoid\":\n",
        "                    dA_prev = np.multiply(dh_prev, (Activation.sigmoid(A_prev) * (1 - Activation.sigmoid(A_prev))))\n",
        "            self.derivatives[\"dA\" + str(k - 1)] = dA_prev\n",
        "\n",
        "    def do_mgd(self,eta,beta):\n",
        "        for l in range(1,self.layers+2):\n",
        "            self.previous_updates[\"W\"+str(l)]=beta*self.previous_updates[\"W\"+str(l)]+self.derivatives[\"dW\"+str(l)]\n",
        "            self.previous_updates[\"b\"+str(l)]=beta*self.previous_updates[\"b\"+str(l)]+self.derivatives[\"db\"+str(l)]\n",
        "            self.thetas[\"W\"+str(l)]=self.thetas[\"W\"+str(l)]-eta*self.previous_updates[\"W\"+str(l)]\n",
        "            self.thetas[\"b\"+str(l)]=self.thetas[\"b\"+str(l)]-eta*self.previous_updates[\"b\"+str(l)]\n",
        "\n",
        "\n",
        "\n",
        "    def compute(self):\n",
        "        epoch = 100\n",
        "        eta = 0.01\n",
        "        for iter in range(epoch):\n",
        "            self.forward_propagation()\n",
        "            self.backward_propagation()\n",
        "            # Update weights and biases\n",
        "            for l in range(1,self.layers):\n",
        "                W = self.thetas[\"W\" + str(l)]\n",
        "                dW = self.derivatives[\"dW\" + str(l)]\n",
        "                W = W - eta * dW\n",
        "                # print(eta*dW)\n",
        "                self.thetas[\"W\" + str(l)] = W\n",
        "                b = self.thetas[\"b\" + str(l)]\n",
        "                db = self.derivatives[\"db\" + str(l)]\n",
        "                b = b - eta * db\n",
        "                self.thetas[\"b\" + str(l)] = b\n",
        "            if iter >=0 :\n",
        "                OneHotOfTrueOutput = np.transpose(np.eye(10)[self.y_train[:]])\n",
        "                sum = -np.sum(OneHotOfTrueOutput * np.log(self.y_hat), axis=0)\n",
        "                print(sum)\n",
        "\n",
        "\n",
        "\n",
        "neural_network = Neural_Network()\n",
        "neural_network.compute()\n",
        "\n"
      ],
      "metadata": {
        "id": "so5OWDZD3Yjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#question 1\n",
        "# !pip install wandb\n",
        "# import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# wandb.login()\n",
        "# wandb.init(\n",
        "#       # Set the project where this run will be logged\n",
        "#       project=\"deep-learning\",\n",
        "#       # We pass a run name (otherwise it’ll be randomly assigned, like sunshine-lollypop-10)\n",
        "#       # Track hyperparameters and run metadata\n",
        "#       config={\n",
        "#       \"epochs\": 1\n",
        "#       })\n",
        "\n",
        "(train_input, train_output), (test_input,test_output) = fashion_mnist.load_data()\n",
        "\n",
        "class_names = {\n",
        "    0: \"T-shirt/top\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle boot\"\n",
        "}\n",
        "\n",
        "# Plot one sample image for each class\n",
        "class_counts = np.bincount(train_output)\n",
        "total=len(train_output)\n",
        "for i in range(len(class_names)):\n",
        "    # Find the first image with the corresponding class label\n",
        "    idx=np.where(train_output==i)[0][0]\n",
        "    # plotting the image\n",
        "    plt.subplot(2,5,i+1)\n",
        "    plt.imshow(train_input[idx],cmap='gray')\n",
        "    plt.title(class_names[i])\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "# wandb.log({\"Sample Images\": plt})"
      ],
      "metadata": {
        "id": "1MCZ5quDV-9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#question 2\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the pixel values\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "# Split the training data into training and validation sets\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "\n",
        "# Reshape the data for the neural network\n",
        "x_val_T = np.transpose(x_val.reshape(x_val.shape[0], -1))\n",
        "x_train_T = np.transpose(x_train.reshape(x_train.shape[0], -1))\n",
        "x_test_T = np.transpose(x_test.reshape(x_test.shape[0], -1))\n",
        "y_train_T = y_train.reshape(1, -1)\n",
        "y_val_T = y_val.reshape(1, -1)\n",
        "y_test_T = y_test.reshape(1, -1)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, init_mode='random', num_hidden_layers=6, neurons_per_hidden_layer=32,\n",
        "                 activation_function='sigmoid', train_input=x_train_T, train_output=y_train_T, val_input=x_val_T,\n",
        "                 val_output=y_val_T):\n",
        "        self.init_mode = init_mode\n",
        "        self.num_hidden_layers = num_hidden_layers\n",
        "        self.neurons_per_hidden_layer = neurons_per_hidden_layer\n",
        "        self.activation_function = activation_function\n",
        "        self.train_input = train_input\n",
        "        self.train_output = train_output\n",
        "        self.val_input = val_input\n",
        "        self.val_output = val_output\n",
        "        self.n_layers = num_hidden_layers + 2\n",
        "        self.n_input = train_input.shape[0]\n",
        "        self.n_output = np.max(train_output) + 1\n",
        "        self.n_neurons = [self.n_input] + [neurons_per_hidden_layer] * num_hidden_layers + [self.n_output]\n",
        "        self.cache = {\"H0\": train_input, \"A0\": train_input}\n",
        "        self.theta = {}\n",
        "        self.grads = {}\n",
        "        self.initialize_parameters()\n",
        "\n",
        "    def initialize_parameters(self):\n",
        "        for l in range(1, self.n_layers):\n",
        "            if self.init_mode == \"random\":\n",
        "                self.theta[f\"W{l}\"] = np.random.randn(self.n_neurons[l], self.n_neurons[l - 1])\n",
        "            self.theta[f\"b{l}\"] = np.zeros((self.n_neurons[l], 1))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def relu(self, x):\n",
        "        return np.maximum(x, 0)\n",
        "\n",
        "    def tanh(self, x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    def feedforward(self, input_data):\n",
        "        H = input_data\n",
        "        activation = self.activation_function\n",
        "        for l in range(1, self.n_layers - 1):\n",
        "            W = self.theta[f\"W{l}\"]\n",
        "            b = self.theta[f\"b{l}\"]\n",
        "            A = np.dot(W, H) + b\n",
        "            H = getattr(self, activation)(A)\n",
        "        W = self.theta[f\"W{self.n_layers - 1}\"]\n",
        "        b = self.theta[f\"b{self.n_layers - 1}\"]\n",
        "        A = np.dot(W, H) + b\n",
        "        y_predicted = np.exp(A) / np.sum(np.exp(A), axis=0)\n",
        "        return y_predicted\n",
        "\n",
        "# Create an instance of the NeuralNetwork class\n",
        "my_model = NeuralNetwork()\n",
        "\n",
        "# Perform a feedforward pass on the training data\n",
        "my_model.feedforward(x_train_T)"
      ],
      "metadata": {
        "id": "J5lGVSyLWPGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip install wandb\n",
        "\n",
        "# import wandb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# wandb.login()\n",
        "\n",
        "# wandb.init(\n",
        "#     # set the wandb project where this run will be logged\n",
        "#     project=\"deep-learning\",\n",
        "\n",
        "#     # track hyperparameters and run metadata\n",
        "#     config={\n",
        "#     \"eta\": 0.02\n",
        "#     }\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.datasets import m\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "'''normalizing the data'''\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "x_train_T = x_train.reshape(-1, x_train.shape[1]*x_train.shape[2]).T\n",
        "x_val_T = x_val.reshape(-1, x_val.shape[1]*x_val.shape[2]).T\n",
        "x_test_T = x_test.reshape(-1, x_test.shape[1]*x_test.shape[2]).T\n",
        "y_train_T, y_val_T, y_test_T = y_train.reshape(1, -1), y_val.reshape(1, -1), y_test.reshape(1, -1)\n",
        "\n",
        "\n",
        "\n",
        "class Compute:\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return  1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu(x):\n",
        "        return np.maximum(x,0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return np.tanh(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(x):\n",
        "        return Compute.softmax(x) * (1-Compute.softmax(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(Z):\n",
        "        s = Compute.sigmoid(Z)\n",
        "        dA = s * (1 - s)\n",
        "        return dA\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu_derivative(x):\n",
        "        return 1*(x > 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(x):\n",
        "        return (1 - (np.tanh(x)**2))\n",
        "\n",
        "\n",
        "class Update:\n",
        "    @staticmethod\n",
        "    def simple_gradient_descent(eta):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            W -= eta * dW\n",
        "            b -= eta * db\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss):\n",
        "        theta = {}\n",
        "        input_data = my_network.TrainInput[:, i:i + batch_size]\n",
        "        output_data = my_network.TrainOutput[0, i:i + batch_size]\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            theta[\"W\" + str(l)] = my_network.theta[\"W\" + str(l)] - beta * previous_updates[\"W\" + str(l)]\n",
        "            theta[\"b\" + str(l)] = my_network.theta[\"b\" + str(l)] - beta * previous_updates[\"b\" + str(l)]\n",
        "        y_predicted = my_network.forward(input_data, my_network.activation_function, my_network.theta)\n",
        "        e_y = np.transpose(np.eye(my_network.n_output)[output_data])\n",
        "        my_network.backpropagation(y_predicted, e_y, batch_size, loss, my_network.activation_function, my_network.theta)\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + my_network.grads[\"dW\" + str(l)]\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + my_network.grads[\"db\" + str(l)]\n",
        "            my_network.theta[\"W\" + str(l)] -= eta * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= eta * my_network.grads[\"db\" + str(l)]\n",
        "        return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def momentum_gradient_descent(eta, beta, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            uW, ub = previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)]\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            uW = beta * uW + dW\n",
        "            ub = beta * ub + db\n",
        "            W -= eta * uW\n",
        "            b -= eta * ub\n",
        "            previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)] = uW, ub\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def rms_prop(eta, beta, epsilon, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"dW\" + str(l)])\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"db\" + str(l)])\n",
        "            factorW = eta / (np.sqrt(previous_updates[\"W\" + str(l)] + epsilon))\n",
        "            factorb = eta / (np.sqrt(previous_updates[\"b\" + str(l)] + epsilon))\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * my_network.grads[\"db\" + str(l)]\n",
        "\n",
        "    @staticmethod\n",
        "    def nadam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            term1 = 1 - (beta1 ** (t))\n",
        "            term2 = (1 - beta1) * my_network.grads[\"dW\" + str(l)] / term1\n",
        "            term3 = (1 - beta1) * my_network.grads[\"db\" + str(l)] / term1\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * (beta1 * MW_corrected + term2)\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * (beta1 * Mb_corrected + term3)\n",
        "        return M, V, t + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def adam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** t))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** t))\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** t))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** t))\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * MW_corrected\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * Mb_corrected\n",
        "        return M, V, t + 1\n",
        "\n",
        "\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  theta = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "\n",
        "\n",
        "  def __init__(self,mode_of_initialization=\"random\",number_of_hidden_layers=6,num_neurons_in_hidden_layers=32,activation=\"sigmoid\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T):\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.theta[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.theta[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.theta[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def forward(self, X, activation, theta):\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1, self.n_layers):\n",
        "        H = self.cache[\"H\" + str(l - 1)]\n",
        "        W = self.theta[\"W\" + str(l)]\n",
        "        b = self.theta[\"b\" + str(l)]\n",
        "        A = np.dot(W, H) + b\n",
        "        self.cache[\"A\" + str(l)] = A\n",
        "        H = self.apply_activation(A, activation)\n",
        "        self.cache[\"H\" + str(l)] = H\n",
        "    Al = self.cache[\"A\" + str(self.n_layers - 1)]\n",
        "    y_hat= Compute.softmax(Al)\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "  def apply_activation(self, A, activation):\n",
        "        if activation == 'sigmoid':\n",
        "            return Compute.sigmoid(A)\n",
        "        elif activation == 'relu':\n",
        "            return Compute.Relu(A)\n",
        "        elif activation == 'tanh':\n",
        "            return Compute.tanh(A)\n",
        "\n",
        "  def loss(self, input, true_output, predicted_output, loss, batch_size):\n",
        "    if loss == 'cross_entropy':\n",
        "        one_hot_true_output = np.eye(self.n_output)[true_output[0]].T\n",
        "        loss_value = -np.sum(one_hot_true_output * np.log(predicted_output + 1e-9)) / batch_size\n",
        "    return loss_value\n",
        "\n",
        "  def accuracy(self, input, true_output, predicted_output):\n",
        "    predicted_labels = np.argmax(predicted_output, axis=0)\n",
        "    correct_predictions = np.sum(true_output == predicted_labels)\n",
        "    total_samples = true_output.shape[1]\n",
        "    accuracy_percentage = (correct_predictions / total_samples) * 100\n",
        "    return accuracy_percentage\n",
        "\n",
        "  def backpropagation(self, y_predicted, e_y, batch_size, loss, activation, theta):\n",
        "        if loss == 'cross_entropy':\n",
        "            dA = y_predicted - e_y\n",
        "        m = dA.shape[1]\n",
        "        self.grads[\"dA\" + str(self.n_layers - 1)] = dA\n",
        "\n",
        "        for k in range(self.n_layers - 1, 0, -1):\n",
        "            dA = self.grads[\"dA\" + str(k)]\n",
        "            H_prev = self.cache[\"H\" + str(k - 1)]\n",
        "            A_prev = self.cache[\"A\" + str(k - 1)]\n",
        "            W = self.theta[\"W\" + str(k)]\n",
        "\n",
        "            dW, db, dH_prev, dA_prev = self.calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size)\n",
        "\n",
        "            self.grads[\"dA\" + str(k - 1)] = dA_prev\n",
        "            self.grads[\"dW\" + str(k)] = dW\n",
        "            self.grads[\"db\" + str(k)] = db\n",
        "\n",
        "        return\n",
        "\n",
        "  def calculate_gradients(self, k, dA, H_prev, A_prev, W, activation, batch_size):\n",
        "        dW = self.calculate_dW(dA, H_prev, batch_size)\n",
        "        db = self.calculate_db(dA, batch_size)\n",
        "        dH_prev, dA_prev = self.calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev) if k > 1 else (np.zeros(H_prev.shape), np.zeros(A_prev.shape))\n",
        "\n",
        "        return dW, db, dH_prev, dA_prev\n",
        "\n",
        "  def calculate_dW(self, dA, H_prev, batch_size):\n",
        "        return np.dot(dA, H_prev.T) / batch_size\n",
        "\n",
        "  def calculate_db(self, dA, batch_size):\n",
        "        return np.sum(dA, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "  def calculate_dH_prev_dA_prev(self, k, W, dA, activation, A_prev):\n",
        "        dH_prev = np.matmul(W.T, dA)\n",
        "        dA_prev = self.calculate_dA_prev(dH_prev, activation, A_prev)\n",
        "        return dH_prev, dA_prev\n",
        "\n",
        "  def calculate_dA_prev(self, dH_prev, activation, A_prev):\n",
        "        if activation == 'sigmoid':\n",
        "            return dH_prev * Compute.sigmoid_derivative(A_prev)\n",
        "        elif activation == 'tanh':\n",
        "            return dH_prev * Compute.tanh_derivative(A_prev)\n",
        "        elif activation == 'relu':\n",
        "            return dH_prev * Compute.Relu_derivative(A_prev)\n",
        "\n",
        "\n",
        "  def compute(self, eta = 0.001,beta = 0.9,beta1 = 0.9,beta2 = 0.999 ,epsilon = 1e-6, optimizer = 'sgd',batch_size = 100,loss = 'cross_entropy',epochs = 20):\n",
        "    train_c_epoch, tarin_acc_per_epoch, val_c_per_epoch, val_acc_per_epoch, previous_updates, M, V = [], [], [], [], {}, {}, {}\n",
        "    for l in range(1 , self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      previous_updates[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    for l in range(1 , self.n_layers):\n",
        "      M[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      M[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "      V[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      V[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    t = 1\n",
        "    for count in range(epochs):\n",
        "      for i in range(0 , self.TrainInput.shape[1],batch_size):\n",
        "        if i + batch_size > self.TrainInput.shape[1]:\n",
        "          continue\n",
        "        theta = self.theta\n",
        "        yPredicted = self.forward(self.TrainInput[:,i:i + batch_size],self.activation_function,theta)\n",
        "        e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])\n",
        "        self.backpropagation(yPredicted,e_y,batch_size,loss,self.activation_function,theta)\n",
        "        if optimizer == 'sgd':   #referred slide page 54\n",
        "            Update.simple_gradient_descent(eta)\n",
        "        elif optimizer == 'nag':\n",
        "            previous_updates=Update.nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss)\n",
        "\n",
        "        elif optimizer == 'momentum': #referred from slide 43\n",
        "          Update.momentum_gradient_descent(eta,beta,previous_updates)\n",
        "\n",
        "        elif optimizer == 'RMSprop':\n",
        "          Update.rms_prop(eta,beta,epsilon,previous_updates)\n",
        "        elif optimizer == 'adam':\n",
        "          epsilon = 1e-8\n",
        "          M , V , t= Update.adam(eta,beta1,beta2,epsilon,M , V , t)\n",
        "        elif optimizer == 'nadam':\n",
        "          epsilon = 1e-8\n",
        "          M , V , t= Update.nadam(eta,beta1,beta2,epsilon,M , V , t)\n",
        "\n",
        "      y_hat = self.forward(self.TrainInput,self.activation_function,self.theta)\n",
        "      valy_hat = self.forward(self.ValInput,self.activation_function,self.theta)\n",
        "      train_cost = self.loss(self.TrainInput,self.TrainOutput,y_hat,loss,self.TrainInput.shape[1])\n",
        "      train_c_epoch.append(train_cost)\n",
        "      val_cost = self.loss(self.ValInput,self.ValOutput,valy_hat,loss,self.ValInput.shape[1])\n",
        "      val_c_per_epoch.append(val_cost)\n",
        "      train_acc = self.accuracy(self.TrainInput, self.TrainOutput,y_hat)\n",
        "      tarin_acc_per_epoch.append(train_acc)\n",
        "\n",
        "      val_acc = self.accuracy(self.ValInput, self.ValOutput,valy_hat)\n",
        "      val_acc_per_epoch.append(val_acc)\n",
        "\n",
        "    #   if count==epochs-1:\n",
        "        # conf_data=confusion_matrix(self.ValOutput[0], valy_hat[0])\n",
        "        # seaborn.heatmap(conf_data, annot=True, cmap='Blues', fmt='g')\n",
        "        # plt.xlabel('Predicted labels')\n",
        "        # plt.ylabel('True labels')\n",
        "        # plt.title('Confusion Matrix')\n",
        "        # plt.show()\n",
        "\n",
        "    #   print(\"---------\"*20)\n",
        "    #   print(f\"Epoch Number = {format(count+1)}\")\n",
        "    #   print(f\"Training Accuracy = {format(tarin_acc_per_epoch[-1])}\")\n",
        "    #   print(f\"Validation Accuracy = {format(val_acc_per_epoch[-1])}\")\n",
        "      # wandb.log({\"training_accuracy\": train_acc,\"validation_accuracy\": val_acc,\"training_loss\":train_cost,\"validation_loss\": val_cost,\"epoch\": count})\n",
        "    return train_c_epoch,tarin_acc_per_epoch,val_c_per_epoch,val_acc_per_epoch\n",
        "\n",
        "\n",
        "my_network = MyNeuralNetwork()\n",
        "my_network.compute(eta = 0.01, beta = 0.9 , beta1 = 0.9 , beta2 = 0.999 , epsilon = 0.001, optimizer = 'sgd' , batch_size = 16 , loss = 'cross_entropy' , epochs = 1)\n"
      ],
      "metadata": {
        "id": "K33zpd9y716C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip install wandb\n",
        "\n",
        "# import wandb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score,ConfusionMatrixDisplay\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# wandb.login()\n",
        "\n",
        "# wandb.init(\n",
        "#     # set the wandb project where this run will be logged\n",
        "#     project=\"deep-learning\",\n",
        "\n",
        "#     # track hyperparameters and run metadata\n",
        "#     config={\n",
        "#     \"eta\": 0.02\n",
        "#     }\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "'''normalizing the data'''\n",
        "x_test = x_test / 255.0\n",
        "x_train = x_train / 255.0\n",
        "\n",
        "\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "x_train_T = x_train.reshape(-1, x_train.shape[1]*x_train.shape[2]).T\n",
        "x_val_T = x_val.reshape(-1, x_val.shape[1]*x_val.shape[2]).T\n",
        "x_test_T = x_test.reshape(-1, x_test.shape[1]*x_test.shape[2]).T\n",
        "y_train_T, y_val_T, y_test_T = y_train.reshape(1, -1), y_val.reshape(1, -1), y_test.reshape(1, -1)\n",
        "\n",
        "class Util:\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_activation(A, activation):\n",
        "            if activation == 'sigmoid':\n",
        "                return Compute.sigmoid(A)\n",
        "            elif activation == 'relu':\n",
        "                return Compute.Relu(A)\n",
        "            elif activation == 'tanh':\n",
        "                return Compute.tanh(A)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(input, true_output, predicted_output, loss, batch_size):\n",
        "        if loss == 'cross_entropy':\n",
        "            one_hot_true_output = np.eye(my_network.n_output)[true_output[0]].T\n",
        "            return -np.sum(one_hot_true_output * np.log(predicted_output + 1e-9)) / batch_size\n",
        "\n",
        "\n",
        "        if loss=='squared_loss':\n",
        "            one_hot_true_output = np.eye(my_network.n_output)[true_output[0]].T\n",
        "            loss_factor=np.square(predicted_output-one_hot_true_output)\n",
        "            return np.sum(loss_factor)/batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(input, true_output, predicted_output):\n",
        "        predicted_labels = np.argmax(predicted_output, axis=0)\n",
        "        correct_predictions = np.sum(true_output == predicted_labels)\n",
        "        total_samples = true_output.shape[1]\n",
        "        accuracy_percentage = (correct_predictions / total_samples) * 100\n",
        "        return accuracy_percentage\n",
        "\n",
        "class Compute:\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return  1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu(x):\n",
        "        return np.maximum(x,0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return (2 * Compute.sigmoid(2 * x)) - 1\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(x):\n",
        "        return Compute.softmax(x) * (1-Compute.softmax(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(Z):\n",
        "        s = Compute.sigmoid(Z)\n",
        "        dA = s * (1 - s)\n",
        "        return dA\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu_derivative(x):\n",
        "        return 1*(x > 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(x):\n",
        "        return (1 - (Compute.tanh(x)**2))\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size):\n",
        "            dW = Compute.calculate_dW(dA, H_prev, batch_size)\n",
        "            db = Compute.calculate_db(dA, batch_size)\n",
        "            dH_prev, dA_prev = Compute.calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev) if k > 1 else (np.zeros(H_prev.shape), np.zeros(A_prev.shape))\n",
        "\n",
        "            return dW, db, dH_prev, dA_prev\n",
        "    @staticmethod\n",
        "    def calculate_dW(dA, H_prev, batch_size):\n",
        "            return np.dot(dA, H_prev.T) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_db(dA, batch_size):\n",
        "            return np.sum(dA, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev):\n",
        "            dH_prev = np.matmul(W.T, dA)\n",
        "            dA_prev = Compute.calculate_dA_prev(dH_prev, activation, A_prev)\n",
        "            return dH_prev, dA_prev\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dA_prev(dH_prev, activation, A_prev):\n",
        "            if activation == 'sigmoid':\n",
        "                return dH_prev * Compute.sigmoid_derivative(A_prev)\n",
        "            elif activation == 'tanh':\n",
        "                return dH_prev * Compute.tanh_derivative(A_prev)\n",
        "            elif activation == 'relu':\n",
        "                return dH_prev * Compute.Relu_derivative(A_prev)\n",
        "\n",
        "\n",
        "class Update:\n",
        "    @staticmethod\n",
        "    def simple_gradient_descent(eta):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            W -= eta * dW\n",
        "            b -= eta * db\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss):\n",
        "        theta = {}\n",
        "        input_data = my_network.TrainInput[:, i:i + batch_size]\n",
        "        output_data = my_network.TrainOutput[0, i:i + batch_size]\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            theta[\"W\" + str(l)] = my_network.theta[\"W\" + str(l)] - beta * previous_updates[\"W\" + str(l)]\n",
        "            theta[\"b\" + str(l)] = my_network.theta[\"b\" + str(l)] - beta * previous_updates[\"b\" + str(l)]\n",
        "        y_predicted = my_network.forward(input_data, my_network.activation_function, my_network.theta)\n",
        "        e_y = np.transpose(np.eye(my_network.n_output)[output_data])\n",
        "        my_network.backpropagation(y_predicted, e_y, batch_size, loss, my_network.activation_function, my_network.theta)\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + my_network.grads[\"dW\" + str(l)]\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + my_network.grads[\"db\" + str(l)]\n",
        "            my_network.theta[\"W\" + str(l)] -= eta * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= eta * my_network.grads[\"db\" + str(l)]\n",
        "        return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def momentum_gradient_descent(eta, beta, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            uW, ub = previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)]\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            uW = beta * uW + dW\n",
        "            ub = beta * ub + db\n",
        "            W -= eta * uW\n",
        "            b -= eta * ub\n",
        "            previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)] = uW, ub\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "            return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def rms_prop(eta, beta, epsilon, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"dW\" + str(l)])\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"db\" + str(l)])\n",
        "            factorW = eta / (np.sqrt(previous_updates[\"W\" + str(l)] + epsilon))\n",
        "            factorb = eta / (np.sqrt(previous_updates[\"b\" + str(l)] + epsilon))\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * my_network.grads[\"db\" + str(l)]\n",
        "            return previous_updates\n",
        "            '''\n",
        "            Working previously fetched an issue that the previous_updates should be returned\n",
        "            if not then it is showing validation accuracy as 9.05%\n",
        "            but after returning this slightly better\n",
        "            '''\n",
        "\n",
        "    @staticmethod\n",
        "    def nadam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            term1 = 1 - (beta1 ** (t))\n",
        "            term2 = (1 - beta1) * my_network.grads[\"dW\" + str(l)] / term1\n",
        "            term3 = (1 - beta1) * my_network.grads[\"db\" + str(l)] / term1\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * (beta1 * MW_corrected + term2)\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * (beta1 * Mb_corrected + term3)\n",
        "        return M, V, t + 1\n",
        "\n",
        "    @staticmethod\n",
        "    def adam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** t))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** t))\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** t))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** t))\n",
        "            my_network.theta[\"W\" + str(l)] -= (eta / (np.sqrt(VW_corrected) + epsilon)) * MW_corrected\n",
        "            my_network.theta[\"b\" + str(l)] -= (eta / (np.sqrt(Vb_corrected) + epsilon)) * Mb_corrected\n",
        "        return M, V, t + 1\n",
        "\n",
        "\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  theta = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "\n",
        "\n",
        "  def __init__(self,mode_of_initialization=\"random\",number_of_hidden_layers=1,num_neurons_in_hidden_layers=4,activation=\"sigmoid\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T):\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.theta[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.theta[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.theta[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def forward(self, X, activation, theta):\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1, self.n_layers):\n",
        "        H = self.cache[\"H\" + str(l - 1)]\n",
        "        W = self.theta[\"W\" + str(l)]\n",
        "        b = self.theta[\"b\" + str(l)]\n",
        "        A = np.dot(W, H) + b\n",
        "        self.cache[\"A\" + str(l)] = A\n",
        "        H = Util.apply_activation(A, activation)\n",
        "        self.cache[\"H\" + str(l)] = H\n",
        "    Al = self.cache[\"A\" + str(self.n_layers - 1)]\n",
        "    y_hat= Compute.softmax(Al)\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "\n",
        "  def backpropagation(self, y_predicted, e_y, batch_size, loss, activation, theta):\n",
        "        if loss == 'cross_entropy':\n",
        "            dA = y_predicted - e_y\n",
        "        elif loss=='squared_loss':\n",
        "            dA=(y_predicted - e_y)*Compute.softmax_derivative(self.cache[\"A\" + str(self.n_layers - 1)])\n",
        "        m = dA.shape[1]\n",
        "        self.grads[\"dA\" + str(self.n_layers - 1)] = dA\n",
        "\n",
        "        for k in range(self.n_layers - 1, 0, -1):\n",
        "            dA = self.grads[\"dA\" + str(k)]\n",
        "            H_prev = self.cache[\"H\" + str(k - 1)]\n",
        "            A_prev = self.cache[\"A\" + str(k - 1)]\n",
        "            W = self.theta[\"W\" + str(k)]\n",
        "\n",
        "            dW, db, dH_prev, dA_prev = Compute.calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size)\n",
        "\n",
        "            self.grads[\"dA\" + str(k - 1)] = dA_prev\n",
        "            self.grads[\"dW\" + str(k)] = dW\n",
        "            self.grads[\"db\" + str(k)] = db\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def compute(self, eta = 0.1,beta = 0.5,beta1 = 0.5,beta2 = 0.5 ,epsilon = 0.000001, optimizer = 'sgd',batch_size = 4,loss = 'cross_entropy',epochs = 1):\n",
        "    train_c_epoch, tarin_acc_per_epoch, val_c_per_epoch, val_acc_per_epoch, previous_updates, M, V = [], [], [], [], {}, {}, {}\n",
        "    for l in range(1 , self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      previous_updates[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    for l in range(1 , self.n_layers):\n",
        "      M[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      M[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "      V[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      V[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    t = 1\n",
        "    for count in range(epochs):\n",
        "      for i in range(0 , self.TrainInput.shape[1],batch_size):\n",
        "        if i + batch_size > self.TrainInput.shape[1]:\n",
        "          continue\n",
        "        theta = self.theta\n",
        "        yPredicted = self.forward(self.TrainInput[:,i:i + batch_size],self.activation_function,theta)\n",
        "        e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])\n",
        "        self.backpropagation(yPredicted,e_y,batch_size,loss,self.activation_function,theta)\n",
        "        if optimizer == 'sgd':   #referred slide page 54\n",
        "            Update.simple_gradient_descent(eta) #working\n",
        "        elif optimizer == 'nag':\n",
        "            previous_updates=Update.nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss) #working\n",
        "\n",
        "        elif optimizer == 'momentum': #referred from slide 43\n",
        "          previous_updates=Update.momentum_gradient_descent(eta,beta,previous_updates) #working\n",
        "\n",
        "        elif optimizer == 'RMSprop':\n",
        "          previosu_updates=Update.rms_prop(eta,beta,epsilon,previous_updates) #not working\n",
        "        elif optimizer == 'adam':\n",
        "          epsilon = 1e-8\n",
        "          M , V ,t= Update.adam(eta,beta1,beta2,epsilon,M , V , t)\n",
        "        elif optimizer == 'nadam':\n",
        "          epsilon = 1e-8\n",
        "          M , V , t= Update.nadam(eta,beta1,beta2,epsilon,M , V , t)\n",
        "\n",
        "      y_hat = self.forward(self.TrainInput,self.activation_function,self.theta)\n",
        "      valy_hat = self.forward(self.ValInput,self.activation_function,self.theta)\n",
        "      train_cost = Util.loss(self.TrainInput,self.TrainOutput,y_hat,loss,self.TrainInput.shape[1])\n",
        "      train_c_epoch.append(train_cost)\n",
        "      val_cost = Util.loss(self.ValInput,self.ValOutput,valy_hat,loss,self.ValInput.shape[1])\n",
        "      val_c_per_epoch.append(val_cost)\n",
        "      train_acc = Util.accuracy(self.TrainInput, self.TrainOutput,y_hat)\n",
        "      tarin_acc_per_epoch.append(train_acc)\n",
        "\n",
        "      val_acc = Util.accuracy(self.ValInput, self.ValOutput,valy_hat)\n",
        "      val_acc_per_epoch.append(val_acc)\n",
        "    #   print(np.eye(self.n_output)[self.ValOutput[0]].T.shape,valy_hat.shape)\n",
        "      conf_matrix = confusion_matrix(np.argmax(np.eye(self.n_output)[self.ValOutput[0]].T, axis=0),\n",
        "                                np.argmax(valy_hat, axis=0))\n",
        "\n",
        "      '''\n",
        "        # Print confusion matrix\n",
        "      cm_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"T-shirt/top\",\n",
        "                                                                        \"Trouser\",\n",
        "                                                                        \"Pullover\",\n",
        "                                                                        \"Dress\",\n",
        "                                                                        \"Coat\",\n",
        "                                                                        \"Sandal\",\n",
        "                                                                        \"Shirt\",\n",
        "                                                                        \"Sneaker\",\n",
        "                                                                        \"Bag\",\n",
        "                                                                        \"Ankle boot\"])\n",
        "\n",
        "        # Customize appearance\n",
        "      cm_display.plot(cmap='Oranges')\n",
        "      plt.xlabel('Predicted Label', fontsize=14)\n",
        "      plt.ylabel('True Label', fontsize=14)\n",
        "      plt.title('Confusion Matrix', fontsize=16)\n",
        "      plt.xticks(rotation=45, ha='right')\n",
        "      plt.tight_layout()\n",
        "      wandb.log({\"Confusion Matrix at epoch\"+str(count+1) : plt})\n",
        "      '''\n",
        "\n",
        "\n",
        "      print(\"---------\"*20)\n",
        "      print(f\"Epoch Number = {format(count+1)}\")\n",
        "      print(f\"Training Accuracy = {format(tarin_acc_per_epoch[-1])}\")\n",
        "      print(f\"Validation Accuracy = {format(val_acc_per_epoch[-1])}\")\n",
        "    #   wandb.log({\"training_accuracy\": train_acc,\"validation_accuracy\": val_acc,\"training_loss\":train_cost,\"validation_loss\": val_cost,\"epoch\": count})\n",
        "    return train_c_epoch,tarin_acc_per_epoch,val_c_per_epoch,val_acc_per_epoch\n",
        "\n",
        "\n",
        "my_network = MyNeuralNetwork(mode_of_initialization=\"xavier\",number_of_hidden_layers=1,num_neurons_in_hidden_layers=4,activation=\"sigmoid\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T)\n",
        "my_network.compute(eta = 0.1,beta = 0.5,beta1 = 0.5,beta2 = 0.5 ,epsilon = 0.05, optimizer = 'adam',batch_size = 4,loss = 'cross_entropy',epochs = 5)\n"
      ],
      "metadata": {
        "id": "pAS--NwnqTa5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# !pip install wandb\n",
        "\n",
        "# import wandb\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score,ConfusionMatrixDisplay\n",
        "from keras.datasets import mnist, fashion_mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# wandb.login()\n",
        "\n",
        "# wandb.init(\n",
        "#     # set the wandb project where this run will be logged\n",
        "#     project=\"deep-learning\",\n",
        "\n",
        "#     # track hyperparameters and run metadata\n",
        "#     config={\n",
        "#     \"eta\": 0.02\n",
        "#     }\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from keras.datasets import fashion_mnist\n",
        "(x_train,y_train),(x_test,y_test) = fashion_mnist.load_data()\n",
        "\n",
        "\n",
        "'''normalizing the data'''\n",
        "# x_test = x_test / 255.0\n",
        "# x_train = x_train / 255.0\n",
        "\n",
        "\n",
        "'''train set,val set ,test set split'''\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=42)\n",
        "x_train_T = x_train.reshape(-1, x_train.shape[1]*x_train.shape[2]).T\n",
        "x_val_T = x_val.reshape(-1, x_val.shape[1]*x_val.shape[2]).T\n",
        "x_test_T = x_test.reshape(-1, x_test.shape[1]*x_test.shape[2]).T\n",
        "y_train_T, y_val_T, y_test_T = y_train.reshape(1, -1), y_val.reshape(1, -1), y_test.reshape(1, -1)\n",
        "\n",
        "class Util:\n",
        "\n",
        "    @staticmethod\n",
        "    def apply_activation(A, activation):\n",
        "            if activation == 'sigmoid':\n",
        "                return Compute.sigmoid(A)\n",
        "            elif activation == 'relu':\n",
        "                return Compute.Relu(A)\n",
        "            elif activation == 'tanh':\n",
        "                return Compute.tanh(A)\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(input, true_output, predicted_output, loss, batch_size):\n",
        "        if loss == 'cross_entropy':\n",
        "            one_hot_true_output = np.eye(my_network.n_output)[true_output[0]].T\n",
        "            return -np.sum(one_hot_true_output * np.log(predicted_output + 1e-9)) / batch_size\n",
        "\n",
        "\n",
        "        if loss=='squared_loss':\n",
        "            one_hot_true_output = np.eye(my_network.n_output)[true_output[0]].T\n",
        "            loss_factor=np.square(predicted_output-one_hot_true_output)\n",
        "            return np.sum(loss_factor)/batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(input, true_output, predicted_output):\n",
        "        predicted_labels = np.argmax(predicted_output, axis=0)\n",
        "        correct_predictions = np.sum(true_output == predicted_labels)\n",
        "        total_samples = true_output.shape[1]\n",
        "        accuracy_percentage = (correct_predictions / total_samples) * 100\n",
        "        return accuracy_percentage\n",
        "\n",
        "class Compute:\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        return  1 / (1 + np.exp(-x))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu(x):\n",
        "        return np.maximum(x,0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh(x):\n",
        "        return (2 * Compute.sigmoid(2 * x)) - 1\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax_derivative(x):\n",
        "        return Compute.softmax(x) * (1-Compute.softmax(x))\n",
        "\n",
        "    @staticmethod\n",
        "    def sigmoid_derivative(Z):\n",
        "        s = Compute.sigmoid(Z)\n",
        "        dA = s * (1 - s)\n",
        "        return dA\n",
        "\n",
        "    @staticmethod\n",
        "    def Relu_derivative(x):\n",
        "        return 1*(x > 0)\n",
        "\n",
        "    @staticmethod\n",
        "    def tanh_derivative(x):\n",
        "        return (1 - (Compute.tanh(x)**2))\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size):\n",
        "            dW = Compute.calculate_dW(dA, H_prev, batch_size)\n",
        "            db = Compute.calculate_db(dA, batch_size)\n",
        "            dH_prev, dA_prev = Compute.calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev) if k > 1 else (np.zeros(H_prev.shape), np.zeros(A_prev.shape))\n",
        "\n",
        "            return dW, db, dH_prev, dA_prev\n",
        "    @staticmethod\n",
        "    def calculate_dW(dA, H_prev, batch_size):\n",
        "            return np.dot(dA, H_prev.T) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_db(dA, batch_size):\n",
        "            return np.sum(dA, axis=1, keepdims=True) / batch_size\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dH_prev_dA_prev(k, W, dA, activation, A_prev):\n",
        "            dH_prev = np.matmul(W.T, dA)\n",
        "            dA_prev = Compute.calculate_dA_prev(dH_prev, activation, A_prev)\n",
        "            return dH_prev, dA_prev\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_dA_prev(dH_prev, activation, A_prev):\n",
        "            if activation == 'sigmoid':\n",
        "                return dH_prev * Compute.sigmoid_derivative(A_prev)\n",
        "            elif activation == 'tanh':\n",
        "                return dH_prev * Compute.tanh_derivative(A_prev)\n",
        "            elif activation == 'relu':\n",
        "                return dH_prev * Compute.Relu_derivative(A_prev)\n",
        "\n",
        "\n",
        "class Update:\n",
        "    @staticmethod\n",
        "    def simple_gradient_descent(eta):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            W -= eta * dW\n",
        "            b -= eta * db\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "\n",
        "    @staticmethod\n",
        "    def nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss):\n",
        "        theta = {}\n",
        "        input_data = my_network.TrainInput[:, i:i + batch_size]\n",
        "        output_data = my_network.TrainOutput[0, i:i + batch_size]\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            theta[\"W\" + str(l)] = my_network.theta[\"W\" + str(l)] - beta * previous_updates[\"W\" + str(l)]\n",
        "            theta[\"b\" + str(l)] = my_network.theta[\"b\" + str(l)] - beta * previous_updates[\"b\" + str(l)]\n",
        "        y_predicted = my_network.forward(input_data, my_network.activation_function, my_network.theta)\n",
        "        e_y = np.transpose(np.eye(my_network.n_output)[output_data])\n",
        "        my_network.backpropagation(y_predicted, e_y, batch_size, loss, my_network.activation_function, my_network.theta)\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + my_network.grads[\"dW\" + str(l)]\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + my_network.grads[\"db\" + str(l)]\n",
        "            my_network.theta[\"W\" + str(l)] -= eta * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= eta * my_network.grads[\"db\" + str(l)]\n",
        "        return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def momentum_gradient_descent(eta, beta, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            uW, ub = previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)]\n",
        "            W, dW = my_network.theta[\"W\" + str(l)], my_network.grads[\"dW\" + str(l)]\n",
        "            b, db = my_network.theta[\"b\" + str(l)], my_network.grads[\"db\" + str(l)]\n",
        "            uW = beta * uW + dW\n",
        "            ub = beta * ub + db\n",
        "            W -= eta * uW\n",
        "            b -= eta * ub\n",
        "            previous_updates[\"W\" + str(l)], previous_updates[\"b\" + str(l)] = uW, ub\n",
        "            my_network.theta[\"W\" + str(l)], my_network.theta[\"b\" + str(l)] = W, b\n",
        "            return previous_updates\n",
        "\n",
        "    @staticmethod\n",
        "    def rms_prop(eta, beta, epsilon, previous_updates):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            previous_updates[\"W\" + str(l)] = beta * previous_updates[\"W\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"dW\" + str(l)])\n",
        "            previous_updates[\"b\" + str(l)] = beta * previous_updates[\"b\" + str(l)] + (1 - beta) * np.square(\n",
        "                my_network.grads[\"db\" + str(l)])\n",
        "            factorW = eta / (np.sqrt(previous_updates[\"W\" + str(l)] + epsilon))\n",
        "            factorb = eta / (np.sqrt(previous_updates[\"b\" + str(l)] + epsilon))\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * my_network.grads[\"dW\" + str(l)]\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * my_network.grads[\"db\" + str(l)]\n",
        "            return previous_updates\n",
        "            '''\n",
        "            Working previously fetched an issue that the previous_updates should be returned\n",
        "            if not then it is showing validation accuracy as 9.05%\n",
        "            but after returning this slightly better\n",
        "            '''\n",
        "\n",
        "    @staticmethod\n",
        "    def nadam(eta, beta1, beta2, epsilon, M, V, t):\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            MW_corrected = M[\"W\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "            Mb_corrected = M[\"b\" + str(l)] / (1 - (beta1 ** (t)))\n",
        "\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            VW_corrected = V[\"W\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "            Vb_corrected = V[\"b\" + str(l)] / (1 - (beta2 ** (t)))\n",
        "\n",
        "            factorW = eta / (np.sqrt(VW_corrected) + epsilon)\n",
        "            factorb = eta / (np.sqrt(Vb_corrected) + epsilon)\n",
        "            term1 = 1 - (beta1 ** (t))\n",
        "            term2 = (1 - beta1) * my_network.grads[\"dW\" + str(l)] / term1\n",
        "            term3 = (1 - beta1) * my_network.grads[\"db\" + str(l)] / term1\n",
        "            my_network.theta[\"W\" + str(l)] -= factorW * (beta1 * MW_corrected + term2)\n",
        "            my_network.theta[\"b\" + str(l)] -= factorb * (beta1 * Mb_corrected + term3)\n",
        "        return M, V\n",
        "\n",
        "    @staticmethod\n",
        "    def adam(eta, beta1, beta2, epsilon, M, V, t): #taken from slide-2 page 42 [cs6910]\n",
        "        for l in range(1, my_network.n_layers):\n",
        "            M[\"W\" + str(l)] = beta1 * M[\"W\" + str(l)] + (1 - beta1) * my_network.grads[\"dW\" + str(l)]\n",
        "            M[\"b\" + str(l)] = beta1 * M[\"b\" + str(l)] + (1 - beta1) * my_network.grads[\"db\" + str(l)]\n",
        "            V[\"W\" + str(l)] = beta2 * V[\"W\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"dW\" + str(l)])\n",
        "            V[\"b\" + str(l)] = beta2 * V[\"b\" + str(l)] + (1 - beta2) * np.square(my_network.grads[\"db\" + str(l)])\n",
        "            MW_hat = M[\"W\" + str(l)] / (1 - np.power(beta1,t))\n",
        "            Mb_hat = M[\"b\" + str(l)] / (1 - np.power(beta1,t))\n",
        "            VW_hat = V[\"W\" + str(l)] / (1 - np.power(beta2,t))\n",
        "            Vb_hat = V[\"b\" + str(l)] / (1 - np.power(beta2,t))\n",
        "            my_network.theta[\"W\" + str(l)] -= (eta / (np.sqrt(VW_hat) + epsilon)) * MW_hat\n",
        "            my_network.theta[\"b\" + str(l)] -= (eta / (np.sqrt(Vb_hat) + epsilon)) * Mb_hat\n",
        "        return M, V\n",
        "\n",
        "\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "  mode_of_initialization = \"\"\n",
        "  n_layers = 0\n",
        "  activation_function = \"\"\n",
        "  n_input = 0\n",
        "  n_output = 0\n",
        "  n_neurons = []\n",
        "  TrainInput = []\n",
        "  TrainOutput = []\n",
        "  ValInput = []\n",
        "  ValOutput = []\n",
        "  theta = {}\n",
        "  cache = {}\n",
        "  grads = {}\n",
        "\n",
        "\n",
        "  def __init__(self,mode_of_initialization=\"random\",number_of_hidden_layers=1,num_neurons_in_hidden_layers=4,activation=\"sigmoid\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T):\n",
        "    self.mode_of_initialization = mode_of_initialization\n",
        "    neuronsPerLayer = []\n",
        "    for i in range(number_of_hidden_layers):\n",
        "      neuronsPerLayer.append(num_neurons_in_hidden_layers)\n",
        "    self.n_layers = number_of_hidden_layers + 2\n",
        "    self.activation_function = activation\n",
        "    self.TrainInput = TrainInput\n",
        "    self.TrainOutput = TrainOutput\n",
        "    self.n_input = TrainInput.shape[0]\n",
        "    self.n_output = TrainOutput[0,TrainOutput.argmax(axis = 1)[0]] + 1\n",
        "    self.n_neurons = neuronsPerLayer\n",
        "    self.n_neurons.append(self.n_output)\n",
        "    self.n_neurons.insert(0 , self.n_input)\n",
        "    self.cache[\"H0\"] = TrainInput\n",
        "    self.cache[\"A0\"] = TrainInput\n",
        "    self.grads = {}\n",
        "    self.ValInput = ValInput\n",
        "    self.ValOutput = ValOutput\n",
        "    for l in range(1,self.n_layers):\n",
        "      if self.mode_of_initialization == \"random\":\n",
        "        self.theta[\"W\" + str(l)] = np.random.randn(self.n_neurons[l] , self.n_neurons[l - 1])\n",
        "      elif self.mode_of_initialization == \"xavier\":\n",
        "        limit = np.sqrt(2 / float(self.n_neurons[l - 1] + self.n_neurons[l]))\n",
        "        self.theta[\"W\" + str(l)] = np.random.normal(0.0, limit, size=(self.n_neurons[l],self.n_neurons[l - 1]))\n",
        "      self.theta[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "\n",
        "\n",
        "  def forward(self, X, activation, theta):\n",
        "    self.cache[\"H0\"] = X\n",
        "    for l in range(1, self.n_layers):\n",
        "        H = self.cache[\"H\" + str(l - 1)]\n",
        "        W = self.theta[\"W\" + str(l)]\n",
        "        b = self.theta[\"b\" + str(l)]\n",
        "        A = np.dot(W, H) + b\n",
        "        self.cache[\"A\" + str(l)] = A\n",
        "        H = Util.apply_activation(A, activation)\n",
        "        self.cache[\"H\" + str(l)] = H\n",
        "    Al = self.cache[\"A\" + str(self.n_layers - 1)]\n",
        "    y_hat= Compute.softmax(Al)\n",
        "\n",
        "    return y_hat\n",
        "\n",
        "\n",
        "  def backpropagation(self, y_predicted, e_y, batch_size, loss, activation, theta):\n",
        "        if loss == 'cross_entropy':\n",
        "            dA = y_predicted - e_y\n",
        "        elif loss=='squared_loss':\n",
        "            dA=(y_predicted - e_y)*Compute.softmax_derivative(self.cache[\"A\" + str(self.n_layers - 1)])\n",
        "        m = dA.shape[1]\n",
        "        self.grads[\"dA\" + str(self.n_layers - 1)] = dA\n",
        "\n",
        "        for k in range(self.n_layers - 1, 0, -1):\n",
        "            dA = self.grads[\"dA\" + str(k)]\n",
        "            H_prev = self.cache[\"H\" + str(k - 1)]\n",
        "            A_prev = self.cache[\"A\" + str(k - 1)]\n",
        "            W = self.theta[\"W\" + str(k)]\n",
        "\n",
        "            dW, db, dH_prev, dA_prev = Compute.calculate_gradients(k, dA, H_prev, A_prev, W, activation, batch_size)\n",
        "\n",
        "            self.grads[\"dA\" + str(k - 1)] = dA_prev\n",
        "            self.grads[\"dW\" + str(k)] = dW\n",
        "            self.grads[\"db\" + str(k)] = db\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def compute(self, eta = 0.1,beta = 0.5,beta1 = 0.5,beta2 = 0.5 ,epsilon = 0.000001, optimizer = 'sgd',batch_size = 4,loss = 'cross_entropy',epochs = 1):\n",
        "    train_c_epoch, tarin_acc_per_epoch, val_c_per_epoch, val_acc_per_epoch, previous_updates, M, V = [], [], [], [], {}, {}, {}\n",
        "    for l in range(1 , self.n_layers):\n",
        "      previous_updates[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      previous_updates[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    for l in range(1 , self.n_layers):\n",
        "      M[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      M[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "      V[\"W\" + str(l)] = np.zeros((self.n_neurons[l] , self.n_neurons[l - 1]))\n",
        "      V[\"b\" + str(l)] = np.zeros((self.n_neurons[l] , 1))\n",
        "    t = 1\n",
        "    for count in range(epochs):\n",
        "      for i in range(0 , self.TrainInput.shape[1],batch_size):\n",
        "        if i + batch_size > self.TrainInput.shape[1]:\n",
        "          continue\n",
        "        theta = self.theta\n",
        "        yPredicted = self.forward(self.TrainInput[:,i:i + batch_size],self.activation_function,theta)\n",
        "        e_y = np.transpose(np.eye(self.n_output)[self.TrainOutput[0,i : i + batch_size]])\n",
        "        self.backpropagation(yPredicted,e_y,batch_size,loss,self.activation_function,theta)\n",
        "        if optimizer == 'sgd':   #referred slide page 54\n",
        "            Update.simple_gradient_descent(eta) #working\n",
        "        elif optimizer == 'nag':\n",
        "            previous_updates=Update.nesterov_gradient_descent(i,eta, batch_size, beta, previous_updates,loss) #working\n",
        "\n",
        "        elif optimizer == 'momentum': #referred from slide 43\n",
        "          previous_updates=Update.momentum_gradient_descent(eta,beta,previous_updates) #working\n",
        "\n",
        "        elif optimizer == 'RMSprop':\n",
        "          previous_updates=Update.rms_prop(eta,beta,epsilon,previous_updates) #working\n",
        "        elif optimizer == 'adam':\n",
        "          epsilon = 1e-10\n",
        "          M , V = Update.adam(eta,beta1,beta2,epsilon,M , V , count+1)\n",
        "        elif optimizer == 'nadam':\n",
        "          epsilon = 1e-8\n",
        "          M , V = Update.nadam(eta,beta1,beta2,epsilon,M , V , count+1)\n",
        "\n",
        "      y_hat = self.forward(self.TrainInput,self.activation_function,self.theta)\n",
        "      valy_hat = self.forward(self.ValInput,self.activation_function,self.theta)\n",
        "      train_cost = Util.loss(self.TrainInput,self.TrainOutput,y_hat,loss,self.TrainInput.shape[1])\n",
        "      train_c_epoch.append(train_cost)\n",
        "      val_cost = Util.loss(self.ValInput,self.ValOutput,valy_hat,loss,self.ValInput.shape[1])\n",
        "      val_c_per_epoch.append(val_cost)\n",
        "      train_acc = Util.accuracy(self.TrainInput, self.TrainOutput,y_hat)\n",
        "      tarin_acc_per_epoch.append(train_acc)\n",
        "\n",
        "      val_acc = Util.accuracy(self.ValInput, self.ValOutput,valy_hat)\n",
        "      val_acc_per_epoch.append(val_acc)\n",
        "    #   print(np.eye(self.n_output)[self.ValOutput[0]].T.shape,valy_hat.shape)\n",
        "      conf_matrix = confusion_matrix(np.argmax(np.eye(self.n_output)[self.ValOutput[0]].T, axis=0),\n",
        "                                np.argmax(valy_hat, axis=0))\n",
        "\n",
        "      '''\n",
        "        # Print confusion matrix\n",
        "      cm_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[\"T-shirt/top\",\n",
        "                                                                        \"Trouser\",\n",
        "                                                                        \"Pullover\",\n",
        "                                                                        \"Dress\",\n",
        "                                                                        \"Coat\",\n",
        "                                                                        \"Sandal\",\n",
        "                                                                        \"Shirt\",\n",
        "                                                                        \"Sneaker\",\n",
        "                                                                        \"Bag\",\n",
        "                                                                        \"Ankle boot\"])\n",
        "\n",
        "        # Customize appearance\n",
        "      cm_display.plot(cmap='Oranges')\n",
        "      plt.xlabel('Predicted Label', fontsize=14)\n",
        "      plt.ylabel('True Label', fontsize=14)\n",
        "      plt.title('Confusion Matrix', fontsize=16)\n",
        "      plt.xticks(rotation=45, ha='right')\n",
        "      plt.tight_layout()\n",
        "      wandb.log({\"Confusion Matrix at epoch\"+str(count+1) : plt})\n",
        "      '''\n",
        "\n",
        "\n",
        "      print(\"---------\"*20)\n",
        "      print(f\"Epoch Number = {format(count+1)}\")\n",
        "      print(f\"Training Accuracy = {format(tarin_acc_per_epoch[-1])}\")\n",
        "      print(f\"Validation Accuracy = {format(val_acc_per_epoch[-1])}\")\n",
        "    #   wandb.log({\"training_accuracy\": train_acc,\"validation_accuracy\": val_acc,\"training_loss\":train_cost,\"validation_loss\": val_cost,\"epoch\": count})\n",
        "    return train_c_epoch,tarin_acc_per_epoch,val_c_per_epoch,val_acc_per_epoch\n",
        "\n",
        "\n",
        "my_network = MyNeuralNetwork(mode_of_initialization=\"random\",number_of_hidden_layers=1,num_neurons_in_hidden_layers=4,activation=\"sigmoid\",TrainInput=x_train_T,TrainOutput=y_train_T,ValInput=x_val_T,ValOutput=y_val_T)\n",
        "my_network.compute(eta = 0.01,beta = 0.5,beta1 = 0.1,beta2 = 0.1 ,epsilon = 0.05, optimizer = 'nadam',batch_size = 4,loss = 'cross_entropy',epochs = 10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5BuEJ8Iic-3",
        "outputId": "a2ae0ed9-c615-4a56-9551-8c7625488c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 1\n",
            "Training Accuracy = 42.58148148148148\n",
            "Validation Accuracy = 40.91666666666667\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 2\n",
            "Training Accuracy = 57.638888888888886\n",
            "Validation Accuracy = 55.86666666666667\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 3\n",
            "Training Accuracy = 56.5037037037037\n",
            "Validation Accuracy = 55.233333333333334\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 4\n",
            "Training Accuracy = 51.94444444444445\n",
            "Validation Accuracy = 50.6\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 5\n",
            "Training Accuracy = 56.4037037037037\n",
            "Validation Accuracy = 55.333333333333336\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 6\n",
            "Training Accuracy = 56.67592592592593\n",
            "Validation Accuracy = 55.60000000000001\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 7\n",
            "Training Accuracy = 57.70925925925926\n",
            "Validation Accuracy = 56.65\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 8\n",
            "Training Accuracy = 58.06111111111111\n",
            "Validation Accuracy = 57.03333333333334\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 9\n",
            "Training Accuracy = 58.007407407407406\n",
            "Validation Accuracy = 56.89999999999999\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "Epoch Number = 10\n",
            "Training Accuracy = 58.20925925925926\n",
            "Validation Accuracy = 56.98333333333333\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([2.9068382301698588,\n",
              "  2.616044606514456,\n",
              "  2.853810021297734,\n",
              "  3.054523036501255,\n",
              "  2.2641834628837487,\n",
              "  2.260980803329674,\n",
              "  2.2469092412275415,\n",
              "  2.2999240892482593,\n",
              "  2.3447029997300675,\n",
              "  2.2738277499425403],\n",
              " [42.58148148148148,\n",
              "  57.638888888888886,\n",
              "  56.5037037037037,\n",
              "  51.94444444444445,\n",
              "  56.4037037037037,\n",
              "  56.67592592592593,\n",
              "  57.70925925925926,\n",
              "  58.06111111111111,\n",
              "  58.007407407407406,\n",
              "  58.20925925925926],\n",
              " [2.984050032066803,\n",
              "  2.689703106286359,\n",
              "  2.8981520306808277,\n",
              "  3.148085070569006,\n",
              "  2.3297042127606735,\n",
              "  2.3274467454599836,\n",
              "  2.3105831636234075,\n",
              "  2.3934760863428957,\n",
              "  2.3946922831657935,\n",
              "  2.35657990317879],\n",
              " [40.91666666666667,\n",
              "  55.86666666666667,\n",
              "  55.233333333333334,\n",
              "  50.6,\n",
              "  55.333333333333336,\n",
              "  55.60000000000001,\n",
              "  56.65,\n",
              "  57.03333333333334,\n",
              "  56.89999999999999,\n",
              "  56.98333333333333])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qgu3vx9MV6zN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "139c8125-afca-46f4-afce-03835ff16301"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: abhirupmjdr: No such file or directory\n"
          ]
        }
      ]
    }
  ]
}